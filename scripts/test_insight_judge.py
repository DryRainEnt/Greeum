#!/usr/bin/env python3
"""
Test script for InsightJudge - LLM-based unified insight + branch judge

Usage:
    .venv_test/bin/python scripts/test_insight_judge.py [--live]

Options:
    --live    Use live LLM server (requires llama-server running)
"""

import os
import sys
import argparse
from unittest.mock import patch, MagicMock
from typing import List, Tuple

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from greeum.core.insight_judge import InsightJudge, JudgmentResult


# Test cases: (content, expected_is_insight)
TEST_CASES = [
    # TRUE INSIGHTS
    ("PostgreSQL ì¸ë±ìŠ¤ íŠœë‹ìœ¼ë¡œ ì¿¼ë¦¬ ì†ë„ 3ë°° í–¥ìƒì‹œì¼°ë‹¤", True),
    ("Docker ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì›ì¸ì„ ì°¾ì•„ì„œ í•´ê²°í–ˆë‹¤", True),
    ("ë²„ê·¸ ìˆ˜ì •í•¨", True),
    ("fixed the authentication bug", True),
    ("Reactì—ì„œ useMemoë¡œ ë Œë”ë§ ë¬¸ì œ ê³ ì¹¨", True),
    ("API íƒ€ì„ì•„ì›ƒì€ ì»¤ë„¥ì…˜ í’€ ì„¤ì • ë¬¸ì œì˜€ìŒ", True),
    ("ì£¼ì˜: rate limit ê±¸ë ¤ìˆìŒ", True),
    ("ë°°í¬ ì‹¤íŒ¨ ì›ì¸: í™˜ê²½ë³€ìˆ˜ ë¯¸ì„¤ì •", True),
    ("ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ DBê°€ ë³‘ëª©", True),

    # NOT INSIGHTS (noise)
    ("ë„¤", False),
    ("ì•Œê² ìŠµë‹ˆë‹¤", False),
    ("ok", False),
    ("thanks", False),
    ("ì•ˆë…•í•˜ì„¸ìš”", False),
    ("hello", False),
    ("ì˜¤ëŠ˜ ë‚ ì”¨ ì¢‹ë‹¤", False),
    ("ì ì‹¬ ë­ ë¨¹ì§€", False),
    ("ã…‹ã…‹ã…‹", False),
    ("ìŒ", False),
]


def mock_llm_response(content: str) -> str:
    """Generate mock LLM response based on content heuristics."""
    content_lower = content.lower()

    # Noise patterns
    noise_patterns = [
        "ë„¤", "ì•Œê² ", "ok", "thanks", "ì•ˆë…•", "hello", "ë‚ ì”¨", "ì ì‹¬",
        "ã…‹ã…‹", "ã…ã…", "ìŒ", "ê·¸ë ‡", "ì»¤í”¼", "good", "sure", "yes"
    ]

    is_noise = (
        len(content.strip()) < 10 or
        any(p in content_lower for p in noise_patterns)
    )

    if is_noise:
        return """INSIGHT: NO
REASON: Simple acknowledgment or casual content with no technical value
BRANCH: NONE
BRANCH_REASON: Not storing
CATEGORIES:"""
    else:
        return """INSIGHT: YES
REASON: Contains technical insight about development work
BRANCH: NEW_BRANCH
BRANCH_REASON: New technical context
CATEGORIES: problem_solving, debugging"""


def test_parsing():
    """Test response parsing logic."""
    print("\n" + "="*60)
    print("ğŸ“‹ Testing Response Parsing")
    print("="*60)

    judge = InsightJudge(enabled=False)  # Disabled for parsing tests

    test_responses = [
        # Standard YES response
        ("""INSIGHT: YES
REASON: Documents a bug fix
BRANCH: EXISTING:abc123
BRANCH_REASON: Related to API
CATEGORIES: problem_solving, debugging""", True, False),

        # Standard NO response
        ("""INSIGHT: NO
REASON: Simple greeting
BRANCH: NONE
BRANCH_REASON: Not storing
CATEGORIES:""", False, False),

        # NEW_BRANCH response
        ("""INSIGHT: YES
REASON: New project setup
BRANCH: NEW_BRANCH
BRANCH_REASON: New context
CATEGORIES: config, setup""", True, True),

        # Edge case: minimal response
        ("""INSIGHT: YES
BRANCH: NEW_BRANCH""", True, True),
    ]

    passed = 0
    for response, expected_insight, expected_new_branch in test_responses:
        result = judge._parse_response(response, {})

        if result.is_insight == expected_insight and result.create_new_branch == expected_new_branch:
            print(f"  âœ… Parsed correctly: insight={result.is_insight}, new_branch={result.create_new_branch}")
            passed += 1
        else:
            print(f"  âŒ Parse error: expected insight={expected_insight}, new_branch={expected_new_branch}")
            print(f"      Got: insight={result.is_insight}, new_branch={result.create_new_branch}")

    print(f"\n  Parsing tests: {passed}/{len(test_responses)} passed")
    return passed == len(test_responses)


def test_with_mock_llm():
    """Test InsightJudge with mocked LLM responses."""
    print("\n" + "="*60)
    print("ğŸ¤– Testing with Mock LLM")
    print("="*60)

    judge = InsightJudge(enabled=True)

    # Mock the LLM call
    correct = 0
    total = len(TEST_CASES)

    for content, expected in TEST_CASES:
        # Directly test parsing with mock response
        mock_response = mock_llm_response(content)
        result = judge._parse_response(mock_response, {})

        if result.is_insight == expected:
            correct += 1
            status = "âœ…"
        else:
            status = "âŒ"
            print(f"  {status} '{content[:30]}...' expected={expected}, got={result.is_insight}")

    accuracy = correct / total * 100
    print(f"\n  Mock LLM accuracy: {correct}/{total} ({accuracy:.1f}%)")

    return accuracy >= 80


def test_with_live_llm():
    """Test InsightJudge with live LLM server."""
    print("\n" + "="*60)
    print("ğŸ”´ Testing with Live LLM")
    print("="*60)

    judge = InsightJudge(enabled=True, timeout=10.0)

    if not judge.is_available():
        print("  âš ï¸  LLM server not available at", judge.llm_url)
        print("  Skipping live tests. Start llama-server to run these tests.")
        return None

    print(f"  âœ… LLM server available at {judge.llm_url}")

    correct = 0
    total = len(TEST_CASES)

    for content, expected in TEST_CASES:
        result = judge.judge(content)

        if result.is_insight == expected:
            correct += 1
            status = "âœ…"
        else:
            status = "âŒ"

        print(f"  {status} '{content[:25]}...' â†’ insight={result.is_insight} (expected={expected})")
        if result.insight_reason:
            print(f"       Reason: {result.insight_reason[:50]}...")

    accuracy = correct / total * 100
    print(f"\n  Live LLM accuracy: {correct}/{total} ({accuracy:.1f}%)")

    # Print stats
    stats = judge.get_stats()
    print(f"\n  Stats: {stats}")

    return accuracy


def test_edge_cases():
    """Test edge cases."""
    print("\n" + "="*60)
    print("ğŸ”¬ Testing Edge Cases")
    print("="*60)

    judge = InsightJudge(enabled=True)

    edge_cases = [
        ("", False),  # Empty
        ("a", False),  # Too short
        ("ë„¤ë„¤ë„¤ë„¤ë„¤", False),  # Repeated acknowledgment
        ("!@#$%^&*()", False),  # Special chars only
        ("12345", False),  # Numbers only
        ("ë²„ê·¸ ìˆ˜ì •", True),  # Short but insight
    ]

    passed = 0
    for content, expected in edge_cases:
        # Quick filter test (before LLM)
        if len(content.strip()) < 5:
            result = JudgmentResult(is_insight=False, insight_confidence=1.0, insight_reason="Too short")
        else:
            mock_response = mock_llm_response(content)
            result = judge._parse_response(mock_response, {})

        if result.is_insight == expected:
            passed += 1
            print(f"  âœ… '{content[:20]}' â†’ {result.is_insight}")
        else:
            print(f"  âŒ '{content[:20]}' expected={expected}, got={result.is_insight}")

    print(f"\n  Edge cases: {passed}/{len(edge_cases)} passed")
    return passed == len(edge_cases)


def test_prompt_building():
    """Test prompt building with branches."""
    print("\n" + "="*60)
    print("ğŸ“ Testing Prompt Building")
    print("="*60)

    from greeum.core.insight_judge import SimilarBlock

    judge = InsightJudge(enabled=False)

    # Create mock similar blocks
    mock_blocks = {
        "branch_abc123": [
            SimilarBlock(1, "Fixed API timeout issue", "branch_abc123", 0.85, "hash1"),
            SimilarBlock(2, "Added retry logic", "branch_abc123", 0.75, "hash2"),
        ],
        "branch_def456": [
            SimilarBlock(3, "Setup Docker compose", "branch_def456", 0.65, "hash3"),
        ],
    }

    prompt = judge._build_prompt("New API bug fix", mock_blocks, "Working on backend")

    # Check prompt contains expected parts
    checks = [
        ("Working on backend" in prompt, "Context hint included"),
        ("branch_abc" in prompt, "Branch ID included"),
        ("Fixed API timeout" in prompt, "Similar block content included"),
        ("NEW CONTENT TO JUDGE" in prompt, "Content section included"),
        ("New API bug fix" in prompt, "Actual content included"),
    ]

    passed = 0
    for check, desc in checks:
        if check:
            passed += 1
            print(f"  âœ… {desc}")
        else:
            print(f"  âŒ {desc}")

    print(f"\n  Prompt building: {passed}/{len(checks)} passed")
    return passed == len(checks)


def run_all_tests(live: bool = False):
    """Run all tests."""
    print("ğŸ¯ InsightJudge Test Suite")
    print("="*60)

    results = []

    # Test 1: Parsing
    results.append(("Parsing", test_parsing()))

    # Test 2: Mock LLM
    results.append(("Mock LLM", test_with_mock_llm()))

    # Test 3: Edge cases
    results.append(("Edge Cases", test_edge_cases()))

    # Test 4: Prompt building
    results.append(("Prompt Building", test_prompt_building()))

    # Test 5: Live LLM (optional)
    if live:
        live_result = test_with_live_llm()
        if live_result is not None:
            results.append(("Live LLM", live_result >= 80))

    # Summary
    print("\n" + "="*60)
    print("ğŸ“Š TEST SUMMARY")
    print("="*60)

    all_passed = True
    for name, passed in results:
        status = "âœ…" if passed else "âŒ"
        print(f"  {status} {name}")
        if not passed:
            all_passed = False

    print("\n" + "="*60)
    if all_passed:
        print("ğŸ‰ All tests passed!")
    else:
        print("âš ï¸  Some tests failed")

    return all_passed


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Test InsightJudge")
    parser.add_argument("--live", action="store_true", help="Test with live LLM server")
    args = parser.parse_args()

    success = run_all_tests(live=args.live)
    sys.exit(0 if success else 1)

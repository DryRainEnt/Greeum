#!/usr/bin/env python3
"""
Greeum LLM Context Classifier - Large Scale Stress Test
Estimated runtime: 2-3 hours
"""

import json
import os
import random
import sys
import time
from datetime import datetime
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from greeum.core.context_classifier import get_context_classifier

# =============================================================================
# Configuration
# =============================================================================
OUTPUT_DIR = Path(__file__).parent.parent / "test_results"
OUTPUT_DIR.mkdir(exist_ok=True)

TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
RESULTS_FILE = OUTPUT_DIR / f"stress_test_{TIMESTAMP}.json"
SUMMARY_FILE = OUTPUT_DIR / f"stress_test_{TIMESTAMP}_summary.txt"
LOG_FILE = OUTPUT_DIR / f"stress_test_{TIMESTAMP}.log"

# Test configuration
TARGET_DURATION_HOURS = 2.5
CHECKPOINT_INTERVAL = 300  # Save checkpoint every 5 minutes
CONSISTENCY_REPEATS = 5

# =============================================================================
# Template-based Test Data Generation
# =============================================================================

TEMPLATES = {
    "A": {  # Work/Project
        "patterns": [
            "{action} {target}ì„/ë¥¼ {verb}.",
            "{project} í”„ë¡œì íŠ¸ {status}.",
            "{tool}ë¡œ {task} ìž‘ì—… ì¤‘.",
            "íŒ€ {meeting}ì—ì„œ {topic} ë…¼ì˜.",
            "{system} {component} {action_past}.",
            "{dev_action} ì™„ë£Œí•˜ê³  {next_action}.",
            "í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ìœ¼ë¡œ {feature} {dev_status}.",
            "{code_action} í›„ í…ŒìŠ¤íŠ¸ {test_result}.",
            "ë°°í¬ {deploy_status}, {env} í™˜ê²½.",
            "{review_type} ë¦¬ë·° {review_action}.",
        ],
        "fills": {
            "action": ["êµ¬í˜„", "ìˆ˜ì •", "ë¦¬íŒ©í† ë§", "ìµœì í™”", "ë””ë²„ê¹…", "ì„¤ê³„", "ë¶„ì„", "í…ŒìŠ¤íŠ¸"],
            "target": ["API", "ì„œë²„", "í´ë¼ì´ì–¸íŠ¸", "ë°ì´í„°ë² ì´ìŠ¤", "ìºì‹œ", "ë¡œì§", "ëª¨ë“ˆ", "í•¨ìˆ˜"],
            "verb": ["ì™„ë£Œí–ˆë‹¤", "ì§„í–‰ ì¤‘", "ì‹œìž‘í–ˆë‹¤", "ë§ˆë¬´ë¦¬í–ˆë‹¤"],
            "project": ["Greeum", "ë°±ì—”ë“œ", "í”„ë¡ íŠ¸ì—”ë“œ", "ì¸í”„ë¼", "ML íŒŒì´í”„ë¼ì¸", "ë°ì´í„°"],
            "status": ["ì§„í–‰ ì¤‘", "ì™„ë£Œ", "ì‹œìž‘", "ë§ˆë¬´ë¦¬ ë‹¨ê³„", "í…ŒìŠ¤íŠ¸ ì¤‘"],
            "tool": ["Docker", "Kubernetes", "Git", "Jenkins", "Terraform", "VS Code"],
            "task": ["ë°°í¬", "ì„¤ì •", "ë§ˆì´ê·¸ë ˆì´ì…˜", "ëª¨ë‹ˆí„°ë§", "ìžë™í™”"],
            "meeting": ["ë¯¸íŒ…", "ìŠ¤íƒ ë“œì—…", "íšŒì˜", "ë¦¬ë·°"],
            "topic": ["ì§„í–‰ ìƒí™©", "ì´ìŠˆ", "ì¼ì •", "ê¸°ìˆ  ìŠ¤íƒ", "ì•„í‚¤í…ì²˜"],
            "system": ["ì¸ì¦", "ê²°ì œ", "ì•Œë¦¼", "ê²€ìƒ‰", "ì¶”ì²œ"],
            "component": ["ì‹œìŠ¤í…œ", "ì„œë¹„ìŠ¤", "ëª¨ë“ˆ", "API"],
            "action_past": ["ê°œì„ ", "ìˆ˜ì •", "ì¶”ê°€", "ì œê±°", "ì—…ë°ì´íŠ¸"],
            "dev_action": ["ì½”ë“œ ìž‘ì„±", "ë²„ê·¸ ìˆ˜ì •", "ê¸°ëŠ¥ ì¶”ê°€", "ì„±ëŠ¥ ê°œì„ "],
            "next_action": ["PR ì˜¬ë¦¼", "ì½”ë“œ ë¦¬ë·° ìš”ì²­", "í…ŒìŠ¤íŠ¸ ìž‘ì„±", "ë¬¸ì„œí™”"],
            "feature": ["ìƒˆ ê¸°ëŠ¥", "ë²„ê·¸ í”½ìŠ¤", "UI ê°œì„ ", "API ë³€ê²½"],
            "dev_status": ["ê°œë°œ ì¤‘", "ì™„ë£Œ", "ê²€í†  ì¤‘", "ë°°í¬ ëŒ€ê¸°"],
            "code_action": ["ì»¤ë°‹", "í‘¸ì‹œ", "ë¨¸ì§€", "ë¦¬ë² ì´ìŠ¤"],
            "test_result": ["í†µê³¼", "ì‹¤íŒ¨ í›„ ìˆ˜ì •", "ì§„í–‰ ì¤‘"],
            "deploy_status": ["ì™„ë£Œ", "ì§„í–‰ ì¤‘", "ë¡¤ë°±", "ëŒ€ê¸°"],
            "env": ["ê°œë°œ", "ìŠ¤í…Œì´ì§•", "í”„ë¡œë•ì…˜", "í…ŒìŠ¤íŠ¸"],
            "review_type": ["ì½”ë“œ", "ì„¤ê³„", "PR", "ì•„í‚¤í…ì²˜"],
            "review_action": ["ì™„ë£Œ", "ì§„í–‰ ì¤‘", "ìš”ì²­", "í”¼ë“œë°± ë°˜ì˜"],
        }
    },
    "B": {  # Personal/Daily
        "patterns": [
            "ì˜¤ëŠ˜ {person}ì™€/ê³¼ {activity}.",
            "{time}ì— {place}ì—ì„œ {food} ë¨¹ì—ˆë‹¤.",
            "{weather} ë‚ ì”¨ë¼ì„œ {outdoor_activity}.",
            "ì£¼ë§ì— {weekend_activity}.",
            "{feeling} ê¸°ë¶„ì´ë¼ {mood_activity}.",
            "{person}í•œí…Œ {communication} ë°›ì•˜ë‹¤.",
            "ì§‘ì—ì„œ {home_activity}.",
            "{entertainment} ë³´ë©´ì„œ {snack} ë¨¹ìŒ.",
            "ì˜¤ëžœë§Œì— {hobby} í–ˆë‹¤.",
            "{exercise} í•˜ê³  {after_exercise}.",
        ],
        "fills": {
            "person": ["ì¹œêµ¬", "ê°€ì¡±", "ë™ìƒ", "ë¶€ëª¨ë‹˜", "ì—°ì¸", "ë™ë£Œ"],
            "activity": ["ì €ë… ì‹ì‚¬", "ì˜í™” ê´€ëžŒ", "ì¹´íŽ˜ ë°©ë¬¸", "ì‡¼í•‘", "ì‚°ì±…", "ê²Œìž„"],
            "time": ["ì ì‹¬", "ì €ë…", "ì•„ì¹¨", "ìƒˆë²½", "ì˜¤í›„"],
            "place": ["ë§›ì§‘", "ì¹´íŽ˜", "ë ˆìŠ¤í† ëž‘", "íŽ¸ì˜ì ", "ì§‘"],
            "food": ["ì¹˜í‚¨", "í”¼ìž", "ë¼ë©´", "ì‚¼ê²¹ì‚´", "ì´ˆë°¥", "íŒŒìŠ¤íƒ€", "í–„ë²„ê±°"],
            "weather": ["ë§‘ì€", "íë¦°", "ë¹„ì˜¤ëŠ”", "ëˆˆì˜¤ëŠ”", "ë”°ëœ»í•œ", "ì‹œì›í•œ"],
            "outdoor_activity": ["ì‚°ì±…í–ˆë‹¤", "ì¡°ê¹…í–ˆë‹¤", "ìžì „ê±° íƒ”ë‹¤", "ë“œë¼ì´ë¸Œ ê°”ë‹¤"],
            "weekend_activity": ["ì—¬í–‰ ë‹¤ë…€ì™”ë‹¤", "ì§‘ì—ì„œ ì‰¬ì—ˆë‹¤", "ì¹œêµ¬ ë§Œë‚¬ë‹¤", "ì²­ì†Œí–ˆë‹¤"],
            "feeling": ["ì¢‹ì€", "í”¼ê³¤í•œ", "ì„¤ë ˆëŠ”", "ìš°ìš¸í•œ", "íŽ¸ì•ˆí•œ"],
            "mood_activity": ["ìŒì•… ë“¤ì—ˆë‹¤", "ì‚°ì±…í–ˆë‹¤", "ìž  ìž¤ë‹¤", "ê²Œìž„í–ˆë‹¤"],
            "communication": ["ì—°ë½", "ì „í™”", "ë©”ì‹œì§€", "ì„ ë¬¼"],
            "home_activity": ["ì²­ì†Œ", "ìš”ë¦¬", "ë¹¨ëž˜", "ì •ë¦¬", "íœ´ì‹"],
            "entertainment": ["ë“œë¼ë§ˆ", "ì˜í™”", "ìœ íŠœë¸Œ", "ë„·í”Œë¦­ìŠ¤", "ì˜ˆëŠ¥"],
            "snack": ["ê³¼ìž", "íŒì½˜", "ì•„ì´ìŠ¤í¬ë¦¼", "ê³¼ì¼"],
            "hobby": ["ê²Œìž„", "ë…ì„œ", "ê·¸ë¦¼ ê·¸ë¦¬ê¸°", "ì•…ê¸° ì—°ì£¼", "ìš”ë¦¬"],
            "exercise": ["í—¬ìŠ¤", "ëŸ¬ë‹", "ìˆ˜ì˜", "ìš”ê°€", "ë“±ì‚°"],
            "after_exercise": ["ìƒ¤ì›Œí–ˆë‹¤", "íœ´ì‹í–ˆë‹¤", "ë°¥ ë¨¹ì—ˆë‹¤", "ìŠ¤íŠ¸ë ˆì¹­í–ˆë‹¤"],
        }
    },
    "C": {  # Learning/Knowledge
        "patterns": [
            "{subject} {study_action}.",
            "{resource}ì—ì„œ {topic} ë°°ì› ë‹¤.",
            "{concept} ê°œë… {understanding}.",
            "ì˜¤ëŠ˜ {learned_thing} ì•Œê²Œ ë¨.",
            "{course} ê°•ì˜ {course_status}.",
            "{book} ì±… ì½ëŠ” ì¤‘, {chapter} ë¶€ë¶„.",
            "{lang} ë¬¸ë²• {lang_action}.",
            "{field} ê´€ë ¨ {material} ì •ë¦¬.",
            "ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œ {algo_count}ê°œ {algo_status}.",
            "{cert} ìžê²©ì¦ {cert_action}.",
        ],
        "fills": {
            "subject": ["Python", "JavaScript", "ë¨¸ì‹ ëŸ¬ë‹", "ë°ì´í„°ë¶„ì„", "ì˜ì–´", "ìˆ˜í•™"],
            "study_action": ["ê³µë¶€ ì¤‘", "ë³µìŠµí–ˆë‹¤", "ì •ë¦¬í–ˆë‹¤", "ì‹¤ìŠµí–ˆë‹¤"],
            "resource": ["ê°•ì˜", "ì±…", "ë¸”ë¡œê·¸", "ë¬¸ì„œ", "íŠœí† ë¦¬ì–¼", "ìœ íŠœë¸Œ"],
            "topic": ["ìƒˆë¡œìš´ ê¸°ëŠ¥", "í•µì‹¬ ê°œë…", "ì‹¤ì „ ì˜ˆì œ", "ë² ìŠ¤íŠ¸ í”„ëž™í‹°ìŠ¤"],
            "concept": ["ê°ì²´ì§€í–¥", "í•¨ìˆ˜í˜•", "ë¹„ë™ê¸°", "ë™ì‹œì„±", "ë””ìžì¸ íŒ¨í„´"],
            "understanding": ["ì´í•´í•¨", "ì–´ë ¤ì›€", "ë³µìŠµ í•„ìš”", "ë§ˆìŠ¤í„°"],
            "learned_thing": ["ìƒˆë¡œìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬", "ìµœì í™” ê¸°ë²•", "ë²„ê·¸ í•´ê²°ë²•", "ì„¤ê³„ íŒ¨í„´"],
            "course": ["ì˜¨ë¼ì¸", "ì˜¤í”„ë¼ì¸", "ë¶€íŠ¸ìº í”„", "ëŒ€í•™"],
            "course_status": ["ìˆ˜ê°• ì¤‘", "ì™„ë£Œ", "ì‹œìž‘", "ë³µìŠµ"],
            "book": ["ê¸°ìˆ ", "êµì–‘", "ì „ê³µ", "ìžê¸°ê³„ë°œ"],
            "chapter": ["ì´ˆë°˜", "ì¤‘ë°˜", "í›„ë°˜", "ë§ˆë¬´ë¦¬"],
            "lang": ["ì˜ì–´", "ì¼ë³¸ì–´", "ì¤‘êµ­ì–´", "í•œêµ­ì–´", "í”„ëž‘ìŠ¤ì–´"],
            "lang_action": ["ê³µë¶€", "ì—°ìŠµ", "ë³µìŠµ", "ì•”ê¸°"],
            "field": ["AI", "í´ë¼ìš°ë“œ", "ë³´ì•ˆ", "ë„¤íŠ¸ì›Œí¬", "ë°ì´í„°ë² ì´ìŠ¤"],
            "material": ["ë…¼ë¬¸", "ë¬¸ì„œ", "ìžë£Œ", "ë…¸íŠ¸"],
            "algo_count": ["3", "5", "10", "1", "7"],
            "algo_status": ["í’€ì—ˆë‹¤", "ë„ì „ ì¤‘", "ë³µìŠµ", "ì‹¤íŒ¨ í›„ ìž¬ë„ì „"],
            "cert": ["AWS", "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬", "SQLD", "í† ìµ", "OPIC"],
            "cert_action": ["ì¤€ë¹„ ì¤‘", "ì·¨ë“", "ê³µë¶€ ì‹œìž‘", "ì‹œí—˜ ì˜ˆì •"],
        }
    }
}

EDGE_CASES = [
    ("", "edge"),
    (".", "edge"),
    ("...", "edge"),
    ("ã…‹ã…‹ã…‹", "edge"),
    ("ã… ã… ", "edge"),
    ("ì˜¤ëŠ˜", "edge"),
    ("ì¢‹ì•„", "edge"),
    ("123", "edge"),
    ("!@#$%", "edge"),
    ("hello", "edge"),
    ("ã“ã‚“ã«ã¡ã¯", "edge"),
    ("ä½ å¥½", "edge"),
    ("ðŸŽ‰ðŸŽŠðŸŽ", "edge"),
    ("a" * 500, "edge"),  # Very long
    ("ì˜¤ëŠ˜ ì½”ë”©í•˜ë©´ì„œ ì¹œêµ¬ëž‘ í†µí™”í•˜ê³  ìƒˆë¡œìš´ ê°œë…ë„ ë°°ì› ë‹¤", "edge"),  # Mixed
]


def generate_sentence(category: str) -> str:
    """Generate a random sentence for the given category."""
    template_data = TEMPLATES[category]
    pattern = random.choice(template_data["patterns"])
    fills = template_data["fills"]

    # Replace placeholders
    result = pattern
    for key, values in fills.items():
        placeholder = "{" + key + "}"
        if placeholder in result:
            result = result.replace(placeholder, random.choice(values), 1)

    return result


def generate_test_dataset(count: int) -> list:
    """Generate a balanced test dataset."""
    dataset = []
    per_category = count // 3

    for category in ["A", "B", "C"]:
        for _ in range(per_category):
            sentence = generate_sentence(category)
            dataset.append({"content": sentence, "expected": category, "type": "generated"})

    # Add edge cases
    for content, case_type in EDGE_CASES:
        dataset.append({"content": content, "expected": "?", "type": case_type})

    random.shuffle(dataset)
    return dataset


# =============================================================================
# Test Runner
# =============================================================================

def log(message: str):
    """Log message to file and stdout."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_line = f"[{timestamp}] {message}"
    print(log_line)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(log_line + "\n")


def save_checkpoint(results: dict, stats: dict):
    """Save intermediate results."""
    checkpoint = {
        "timestamp": datetime.now().isoformat(),
        "stats": stats,
        "results_count": len(results.get("classifications", [])),
    }

    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        json.dump({"checkpoint": checkpoint, "results": results}, f, ensure_ascii=False, indent=2)

    log(f"Checkpoint saved: {stats['total_classifications']} classifications")


def run_stress_test():
    """Main stress test runner."""
    log("=" * 70)
    log("Greeum LLM Context Classifier - Large Scale Stress Test")
    log("=" * 70)
    log(f"Target duration: {TARGET_DURATION_HOURS} hours")
    log(f"Results will be saved to: {RESULTS_FILE}")
    log("")

    # Initialize classifier
    classifier = get_context_classifier()
    if not classifier.is_available():
        log("ERROR: LLM classifier not available!")
        return

    log(f"LLM server: {classifier.llm_url}")
    log("")

    # Generate initial dataset
    log("Generating test dataset...")
    dataset = generate_test_dataset(5000)
    log(f"Generated {len(dataset)} test cases")
    log("")

    # Initialize results
    results = {
        "classifications": [],
        "consistency_tests": [],
        "timing_samples": [],
        "errors": [],
    }

    stats = {
        "total_classifications": 0,
        "correct": 0,
        "incorrect": 0,
        "edge_cases": 0,
        "total_time_ms": 0,
        "min_time_ms": float("inf"),
        "max_time_ms": 0,
        "errors": 0,
        "by_category": {"A": {"total": 0, "correct": 0}, "B": {"total": 0, "correct": 0}, "C": {"total": 0, "correct": 0}},
        "consistency_failures": 0,
    }

    start_time = time.time()
    last_checkpoint = start_time
    target_end = start_time + (TARGET_DURATION_HOURS * 3600)

    log("Starting stress test...")
    log("")

    iteration = 0

    while time.time() < target_end:
        iteration += 1

        # Phase 1: Classification accuracy test
        test_case = random.choice(dataset)
        content = test_case["content"]
        expected = test_case["expected"]

        try:
            t_start = time.time()
            result = classifier.classify(content)
            t_elapsed = (time.time() - t_start) * 1000

            stats["total_classifications"] += 1
            stats["total_time_ms"] += t_elapsed
            stats["min_time_ms"] = min(stats["min_time_ms"], t_elapsed)
            stats["max_time_ms"] = max(stats["max_time_ms"], t_elapsed)

            if expected == "?":
                stats["edge_cases"] += 1
            elif result.slot == expected:
                stats["correct"] += 1
                stats["by_category"][expected]["correct"] += 1
                stats["by_category"][expected]["total"] += 1
            else:
                stats["incorrect"] += 1
                stats["by_category"][expected]["total"] += 1
                results["classifications"].append({
                    "content": content[:100],
                    "expected": expected,
                    "got": result.slot,
                    "time_ms": t_elapsed,
                    "fallback": result.fallback_used,
                })

            # Sample timing data periodically
            if iteration % 100 == 0:
                results["timing_samples"].append({
                    "iteration": iteration,
                    "time_ms": t_elapsed,
                    "timestamp": time.time() - start_time,
                })

        except Exception as e:
            stats["errors"] += 1
            results["errors"].append({
                "iteration": iteration,
                "content": content[:50],
                "error": str(e),
            })

        # Phase 2: Consistency test (every 500 iterations)
        if iteration % 500 == 0:
            test_sentence = generate_sentence(random.choice(["A", "B", "C"]))
            slots = []
            for _ in range(CONSISTENCY_REPEATS):
                try:
                    r = classifier.classify(test_sentence)
                    slots.append(r.slot)
                except:
                    slots.append("ERROR")

            if len(set(slots)) > 1:
                stats["consistency_failures"] += 1
                results["consistency_tests"].append({
                    "sentence": test_sentence[:50],
                    "results": slots,
                    "iteration": iteration,
                })

        # Progress update every 1000 iterations
        if iteration % 1000 == 0:
            elapsed_hours = (time.time() - start_time) / 3600
            remaining_hours = (target_end - time.time()) / 3600
            accuracy = stats["correct"] / max(1, stats["correct"] + stats["incorrect"]) * 100
            avg_time = stats["total_time_ms"] / max(1, stats["total_classifications"])

            log(f"Progress: {iteration} iterations | "
                f"Accuracy: {accuracy:.1f}% | "
                f"Avg time: {avg_time:.1f}ms | "
                f"Elapsed: {elapsed_hours:.1f}h | "
                f"Remaining: {remaining_hours:.1f}h")

        # Save checkpoint periodically
        if time.time() - last_checkpoint > CHECKPOINT_INTERVAL:
            save_checkpoint(results, stats)
            last_checkpoint = time.time()

    # Final save
    total_time = time.time() - start_time

    stats["total_time_seconds"] = total_time
    stats["avg_time_ms"] = stats["total_time_ms"] / max(1, stats["total_classifications"])
    stats["accuracy"] = stats["correct"] / max(1, stats["correct"] + stats["incorrect"]) * 100
    stats["throughput"] = stats["total_classifications"] / total_time

    # Category-wise accuracy
    for cat in ["A", "B", "C"]:
        cat_stats = stats["by_category"][cat]
        cat_stats["accuracy"] = cat_stats["correct"] / max(1, cat_stats["total"]) * 100

    save_checkpoint(results, stats)

    # Write summary
    with open(SUMMARY_FILE, "w", encoding="utf-8") as f:
        f.write("=" * 70 + "\n")
        f.write("GREEUM LLM CLASSIFIER STRESS TEST - FINAL SUMMARY\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"Test completed at: {datetime.now().isoformat()}\n")
        f.write(f"Total duration: {total_time/3600:.2f} hours\n\n")

        f.write("OVERALL STATISTICS\n")
        f.write("-" * 40 + "\n")
        f.write(f"Total classifications: {stats['total_classifications']:,}\n")
        f.write(f"Correct: {stats['correct']:,}\n")
        f.write(f"Incorrect: {stats['incorrect']:,}\n")
        f.write(f"Edge cases: {stats['edge_cases']:,}\n")
        f.write(f"Errors: {stats['errors']:,}\n")
        f.write(f"Overall accuracy: {stats['accuracy']:.2f}%\n\n")

        f.write("PERFORMANCE\n")
        f.write("-" * 40 + "\n")
        f.write(f"Average response time: {stats['avg_time_ms']:.2f}ms\n")
        f.write(f"Min response time: {stats['min_time_ms']:.2f}ms\n")
        f.write(f"Max response time: {stats['max_time_ms']:.2f}ms\n")
        f.write(f"Throughput: {stats['throughput']:.2f} req/sec\n\n")

        f.write("CATEGORY-WISE ACCURACY\n")
        f.write("-" * 40 + "\n")
        for cat, cat_stats in stats["by_category"].items():
            f.write(f"  {cat}: {cat_stats['accuracy']:.2f}% ({cat_stats['correct']}/{cat_stats['total']})\n")
        f.write("\n")

        f.write("CONSISTENCY\n")
        f.write("-" * 40 + "\n")
        f.write(f"Consistency failures: {stats['consistency_failures']}\n\n")

        f.write("FILES\n")
        f.write("-" * 40 + "\n")
        f.write(f"Full results: {RESULTS_FILE}\n")
        f.write(f"Log file: {LOG_FILE}\n")

    log("")
    log("=" * 70)
    log("STRESS TEST COMPLETED")
    log("=" * 70)
    log(f"Total classifications: {stats['total_classifications']:,}")
    log(f"Accuracy: {stats['accuracy']:.2f}%")
    log(f"Throughput: {stats['throughput']:.2f} req/sec")
    log(f"Summary saved to: {SUMMARY_FILE}")


if __name__ == "__main__":
    run_stress_test()

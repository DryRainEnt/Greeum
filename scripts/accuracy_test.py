#!/usr/bin/env python3
"""
Greeum Accuracy Stress Test
Tests precision and recall of v5.0 components

Usage:
    .venv_test/bin/python scripts/accuracy_test.py
"""

import os
import sys
import tempfile
import random
from typing import List, Dict, Tuple, Set
from dataclasses import dataclass

# Add parent to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from greeum.core import (
    BlockManager,
    DatabaseManager,
    BM25Index,
    HybridScorer,
    InsightFilter,
)
from greeum.text_utils import generate_simple_embedding


@dataclass
class AccuracyResult:
    name: str
    total: int
    correct: int
    false_positives: int = 0
    false_negatives: int = 0

    @property
    def accuracy(self) -> float:
        return self.correct / self.total if self.total > 0 else 0

    @property
    def precision(self) -> float:
        tp = self.correct
        fp = self.false_positives
        return tp / (tp + fp) if (tp + fp) > 0 else 0

    @property
    def recall(self) -> float:
        tp = self.correct
        fn = self.false_negatives
        return tp / (tp + fn) if (tp + fn) > 0 else 0

    @property
    def f1(self) -> float:
        p, r = self.precision, self.recall
        return 2 * p * r / (p + r) if (p + r) > 0 else 0

    def print_summary(self):
        print(f"\n{'='*60}")
        print(f"üìä {self.name}")
        print(f"{'='*60}")
        print(f"  Total: {self.total}")
        print(f"  Correct: {self.correct}")
        print(f"  Accuracy: {self.accuracy*100:.1f}%")
        if self.false_positives > 0 or self.false_negatives > 0:
            print(f"  Precision: {self.precision*100:.1f}%")
            print(f"  Recall: {self.recall*100:.1f}%")
            print(f"  F1 Score: {self.f1*100:.1f}%")


# ============================================================
# Test Data - Ground Truth
# ============================================================

# InsightFilter test cases: (content, expected_is_insight)
INSIGHT_TEST_CASES = [
    # TRUE POSITIVES - Should be detected as insights
    ("PostgreSQL Ïù∏Îç±Ïä§ ÌäúÎãùÏúºÎ°ú ÏøºÎ¶¨ ÏÜçÎèÑ 3Î∞∞ Ìñ•ÏÉÅÏãúÏº∞Îã§", True),
    ("Docker Î©îÎ™®Î¶¨ ÎàÑÏàò ÏõêÏù∏ÏùÑ Ï∞æÏïÑÏÑú Ìï¥Í≤∞ÌñàÎã§", True),
    ("React Ïª¥Ìè¨ÎÑåÌä∏ÏóêÏÑú useMemoÎ•º Ïç®ÏÑú Î¶¨Î†åÎçîÎßÅ Î¨∏Ï†úÎ•º Í≥†Ï≥§Îã§", True),
    ("API ÌÉÄÏûÑÏïÑÏõÉ Ïù¥ÏäàÎäî Ïª§ÎÑ•ÏÖò ÌíÄ ÏÑ§Ï†ïÏù¥ Î¨∏Ï†úÏòÄÎã§", True),
    ("Git rebase ÎåÄÏã† mergeÎ•º ÏÑ†ÌÉùÌïú Ïù¥Ïú†Îäî ÌûàÏä§ÌÜ†Î¶¨ Î≥¥Ï°¥ ÎïåÎ¨∏", True),
    ("Redis Ï∫êÏãú TTLÏùÑ 5Î∂ÑÏúºÎ°ú ÏÑ§Ï†ïÌñàÎçîÎãà ÌûàÌä∏Ïú®Ïù¥ Ïò¨ÎùºÍ∞îÎã§", True),
    ("ÌÖåÏä§Ìä∏ Ïª§Î≤ÑÎ¶¨ÏßÄ 80% Îã¨ÏÑ±ÌïòÎ†§Î©¥ edge case Ï∂îÍ∞Ä ÌïÑÏöî", True),
    ("Ïù¥ Î≤ÑÍ∑∏Îäî async/await ÎàÑÎùΩÏù¥ ÏõêÏù∏Ïù¥ÏóàÎã§", True),
    ("Î∞∞Ìè¨ Ïã§Ìå® ÏõêÏù∏: ÌôòÍ≤ΩÎ≥ÄÏàò ÎØ∏ÏÑ§Ï†ï", True),
    ("ÏÑ±Îä• ÌîÑÎ°úÌååÏùºÎßÅ Í≤∞Í≥º DB ÏøºÎ¶¨Í∞Ä Î≥ëÎ™©Ïù¥ÏóàÎã§", True),
    ("JWT ÌÜ†ÌÅ∞ Í∞±Ïã† Î°úÏßÅÏóêÏÑú Î†àÏù¥Ïä§ Ïª®ÎîîÏÖò Î∞úÍ≤¨", True),
    ("Î°úÎìúÎ∞∏Îü∞ÏÑú ÏÑ§Ï†ï Î≥ÄÍ≤ΩÏúºÎ°ú ÏßÄÏó∞ÏãúÍ∞Ñ 50% Í∞êÏÜå", True),
    ("Fixed the authentication bug by adding proper error handling", True),
    ("Discovered that the memory leak was caused by unclosed connections", True),
    ("Decided to use PostgreSQL instead of MySQL for better JSON support", True),
    ("The solution was to add an index on the user_id column", True),
    ("Learned that React hooks must be called at the top level", True),
    ("Ï£ºÏùò: Ïù¥ APIÎäî rate limitÏù¥ ÏûàÏúºÎãà Ï∫êÏã± ÌïÑÏàò", True),
    ("Ï§ëÏöîÌïú ÍµêÌõà: ÌîÑÎ°úÎçïÏÖòÏóêÏÑú DEBUG Î™®Îìú ÎÅÑÍ∏∞", True),
    ("webpack ÏÑ§Ï†ïÏóêÏÑú tree shaking ÌôúÏÑ±ÌôîÌïòÎãà Î≤àÎì§ ÌÅ¨Í∏∞ 30% Í∞êÏÜå", True),

    # TRUE NEGATIVES - Should NOT be detected as insights
    ("ÎÑ§", False),
    ("ÏïåÍ≤†ÏäµÎãàÎã§", False),
    ("Ïùå", False),
    ("Í∑∏Î†áÍµ∞Ïöî", False),
    ("Ïò§ÏºÄÏù¥", False),
    ("ÎÑ§ ÎßûÏïÑÏöî", False),
    ("ÏïàÎÖïÌïòÏÑ∏Ïöî", False),
    ("Í∞êÏÇ¨Ìï©ÎãàÎã§", False),
    ("Ï¢ãÏïÑÏöî", False),
    ("„Öã„Öã„Öã", False),
    ("„Öé„Öé", False),
    ("?", False),
    ("ok", False),
    ("yes", False),
    ("thanks", False),
    ("hello", False),
    ("sure", False),
    ("got it", False),
    ("hmm", False),
    ("I see", False),
    ("Ïò§Îäò ÎÇ†Ïî® Ï¢ãÎã§", False),
    ("Ï†êÏã¨ Î≠ê Î®πÏßÄ", False),
    ("Ïª§Ìîº ÎßàÏãúÎü¨ Í∞ÄÏûê", False),
    ("ÌöåÏùò ÏãúÍ∞ÑÏù¥Ïïº", False),
    ("Ïû†ÍπêÎßåÏöî", False),
]

# BM25 Search test cases: (query_keywords, expected_doc_ids in top-3)
# We'll build a corpus and test retrieval
BM25_CORPUS = [
    {"id": "doc1", "keywords": ["PostgreSQL", "Ïù∏Îç±Ïä§", "ÏøºÎ¶¨", "ÌäúÎãù", "ÏÑ±Îä•"]},
    {"id": "doc2", "keywords": ["React", "Ïª¥Ìè¨ÎÑåÌä∏", "Î†åÎçîÎßÅ", "ÏµúÏ†ÅÌôî", "useMemo"]},
    {"id": "doc3", "keywords": ["Docker", "Ïª®ÌÖåÏù¥ÎÑà", "Î©îÎ™®Î¶¨", "ÎàÑÏàò", "ÎîîÎ≤ÑÍπÖ"]},
    {"id": "doc4", "keywords": ["Redis", "Ï∫êÏãú", "TTL", "ÏÑ§Ï†ï", "ÌûàÌä∏Ïú®"]},
    {"id": "doc5", "keywords": ["API", "ÌÉÄÏûÑÏïÑÏõÉ", "Ïª§ÎÑ•ÏÖò", "ÌíÄ", "ÏÑ§Ï†ï"]},
    {"id": "doc6", "keywords": ["Git", "rebase", "merge", "Î∏åÎûúÏπò", "ÌûàÏä§ÌÜ†Î¶¨"]},
    {"id": "doc7", "keywords": ["ÌÖåÏä§Ìä∏", "Ïª§Î≤ÑÎ¶¨ÏßÄ", "Ïú†Îãõ", "ÌÜµÌï©", "edge"]},
    {"id": "doc8", "keywords": ["JWT", "ÌÜ†ÌÅ∞", "Ïù∏Ï¶ù", "Í∞±Ïã†", "Î≥¥Ïïà"]},
    {"id": "doc9", "keywords": ["Kubernetes", "Î∞∞Ìè¨", "ÌååÎìú", "ÏÑúÎπÑÏä§", "Ïä§ÏºÄÏùºÎßÅ"]},
    {"id": "doc10", "keywords": ["webpack", "Î≤àÎì§", "tree", "shaking", "ÏµúÏ†ÅÌôî"]},
]

BM25_QUERIES = [
    (["PostgreSQL", "ÏøºÎ¶¨", "ÏÑ±Îä•"], {"doc1"}),  # Should find doc1
    (["React", "Î†åÎçîÎßÅ"], {"doc2"}),  # Should find doc2
    (["Docker", "Î©îÎ™®Î¶¨"], {"doc3"}),  # Should find doc3
    (["Ï∫êÏãú", "Redis"], {"doc4"}),  # Should find doc4
    (["API", "ÌÉÄÏûÑÏïÑÏõÉ"], {"doc5"}),  # Should find doc5
    (["Git", "Î∏åÎûúÏπò"], {"doc6"}),  # Should find doc6
    (["ÌÖåÏä§Ìä∏", "Ïª§Î≤ÑÎ¶¨ÏßÄ"], {"doc7"}),  # Should find doc7
    (["JWT", "Ïù∏Ï¶ù"], {"doc8"}),  # Should find doc8
    (["Kubernetes", "Î∞∞Ìè¨"], {"doc9"}),  # Should find doc9
    (["webpack", "Î≤àÎì§"], {"doc10"}),  # Should find doc10
    (["ÏµúÏ†ÅÌôî", "ÏÑ±Îä•"], {"doc1", "doc2", "doc10"}),  # Multiple matches
    (["ÏÑ§Ï†ï", "Redis"], {"doc4", "doc5"}),  # Multiple matches
]

# Hybrid scoring test: vector + BM25 should rank better than either alone
HYBRID_TEST_CASES = [
    # (vector_sim, query_kw, doc_kw, expected_high_score)
    # High vector + high BM25 = highest
    (0.9, ["PostgreSQL", "ÏøºÎ¶¨"], ["PostgreSQL", "ÏøºÎ¶¨", "ÌäúÎãù"], True),
    # High vector + low BM25 = medium
    (0.9, ["React"], ["Docker", "Ïª®ÌÖåÏù¥ÎÑà"], False),
    # Low vector + high BM25 = medium
    (0.3, ["Redis", "Ï∫êÏãú"], ["Redis", "Ï∫êÏãú", "TTL"], False),
    # Low vector + low BM25 = lowest
    (0.1, ["API"], ["webpack", "Î≤àÎì§"], False),
]


def test_insight_filter_accuracy() -> AccuracyResult:
    """Test InsightFilter classification accuracy"""
    result = AccuracyResult("InsightFilter Accuracy", total=0, correct=0)
    filter_inst = InsightFilter()

    for content, expected in INSIGHT_TEST_CASES:
        result.total += 1
        filter_result = filter_inst.filter(content)
        predicted = filter_result.is_insight

        if predicted == expected:
            result.correct += 1
        elif predicted and not expected:
            result.false_positives += 1
        elif not predicted and expected:
            result.false_negatives += 1
            print(f"  ‚ùå FN: '{content[:40]}...' (expected insight)")

    return result


def test_bm25_search_accuracy() -> AccuracyResult:
    """Test BM25 search retrieval accuracy"""
    result = AccuracyResult("BM25 Search Accuracy", total=0, correct=0)

    # Build index
    bm25 = BM25Index()
    for doc in BM25_CORPUS:
        bm25.add_document(doc["id"], doc["keywords"])

    for query_keywords, expected_ids in BM25_QUERIES:
        result.total += 1
        search_results = bm25.search(query_keywords, top_k=3)
        found_ids = {doc_id for doc_id, _ in search_results}

        # Check if any expected doc is in top-3
        if found_ids & expected_ids:
            result.correct += 1
        else:
            result.false_negatives += 1
            print(f"  ‚ùå Miss: query={query_keywords}, expected={expected_ids}, got={found_ids}")

    return result


def test_bm25_ranking_accuracy() -> AccuracyResult:
    """Test BM25 ranking correctness (relevant docs should rank higher)"""
    result = AccuracyResult("BM25 Ranking Accuracy", total=0, correct=0)

    # Build index
    bm25 = BM25Index()
    for doc in BM25_CORPUS:
        bm25.add_document(doc["id"], doc["keywords"])

    for query_keywords, expected_ids in BM25_QUERIES:
        result.total += 1
        search_results = bm25.search(query_keywords, top_k=10)

        if not search_results:
            result.false_negatives += 1
            continue

        # Check if top result is in expected
        top_doc_id = search_results[0][0]
        if top_doc_id in expected_ids:
            result.correct += 1
        else:
            # Check if expected doc exists but ranked lower
            found_ranks = {doc_id: rank for rank, (doc_id, _) in enumerate(search_results)}
            for exp_id in expected_ids:
                if exp_id in found_ranks:
                    print(f"  ‚ö†Ô∏è Rank: query={query_keywords}, expected {exp_id} at rank {found_ranks[exp_id]+1}")
            result.false_negatives += 1

    return result


def test_hybrid_scorer_accuracy() -> AccuracyResult:
    """Test HybridScorer fusion correctness"""
    result = AccuracyResult("HybridScorer Accuracy", total=0, correct=0)

    # Build BM25 index for scorer
    bm25 = BM25Index()
    for doc in BM25_CORPUS:
        bm25.add_document(doc["id"], doc["keywords"])

    scorer = HybridScorer(bm25_index=bm25, vector_weight=0.5, bm25_weight=0.5)

    scores = []
    for vec_sim, query_kw, doc_kw, _ in HYBRID_TEST_CASES:
        hybrid_score = scorer.score(vec_sim, query_kw, doc_kw)
        scores.append(hybrid_score)

    # Test 1: High vec + high BM25 should be highest
    result.total += 1
    if scores[0] > scores[1] and scores[0] > scores[2] and scores[0] > scores[3]:
        result.correct += 1
    else:
        print(f"  ‚ùå High+High not highest: scores={scores}")

    # Test 2: Low vec + low BM25 should be lowest
    result.total += 1
    if scores[3] < scores[0] and scores[3] < scores[1] and scores[3] < scores[2]:
        result.correct += 1
    else:
        print(f"  ‚ùå Low+Low not lowest: scores={scores}")

    # Test 3: Mixing should give medium scores
    result.total += 1
    avg_mixed = (scores[1] + scores[2]) / 2
    if scores[0] > avg_mixed > scores[3]:
        result.correct += 1
    else:
        print(f"  ‚ùå Mixed not in middle: high={scores[0]}, mixed_avg={avg_mixed}, low={scores[3]}")

    return result


def test_insight_filter_edge_cases() -> AccuracyResult:
    """Test InsightFilter on edge cases"""
    result = AccuracyResult("InsightFilter Edge Cases", total=0, correct=0)
    filter_inst = InsightFilter()

    edge_cases = [
        # Short but valuable
        ("Î≤ÑÍ∑∏ ÏàòÏ†ïÌï®", True),
        ("fixed bug", True),
        # Long but not valuable
        ("Ïò§Îäò ÌïòÎ£® Ï¢ÖÏùº ÌöåÏùòÎßå ÌñàÎäîÎç∞ Ï†ïÎßê ÌîºÍ≥§ÌïòÎÑ§Ïöî Ïª§Ìîº ÎßàÏãúÎü¨ Í∞ÄÏïºÍ≤†Ïñ¥Ïöî", False),
        # Mixed language
        ("ReactÏóêÏÑú useMemoÎ°ú performance Í∞úÏÑ†ÌñàÏùå", True),
        # Technical terms without insight
        ("Docker Redis Kubernetes PostgreSQL", False),
        # Question (usually not insight)
        ("Ïù¥Í±∞ Ïôú Ïïà ÎêòÏßÄ?", False),
        # Code-like content
        ("def fix_bug(): pass", False),
        # Numbered insight
        ("1. Ï∫êÏãú TTL 5Î∂ÑÏúºÎ°ú ÏÑ§Ï†ï 2. ÌûàÌä∏Ïú® 90% Îã¨ÏÑ±", True),
    ]

    for content, expected in edge_cases:
        result.total += 1
        filter_result = filter_inst.filter(content)
        predicted = filter_result.is_insight

        if predicted == expected:
            result.correct += 1
        else:
            marker = "FP" if predicted else "FN"
            print(f"  ‚ùå {marker}: '{content[:40]}...'")
            if predicted:
                result.false_positives += 1
            else:
                result.false_negatives += 1

    return result


def test_bm25_partial_match() -> AccuracyResult:
    """Test BM25 with partial keyword matches"""
    result = AccuracyResult("BM25 Partial Match", total=0, correct=0)

    bm25 = BM25Index()
    for doc in BM25_CORPUS:
        bm25.add_document(doc["id"], doc["keywords"])

    partial_queries = [
        # Single keyword should still find relevant docs
        (["PostgreSQL"], {"doc1"}),
        (["Docker"], {"doc3"}),
        (["Ï∫êÏãú"], {"doc4"}),
        # Typo-like (different but related)
        (["Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§", "ÏøºÎ¶¨"], {"doc1"}),  # Related to PostgreSQL
    ]

    for query_keywords, expected_ids in partial_queries:
        result.total += 1
        search_results = bm25.search(query_keywords, top_k=5)
        found_ids = {doc_id for doc_id, _ in search_results}

        if found_ids & expected_ids:
            result.correct += 1
        else:
            # Partial match might not always work
            pass

    return result


def run_accuracy_tests():
    """Run all accuracy tests"""
    print("üéØ Greeum Accuracy Stress Test")
    print("=" * 60)

    results: List[AccuracyResult] = []

    # Test 1: InsightFilter accuracy
    print("\n[1/6] Testing InsightFilter classification...")
    results.append(test_insight_filter_accuracy())

    # Test 2: InsightFilter edge cases
    print("\n[2/6] Testing InsightFilter edge cases...")
    results.append(test_insight_filter_edge_cases())

    # Test 3: BM25 search accuracy
    print("\n[3/6] Testing BM25 search retrieval...")
    results.append(test_bm25_search_accuracy())

    # Test 4: BM25 ranking accuracy
    print("\n[4/6] Testing BM25 ranking correctness...")
    results.append(test_bm25_ranking_accuracy())

    # Test 5: BM25 partial match
    print("\n[5/6] Testing BM25 partial match...")
    results.append(test_bm25_partial_match())

    # Test 6: HybridScorer accuracy
    print("\n[6/6] Testing HybridScorer fusion...")
    results.append(test_hybrid_scorer_accuracy())

    # Print results
    print("\n" + "=" * 60)
    print("üìà RESULTS SUMMARY")
    print("=" * 60)

    for r in results:
        r.print_summary()

    # Overall summary
    print("\n" + "=" * 60)
    print("üèÅ OVERALL")
    print("=" * 60)

    total_tests = sum(r.total for r in results)
    total_correct = sum(r.correct for r in results)
    overall_accuracy = total_correct / total_tests if total_tests > 0 else 0

    print(f"  Total tests: {total_tests}")
    print(f"  Total correct: {total_correct}")
    print(f"  Overall accuracy: {overall_accuracy*100:.1f}%")

    # Grade
    if overall_accuracy >= 0.95:
        grade = "A+ (Excellent)"
    elif overall_accuracy >= 0.90:
        grade = "A (Very Good)"
    elif overall_accuracy >= 0.80:
        grade = "B (Good)"
    elif overall_accuracy >= 0.70:
        grade = "C (Acceptable)"
    else:
        grade = "D (Needs Improvement)"

    print(f"\n  Accuracy Grade: {grade}")

    # Detailed breakdown
    print("\n" + "-" * 60)
    print("üìã DETAILED BREAKDOWN")
    print("-" * 60)
    for r in results:
        status = "‚úÖ" if r.accuracy >= 0.8 else "‚ö†Ô∏è" if r.accuracy >= 0.6 else "‚ùå"
        print(f"  {status} {r.name}: {r.accuracy*100:.0f}% ({r.correct}/{r.total})")


if __name__ == "__main__":
    run_accuracy_tests()

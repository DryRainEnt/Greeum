#!/usr/bin/env python3
"""
ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì „ìš© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ì¼ë°˜ ê°œë°œ ì›Œí¬í”Œë¡œìš°ì™€ ë¶„ë¦¬í•˜ì—¬ ì‹¤í–‰
"""

import subprocess
import sys
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

class PerformanceTestRunner:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì „ìš© ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.results = {}
        
    def run_performance_tests(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        print("=" * 60)
        
        start_time = time.time()
        
        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
        cmd = [
            "pytest",
            "-m", "performance",
            "-v",
            "--tb=short",
            "--durations=10",  # ê°€ì¥ ì˜¤ë˜ ê±¸ë¦° 10ê°œ í…ŒìŠ¤íŠ¸ í‘œì‹œ
            "--maxfail=5",     # 5ê°œ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
            "--timeout=600"    # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        ]
        
        try:
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=1800  # 30ë¶„ ì „ì²´ íƒ€ì„ì•„ì›ƒ
            )
            
            elapsed_time = time.time() - start_time
            
            return {
                'command': ' '.join(cmd),
                'return_code': result.returncode,
                'elapsed_time': elapsed_time,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'success': result.returncode == 0,
                'timestamp': datetime.now().isoformat()
            }
            
        except subprocess.TimeoutExpired:
            elapsed_time = time.time() - start_time
            return {
                'command': ' '.join(cmd),
                'return_code': -1,
                'elapsed_time': elapsed_time,
                'stdout': '',
                'stderr': f'Timeout after {elapsed_time:.2f} seconds',
                'success': False,
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            elapsed_time = time.time() - start_time
            return {
                'command': ' '.join(cmd),
                'return_code': -1,
                'elapsed_time': elapsed_time,
                'stdout': '',
                'stderr': str(e),
                'success': False,
                'timestamp': datetime.now().isoformat()
            }
    
    def run_benchmark_tests(self) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        print("-" * 40)
        
        start_time = time.time()
        
        # ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        benchmark_scripts = [
            "benchmark/performance_benchmark.py",
            "benchmark/optimized_benchmark.py",
            "benchmark/ultra_optimized_benchmark.py"
        ]
        
        results = []
        
        for script in benchmark_scripts:
            script_path = self.project_root / script
            if script_path.exists():
                print(f"ì‹¤í–‰ ì¤‘: {script}")
                
                try:
                    result = subprocess.run(
                        [sys.executable, str(script_path)],
                        cwd=self.project_root,
                        capture_output=True,
                        text=True,
                        timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                    )
                    
                    results.append({
                        'script': script,
                        'return_code': result.returncode,
                        'stdout': result.stdout,
                        'stderr': result.stderr,
                        'success': result.returncode == 0
                    })
                    
                except subprocess.TimeoutExpired:
                    results.append({
                        'script': script,
                        'return_code': -1,
                        'stdout': '',
                        'stderr': 'Timeout',
                        'success': False
                    })
                except Exception as e:
                    results.append({
                        'script': script,
                        'return_code': -1,
                        'stdout': '',
                        'stderr': str(e),
                        'success': False
                    })
        
        elapsed_time = time.time() - start_time
        
        return {
            'type': 'benchmark',
            'elapsed_time': elapsed_time,
            'results': results,
            'success': all(r['success'] for r in results),
            'timestamp': datetime.now().isoformat()
        }
    
    def generate_performance_report(self, pytest_result: Dict[str, Any], benchmark_result: Dict[str, Any]) -> str:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 80)
        report.append("ğŸš€ GREEUM ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸")
        report.append("=" * 80)
        report.append(f"ì‹¤í–‰ ì‹œê°„: {pytest_result['timestamp']}")
        report.append("")
        
        # pytest ê²°ê³¼
        report.append("ğŸ“‹ pytest ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        report.append("-" * 40)
        status = "âœ… ì„±ê³µ" if pytest_result['success'] else "âŒ ì‹¤íŒ¨"
        report.append(f"ìƒíƒœ: {status}")
        report.append(f"ì‹¤í–‰ ì‹œê°„: {pytest_result['elapsed_time']:.2f}ì´ˆ")
        report.append(f"ëª…ë ¹ì–´: {pytest_result['command']}")
        
        if not pytest_result['success']:
            report.append(f"ì—ëŸ¬: {pytest_result['stderr'][:300]}...")
        
        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        if benchmark_result:
            report.append("\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            report.append("-" * 40)
            report.append(f"ì´ ì‹¤í–‰ ì‹œê°„: {benchmark_result['elapsed_time']:.2f}ì´ˆ")
            
            for result in benchmark_result['results']:
                status = "âœ… ì„±ê³µ" if result['success'] else "âŒ ì‹¤íŒ¨"
                report.append(f"  {status} {result['script']}")
                if not result['success']:
                    report.append(f"    ì—ëŸ¬: {result['stderr'][:200]}...")
        
        # ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ (stdoutì—ì„œ)
        if pytest_result['stdout']:
            report.append("\nğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:")
            report.append("-" * 40)
            
            # ê°€ì¥ ì˜¤ë˜ ê±¸ë¦° í…ŒìŠ¤íŠ¸ë“¤ ì¶”ì¶œ
            lines = pytest_result['stdout'].split('\n')
            duration_lines = [line for line in lines if 'slowest' in line.lower() or 'durations' in line.lower()]
            
            for line in duration_lines[:5]:  # ìƒìœ„ 5ê°œë§Œ
                report.append(f"  {line.strip()}")
        
        return "\n".join(report)
    
    def save_results(self, pytest_result: Dict[str, Any], benchmark_result: Dict[str, Any] = None):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"performance_test_results_{timestamp}.json"
        
        results_file = self.project_root / "test_results" / filename
        results_file.parent.mkdir(exist_ok=True)
        
        data = {
            'pytest_results': pytest_result,
            'benchmark_results': benchmark_result,
            'summary': {
                'total_elapsed_time': pytest_result['elapsed_time'] + (benchmark_result['elapsed_time'] if benchmark_result else 0),
                'pytest_success': pytest_result['success'],
                'benchmark_success': benchmark_result['success'] if benchmark_result else None,
                'overall_success': pytest_result['success'] and (benchmark_result['success'] if benchmark_result else True)
            }
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“ ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return results_file

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Greeum ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    parser.add_argument("--benchmark", action="store_true", help="ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ë„ í•¨ê»˜ ì‹¤í–‰")
    parser.add_argument("--save-results", action="store_true", help="ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥")
    
    args = parser.parse_args()
    
    runner = PerformanceTestRunner()
    
    print("ğŸš€ Greeum ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ì‹œì‘")
    print(f"ë²¤ì¹˜ë§ˆí¬ í¬í•¨: {'ì˜ˆ' if args.benchmark else 'ì•„ë‹ˆì˜¤'}")
    
    # pytest ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    pytest_result = runner.run_performance_tests()
    
    # ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì˜µì…˜)
    benchmark_result = None
    if args.benchmark:
        benchmark_result = runner.run_benchmark_tests()
    
    # ê²°ê³¼ ì¶œë ¥
    print(runner.generate_performance_report(pytest_result, benchmark_result))
    
    # ê²°ê³¼ ì €ì¥
    if args.save_results:
        runner.save_results(pytest_result, benchmark_result)
    
    # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¢…ë£Œ ì½”ë“œ 1 ë°˜í™˜
    overall_success = pytest_result['success'] and (benchmark_result['success'] if benchmark_result else True)
    
    if not overall_success:
        print("\nâŒ ì¼ë¶€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    else:
        print("\nğŸ‰ ëª¨ë“  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        sys.exit(0)

if __name__ == "__main__":
    main()

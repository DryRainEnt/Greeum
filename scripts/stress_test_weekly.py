#!/usr/bin/env python3
"""
Greeum v2.0.4 ì£¼ê°„ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ê²€ì¦
- ì¥ì‹œê°„ ì‹¤í–‰ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
- ê·¹í•œ ìƒí™©ì—ì„œì˜ ë³µêµ¬ ëŠ¥ë ¥ ê²€ì¦
"""

import time
import psutil
import gc
import json
import sys
import threading
import queue
import random
import string
import tempfile
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from contextlib import contextmanager
from typing import Dict, List, Any, Optional
import concurrent.futures

# Greeum ëª¨ë“ˆ import
sys.path.insert(0, str(Path(__file__).parent.parent))
from greeum.core.database_manager import DatabaseManager
from greeum.core.block_manager import BlockManager
from greeum.text_utils import process_user_input
from greeum.embedding_models import get_embedding


class StressTestSuite:
    """ì£¼ê°„ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "version": "2.0.4",
            "platform": sys.platform,
            "python_version": sys.version,
            "stress_tests": {},
            "system_info": self._get_system_info()
        }
        
        # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ ë””ë ‰í† ë¦¬
        self.test_dir = Path(tempfile.mkdtemp(prefix="greeum_stress_"))
        print(f"ğŸ“ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬: {self.test_dir}")
        
        # ë‹¤ì–‘í•œ ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        self.primary_db = DatabaseManager(str(self.test_dir / "primary.db"))
        self.secondary_db = DatabaseManager(str(self.test_dir / "secondary.db"))
        
        self.primary_bm = BlockManager(self.primary_db)
        self.secondary_bm = BlockManager(self.secondary_db)
    
    def _get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        return {
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": round(psutil.virtual_memory().total / 1024**3, 2),
            "disk_free_gb": round(shutil.disk_usage("/").free / 1024**3, 2),
            "python_version": sys.version.split()[0]
        }
    
    @contextmanager
    def monitor_resources(self, test_name: str, duration_hint: Optional[int] = None):
        """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ì»¨í…ìŠ¤íŠ¸"""
        start_time = time.perf_counter()
        process = psutil.Process()
        
        # ì‹œì‘ ìƒíƒœ
        start_stats = {
            "cpu_percent": process.cpu_percent(),
            "memory_mb": process.memory_info().rss / 1024 / 1024,
            "open_files": len(process.open_files()) if hasattr(process, 'open_files') else 0
        }
        
        # ëª¨ë‹ˆí„°ë§ ë°ì´í„° ìˆ˜ì§‘ìš©
        monitoring_data = {
            "peak_memory_mb": start_stats["memory_mb"],
            "peak_cpu_percent": start_stats["cpu_percent"],
            "memory_samples": [start_stats["memory_mb"]],
            "cpu_samples": [start_stats["cpu_percent"]]
        }
        
        # ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        stop_monitoring = threading.Event()
        
        def monitor_worker():
            while not stop_monitoring.wait(1.0):  # 1ì´ˆë§ˆë‹¤ ìƒ˜í”Œë§
                try:
                    current_memory = process.memory_info().rss / 1024 / 1024
                    current_cpu = process.cpu_percent()
                    
                    monitoring_data["memory_samples"].append(current_memory)
                    monitoring_data["cpu_samples"].append(current_cpu)
                    
                    monitoring_data["peak_memory_mb"] = max(
                        monitoring_data["peak_memory_mb"], current_memory
                    )
                    monitoring_data["peak_cpu_percent"] = max(
                        monitoring_data["peak_cpu_percent"], current_cpu
                    )
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    break
        
        monitor_thread = threading.Thread(target=monitor_worker, daemon=True)
        monitor_thread.start()
        
        try:
            yield monitoring_data
        finally:
            # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            stop_monitoring.set()
            monitor_thread.join(timeout=2)
            
            # ì¢…ë£Œ ìƒíƒœ
            end_time = time.perf_counter()
            end_stats = {
                "cpu_percent": process.cpu_percent(),
                "memory_mb": process.memory_info().rss / 1024 / 1024,
                "open_files": len(process.open_files()) if hasattr(process, 'open_files') else 0
            }
            
            # ê²°ê³¼ ì €ì¥
            self.results["stress_tests"][test_name] = {
                "duration_seconds": round(end_time - start_time, 2),
                "start_stats": start_stats,
                "end_stats": end_stats,
                "peak_memory_mb": round(monitoring_data["peak_memory_mb"], 2),
                "peak_cpu_percent": round(monitoring_data["peak_cpu_percent"], 2),
                "avg_memory_mb": round(sum(monitoring_data["memory_samples"]) / len(monitoring_data["memory_samples"]), 2),
                "avg_cpu_percent": round(sum(monitoring_data["cpu_samples"]) / len(monitoring_data["cpu_samples"]), 2),
                "memory_leak_mb": round(end_stats["memory_mb"] - start_stats["memory_mb"], 2),
                "file_leak_count": end_stats["open_files"] - start_stats["open_files"]
            }
            
            print(f"âœ… {test_name}: {self.results['stress_tests'][test_name]['duration_seconds']}ì´ˆ, "
                  f"í”¼í¬ ë©”ëª¨ë¦¬: {self.results['stress_tests'][test_name]['peak_memory_mb']:.1f}MB")
    
    def generate_test_data(self, count: int, complexity: str = "medium") -> List[str]:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        templates = {
            "simple": [
                "ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° {i}",
                "Simple test data {i}",
                "í…ŒìŠ¤íŠ¸ {i}"
            ],
            "medium": [
                "ì¤‘ê°„ ë³µì¡ë„ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…ë‹ˆë‹¤. ë²ˆí˜¸: {i}, í•œê¸€ê³¼ ì˜ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                "Medium complexity test data with number {i}. Contains Korean and English text.",
                "ë³µí•© ì–¸ì–´ í…ŒìŠ¤íŠ¸ ãƒ‡ãƒ¼ã‚¿ {i} - í•œê¸€/English/æ—¥æœ¬èª í˜¼í•©",
                "ê¸°ìˆ ì  ë‚´ìš©: ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”, ì¸ë±ì‹± ì „ëµ, ì¿¼ë¦¬ ìµœì í™” ë°©ë²•ë¡  {i}"
            ],
            "complex": [
                "ë§¤ìš° ë³µì¡í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…ë‹ˆë‹¤. " * 10 + f" ë²ˆí˜¸: {i}",
                "Complex multilingual test data with extensive content. " * 5 + f" Number: {i}",
                "ğŸ¯ ì´ëª¨ì§€ì™€ íŠ¹ìˆ˜ë¬¸ì @#$%^&*()_+ í¬í•¨ í…ŒìŠ¤íŠ¸ ë°ì´í„° {i} âœ¨ğŸš€ğŸ’¡",
                "JSON í˜•íƒœ ë°ì´í„°: " + json.dumps({"id": "{i}", "content": "ë³µì¡í•œ êµ¬ì¡°ì˜ ë°ì´í„°", "metadata": {"tags": ["test", "stress"], "importance": 0.8}})
            ]
        }
        
        template_list = templates.get(complexity, templates["medium"])
        test_data = []
        
        for i in range(count):
            template = random.choice(template_list)
            test_data.append(template.format(i=i))
        
        return test_data
    
    def stress_test_massive_memory_blocks(self, block_counts: List[int] = [1000, 5000, 10000]):
        """ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ ë¸”ë¡ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        for count in block_counts:
            test_name = f"massive_blocks_{count}"
            print(f"ğŸ”¥ ëŒ€ìš©ëŸ‰ ë¸”ë¡ í…ŒìŠ¤íŠ¸ ì‹œì‘: {count}ê°œ")
            
            with self.monitor_resources(test_name):
                test_data = self.generate_test_data(count, "medium")
                
                # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
                batch_size = 100
                successful_inserts = 0
                
                for i in range(0, count, batch_size):
                    batch = test_data[i:i + batch_size]
                    
                    for j, content in enumerate(batch):
                        try:
                            result = process_user_input(content)
                            block_data = {
                                "block_index": i + j,
                                "timestamp": datetime.now().isoformat(),
                                "context": content,
                                "keywords": result.get("keywords", []),
                                "tags": result.get("tags", []),
                                "embedding": result.get("embedding", []),
                                "importance": random.uniform(0.1, 0.9),
                                "hash": f"stress_{i+j}",
                                "prev_hash": f"stress_{i+j-1}" if i+j > 0 else ""
                            }
                            self.primary_db.add_block(block_data)
                            successful_inserts += 1
                            
                        except Exception as e:
                            print(f"   ë¸”ë¡ {i+j} ì¶”ê°€ ì‹¤íŒ¨: {e}")
                    
                    # ë°°ì¹˜ë§ˆë‹¤ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                    if i % (batch_size * 10) == 0:
                        gc.collect()
                        print(f"   ì§„í–‰: {i+len(batch)}/{count} ({((i+len(batch))/count*100):.1f}%)")
                
                # ê²°ê³¼ì— ì„±ê³µë¥  ì¶”ê°€
                self.results["stress_tests"][test_name]["successful_inserts"] = successful_inserts
                self.results["stress_tests"][test_name]["success_rate"] = round(successful_inserts / count * 100, 2)
    
    def stress_test_concurrent_access(self, worker_count: int = 10, operations_per_worker: int = 100):
        """ë™ì‹œ ì ‘ê·¼ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        test_name = f"concurrent_access_{worker_count}workers"
        print(f"ğŸ”¥ ë™ì‹œ ì ‘ê·¼ í…ŒìŠ¤íŠ¸ ì‹œì‘: {worker_count}ê°œ ì›Œì»¤")
        
        with self.monitor_resources(test_name):
            results_queue = queue.Queue()
            errors_queue = queue.Queue()
            
            def worker(worker_id: int):
                """ì›Œì»¤ í•¨ìˆ˜"""
                worker_stats = {"operations": 0, "errors": 0, "start_time": time.time()}
                
                for i in range(operations_per_worker):
                    try:
                        # ëœë¤í•œ ì‘ì—… ì„ íƒ
                        operation = random.choice(["add", "search", "retrieve"])
                        
                        if operation == "add":
                            content = f"ë™ì‹œì ‘ê·¼ í…ŒìŠ¤íŠ¸ ì›Œì»¤{worker_id} ì‘ì—…{i}"
                            result = process_user_input(content)
                            block_data = {
                                "block_index": worker_id * 1000 + i,
                                "timestamp": datetime.now().isoformat(),
                                "context": content,
                                "keywords": result.get("keywords", []),
                                "tags": result.get("tags", []),
                                "embedding": result.get("embedding", []),
                                "importance": 0.5,
                                "hash": f"worker_{worker_id}_{i}",
                                "prev_hash": ""
                            }
                            self.secondary_db.add_block(block_data)
                            
                        elif operation == "search":
                            keywords = [f"ì›Œì»¤{worker_id}", "í…ŒìŠ¤íŠ¸"]
                            self.secondary_db.search_blocks_by_keyword(keywords, limit=5)
                            
                        elif operation == "retrieve":
                            block_id = random.randint(0, max(1, worker_id * 1000 + i - 1))
                            self.secondary_db.get_block(block_id)
                        
                        worker_stats["operations"] += 1
                        
                        # ëœë¤ ëŒ€ê¸° (ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜)
                        time.sleep(random.uniform(0.001, 0.01))
                        
                    except Exception as e:
                        worker_stats["errors"] += 1
                        errors_queue.put(f"Worker {worker_id}, Op {i}: {str(e)}")
                
                worker_stats["end_time"] = time.time()
                worker_stats["duration"] = worker_stats["end_time"] - worker_stats["start_time"]
                results_queue.put((worker_id, worker_stats))
            
            # ìŠ¤ë ˆë“œ í’€ë¡œ ë™ì‹œ ì‹¤í–‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=worker_count) as executor:
                futures = [executor.submit(worker, i) for i in range(worker_count)]
                
                # ëª¨ë“  ì›Œì»¤ ì™„ë£Œ ëŒ€ê¸°
                concurrent.futures.wait(futures, timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            
            # ê²°ê³¼ ìˆ˜ì§‘
            worker_results = []
            while not results_queue.empty():
                worker_results.append(results_queue.get())
            
            errors = []
            while not errors_queue.empty():
                errors.append(errors_queue.get())
            
            # í†µê³„ ê³„ì‚°
            total_operations = sum(stats["operations"] for _, stats in worker_results)
            total_errors = sum(stats["errors"] for _, stats in worker_results)
            avg_duration = sum(stats["duration"] for _, stats in worker_results) / len(worker_results) if worker_results else 0
            
            self.results["stress_tests"][test_name].update({
                "completed_workers": len(worker_results),
                "total_operations": total_operations,
                "total_errors": total_errors,
                "error_rate": round(total_errors / max(1, total_operations) * 100, 2),
                "avg_worker_duration": round(avg_duration, 2),
                "operations_per_second": round(total_operations / max(1, avg_duration), 2)
            })
            
            if errors:
                print(f"   ì˜¤ë¥˜ {len(errors)}ê°œ ë°œìƒ")
    
    def stress_test_memory_leak_detection(self, duration_minutes: int = 30):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ í…ŒìŠ¤íŠ¸"""
        test_name = f"memory_leak_{duration_minutes}min"
        print(f"ğŸ”¥ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹œì‘: {duration_minutes}ë¶„ê°„")
        
        with self.monitor_resources(test_name, duration_minutes * 60):
            end_time = time.time() + (duration_minutes * 60)
            cycle_count = 0
            
            while time.time() < end_time:
                cycle_count += 1
                
                # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—…ë“¤
                test_data = self.generate_test_data(50, "complex")
                
                for i, content in enumerate(test_data):
                    result = process_user_input(content)
                    # ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥í•˜ê³  DBì—ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ (ëˆ„ìˆ˜ í…ŒìŠ¤íŠ¸)
                    
                # ê²€ìƒ‰ ì‘ì—…
                keywords = ["í…ŒìŠ¤íŠ¸", "ë©”ëª¨ë¦¬", "ëˆ„ìˆ˜"]
                try:
                    self.primary_db.search_blocks_by_keyword(keywords, limit=10)
                except:
                    pass
                
                # ì£¼ê¸°ì ìœ¼ë¡œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
                if cycle_count % 10 == 0:
                    gc.collect()
                    print(f"   ì‚¬ì´í´ {cycle_count} ì™„ë£Œ ({((time.time() - (end_time - duration_minutes * 60)) / (duration_minutes * 60) * 100):.1f}%)")
                
                time.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
            
            self.results["stress_tests"][test_name]["completed_cycles"] = cycle_count
    
    def stress_test_disk_space_simulation(self):
        """ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± ì‹œë®¬ë ˆì´ì…˜"""
        test_name = "disk_space_simulation"
        print(f"ğŸ”¥ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘")
        
        with self.monitor_resources(test_name):
            # ë§¤ìš° í° í…ìŠ¤íŠ¸ ë°ì´í„° ìƒì„±í•˜ì—¬ ë””ìŠ¤í¬ ê³µê°„ ì†Œëª¨
            large_content = "Large content for disk space test. " * 1000  # ~30KB per block
            
            successful_adds = 0
            disk_full_errors = 0
            
            for i in range(1000):  # ìµœëŒ€ 30MB ì‹œë„
                try:
                    result = process_user_input(large_content + f" Block {i}")
                    block_data = {
                        "block_index": i + 20000,
                        "timestamp": datetime.now().isoformat(),
                        "context": large_content + f" Block {i}",
                        "keywords": result.get("keywords", []),
                        "tags": result.get("tags", []),
                        "embedding": result.get("embedding", []),
                        "importance": 0.5,
                        "hash": f"disk_test_{i}",
                        "prev_hash": f"disk_test_{i-1}" if i > 0 else ""
                    }
                    self.primary_db.add_block(block_data)
                    successful_adds += 1
                    
                    if i % 100 == 0:
                        print(f"   ì§„í–‰: {i}/1000 ë¸”ë¡")
                    
                except Exception as e:
                    if "disk" in str(e).lower() or "space" in str(e).lower():
                        disk_full_errors += 1
                    print(f"   ë¸”ë¡ {i} ì €ì¥ ì‹¤íŒ¨: {e}")
                    break
            
            self.results["stress_tests"][test_name].update({
                "successful_adds": successful_adds,
                "disk_full_errors": disk_full_errors,
                "total_attempted": i + 1
            })
    
    def stress_test_corruption_recovery(self):
        """ë°ì´í„° ì†ìƒ ë³µêµ¬ í…ŒìŠ¤íŠ¸"""
        test_name = "corruption_recovery"
        print(f"ğŸ”¥ ë°ì´í„° ì†ìƒ ë³µêµ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        with self.monitor_resources(test_name):
            # ë¨¼ì € ì •ìƒ ë°ì´í„° ì¶”ê°€
            normal_data = self.generate_test_data(100, "medium")
            for i, content in enumerate(normal_data):
                result = process_user_input(content)
                block_data = {
                    "block_index": i + 30000,
                    "timestamp": datetime.now().isoformat(),
                    "context": content,
                    "keywords": result.get("keywords", []),
                    "tags": result.get("tags", []),
                    "embedding": result.get("embedding", []),
                    "importance": 0.5,
                    "hash": f"recovery_test_{i}",
                    "prev_hash": f"recovery_test_{i-1}" if i > 0 else ""
                }
                self.primary_db.add_block(block_data)
            
            # ë°ì´í„° ê²€ì¦
            initial_blocks = []
            for i in range(10):
                block = self.primary_db.get_block(i + 30000)
                if block:
                    initial_blocks.append(block)
            
            # ì†ìƒëœ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜ (ì˜ëª»ëœ í˜•ì‹ì˜ ë°ì´í„°)
            corrupted_attempts = 0
            recovery_attempts = 0
            
            corrupted_data_types = [
                None,  # NULL ë°ì´í„°
                "",    # ë¹ˆ ë¬¸ìì—´
                "a" * 100000,  # ë„ˆë¬´ ê¸´ ë¬¸ìì—´
                {"invalid": "json"},  # ì˜ëª»ëœ íƒ€ì…
                "\x00\x01\x02",  # ë°”ì´ë„ˆë¦¬ ë°ì´í„°
            ]
            
            for corrupt_data in corrupted_data_types:
                try:
                    result = process_user_input(str(corrupt_data) if corrupt_data is not None else "NULL")
                    corrupted_attempts += 1
                except Exception:
                    recovery_attempts += 1  # ì˜ˆì™¸ ì²˜ë¦¬ë¡œ ë³µêµ¬
            
            # ë³µêµ¬ í›„ ë°ì´í„° ê²€ì¦
            recovered_blocks = []
            for i in range(10):
                block = self.primary_db.get_block(i + 30000)
                if block:
                    recovered_blocks.append(block)
            
            self.results["stress_tests"][test_name].update({
                "initial_blocks_count": len(initial_blocks),
                "corrupted_attempts": corrupted_attempts,
                "recovery_attempts": recovery_attempts,
                "recovered_blocks_count": len(recovered_blocks),
                "data_integrity_maintained": len(initial_blocks) == len(recovered_blocks)
            })
    
    def analyze_stress_test_results(self) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        analysis = {
            "overall_score": 0,
            "stability_grade": "Unknown",
            "critical_issues": [],
            "warnings": [],
            "recommendations": []
        }
        
        tests = self.results["stress_tests"]
        total_score = 0
        test_count = 0
        
        for test_name, test_result in tests.items():
            test_score = 100  # ì‹œì‘ ì ìˆ˜
            
            # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê²€ì‚¬
            memory_leak = test_result.get("memory_leak_mb", 0)
            if memory_leak > 50:
                analysis["critical_issues"].append(f"{test_name}: ì‹¬ê°í•œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ {memory_leak:.1f}MB")
                test_score -= 50
            elif memory_leak > 10:
                analysis["warnings"].append(f"{test_name}: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ {memory_leak:.1f}MB")
                test_score -= 20
            
            # íŒŒì¼ í•¸ë“¤ ëˆ„ìˆ˜ ê²€ì‚¬
            file_leak = test_result.get("file_leak_count", 0)
            if file_leak > 10:
                analysis["critical_issues"].append(f"{test_name}: íŒŒì¼ í•¸ë“¤ ëˆ„ìˆ˜ {file_leak}ê°œ")
                test_score -= 30
            elif file_leak > 0:
                analysis["warnings"].append(f"{test_name}: íŒŒì¼ í•¸ë“¤ ì¦ê°€ {file_leak}ê°œ")
                test_score -= 10
            
            # ì˜¤ë¥˜ìœ¨ ê²€ì‚¬
            error_rate = test_result.get("error_rate", 0)
            if error_rate > 5:
                analysis["critical_issues"].append(f"{test_name}: ë†’ì€ ì˜¤ë¥˜ìœ¨ {error_rate}%")
                test_score -= 40
            elif error_rate > 1:
                analysis["warnings"].append(f"{test_name}: ì˜¤ë¥˜ìœ¨ {error_rate}%")
                test_score -= 15
            
            # ì„±ê³µë¥  ê²€ì‚¬
            success_rate = test_result.get("success_rate", 100)
            if success_rate < 90:
                analysis["critical_issues"].append(f"{test_name}: ë‚®ì€ ì„±ê³µë¥  {success_rate}%")
                test_score -= 50
            elif success_rate < 95:
                analysis["warnings"].append(f"{test_name}: ì„±ê³µë¥  {success_rate}%")
                test_score -= 20
            
            total_score += max(0, test_score)
            test_count += 1
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        if test_count > 0:
            analysis["overall_score"] = round(total_score / test_count, 1)
        
        # ë“±ê¸‰ ê²°ì •
        score = analysis["overall_score"]
        if score >= 90:
            analysis["stability_grade"] = "A (Excellent)"
        elif score >= 80:
            analysis["stability_grade"] = "B (Good)"
        elif score >= 70:
            analysis["stability_grade"] = "C (Fair)"
        elif score >= 60:
            analysis["stability_grade"] = "D (Poor)"
        else:
            analysis["stability_grade"] = "F (Critical)"
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        if analysis["critical_issues"]:
            analysis["recommendations"].append("ì¦‰ì‹œ ìˆ˜ì • í•„ìš”í•œ ì‹¬ê°í•œ ë¬¸ì œë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if analysis["warnings"]:
            analysis["recommendations"].append("ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•œ ê²½ê³  ì‚¬í•­ë“¤ì´ ìˆìŠµë‹ˆë‹¤.")
        if score >= 90:
            analysis["recommendations"].append("ëª¨ë“  ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")
        else:
            analysis["recommendations"].append("ì•ˆì •ì„± ê°œì„ ì„ ìœ„í•œ ì¶”ê°€ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return analysis
    
    def run_all_stress_tests(self):
        """ëª¨ë“  ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ”¥ Greeum v2.0.4 Weekly Stress Test ì‹œì‘")
        print(f"   ë‚ ì§œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   í”Œë«í¼: {sys.platform}")
        print(f"   ì‹œìŠ¤í…œ: CPU {self.results['system_info']['cpu_count']}ì½”ì–´, "
              f"RAM {self.results['system_info']['memory_total_gb']:.1f}GB")
        print()
        
        try:
            # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            self.stress_test_massive_memory_blocks([1000, 5000])  # 10000ì€ ì‹œê°„ìƒ ì œì™¸
            self.stress_test_concurrent_access(worker_count=8, operations_per_worker=50)
            self.stress_test_memory_leak_detection(duration_minutes=5)  # 30ë¶„ì€ ì‹œê°„ìƒ 5ë¶„ìœ¼ë¡œ ì¶•ì†Œ
            self.stress_test_disk_space_simulation()
            self.stress_test_corruption_recovery()
            
            # ê²°ê³¼ ë¶„ì„
            analysis = self.analyze_stress_test_results()
            self.results["analysis"] = analysis
            
            print()
            print(f"ğŸ“Š ì „ì²´ ì•ˆì •ì„± ì ìˆ˜: {analysis['overall_score']}/100")
            print(f"ğŸ† ì•ˆì •ì„± ë“±ê¸‰: {analysis['stability_grade']}")
            
            if analysis["critical_issues"]:
                print("âŒ ì‹¬ê°í•œ ë¬¸ì œ:")
                for issue in analysis["critical_issues"]:
                    print(f"   - {issue}")
            
            if analysis["warnings"]:
                print("âš ï¸  ê²½ê³ :")
                for warning in analysis["warnings"]:
                    print(f"   - {warning}")
            
            print("ğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            for rec in analysis["recommendations"]:
                print(f"   - {rec}")
            
        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            self.results["error"] = str(e)
            import traceback
            self.results["traceback"] = traceback.format_exc()
        
        finally:
            # ê²°ê³¼ ì €ì¥
            self.save_results()
            
            # ì •ë¦¬
            self.cleanup()
    
    def save_results(self):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        results_dir = Path(__file__).parent.parent / "stress_test_results"
        results_dir.mkdir(exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ íŒŒì¼ëª…
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"weekly_stress_test_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {results_file}")
        
        # ìµœì‹  ê²°ê³¼ë„ ë³„ë„ ì €ì¥
        latest_file = results_dir / "latest_stress_test.json"
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
    
    def cleanup(self):
        """í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬"""
        try:
            if self.test_dir.exists():
                shutil.rmtree(self.test_dir)
                print(f"ğŸ§¹ í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸  ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    stress_test = StressTestSuite()
    stress_test.run_all_stress_tests()


if __name__ == "__main__":
    main()
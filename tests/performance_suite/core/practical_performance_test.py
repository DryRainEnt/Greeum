#!/usr/bin/env python3
"""
Greeum v2.0.5 ì‹¤ìš©ì  ì„±ëŠ¥ ì¸¡ì • ì‹œìŠ¤í…œ

ì‹œê°„ íš¨ìœ¨ì„±ê³¼ ì¸¡ì • ì •í™•ë„ì˜ ê· í˜•ì„ ë§ì¶˜ ì‹¤ìš©ì ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì¶©ë¶„í•œ ìƒ˜í”Œë¡œ í†µê³„ì  ì‹ ë¢°ë„ë¥¼ í™•ë³´í•˜ë©´ì„œë„ í•©ë¦¬ì ì¸ ì‹¤í–‰ ì‹œê°„ì„ ìœ ì§€í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
import statistics
from pathlib import Path

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.abspath('../../..'))

from greeum import DatabaseManager, BlockManager, STMManager, CacheManager
from greeum.core.prompt_wrapper import PromptWrapper
from greeum.embedding_models import get_embedding

logger = logging.getLogger(__name__)

class PracticalPerformanceTest:
    """ì‹¤ìš©ì ì´ê³  ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì • í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir: str = "tests/performance_suite/results/baselines"):
        """ì´ˆê¸°í™”"""
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Greeum ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.db_manager = DatabaseManager()
        self.block_manager = BlockManager(self.db_manager)
        self.stm_manager = STMManager(self.db_manager)
        self.cache_manager = CacheManager(block_manager=self.block_manager, stm_manager=self.stm_manager)
        self.prompt_wrapper = PromptWrapper(self.cache_manager, self.stm_manager)
        
        logger.info(f"PracticalPerformanceTest ì´ˆê¸°í™” ì™„ë£Œ - ë°ì´í„° ë””ë ‰í† ë¦¬: {self.data_dir}")
    
    def run_comprehensive_test(self, 
                              memory_sample_size: int = 100,
                              quality_sample_size: int = 30,
                              concurrency_sample_size: int = 20) -> Dict[str, Any]:
        """
        ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        Args:
            memory_sample_size: ë©”ëª¨ë¦¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜
            quality_sample_size: ì‘ë‹µ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜  
            concurrency_sample_size: ë™ì‹œì„± í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜
        """
        logger.info(f"ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ - ë©”ëª¨ë¦¬:{memory_sample_size}, í’ˆì§ˆ:{quality_sample_size}, ë™ì‹œì„±:{concurrency_sample_size}")
        start_time = time.time()
        
        results = {
            "test_timestamp": datetime.now().isoformat(),
            "greeum_version": "2.0.5",
            "sample_sizes": {
                "memory_search": memory_sample_size,
                "response_quality": quality_sample_size,
                "concurrency": concurrency_sample_size
            },
            "metrics": {}
        }
        
        # 1. ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ§  ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        results["metrics"]["memory_search"] = self._test_memory_search_performance(memory_sample_size)
        
        # 2. ì‘ë‹µ í’ˆì§ˆ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ“ ì‘ë‹µ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì¤‘...")
        results["metrics"]["response_quality"] = self._test_response_quality(quality_sample_size)
        
        # 3. í™•ì¥ì„± í…ŒìŠ¤íŠ¸ (ê°„ì†Œí™”)
        logger.info("ğŸ“ˆ í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        results["metrics"]["scalability"] = self._test_scalability_simplified()
        
        # 4. ë™ì‹œì„± í…ŒìŠ¤íŠ¸ (ê°„ì†Œí™”)
        logger.info("âš¡ ë™ì‹œì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        results["metrics"]["concurrency"] = self._test_concurrency_simplified(concurrency_sample_size)
        
        # 5. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
        logger.info("ğŸ’» ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘...")
        results["metrics"]["system_resources"] = self._monitor_system_resources()
        
        total_time = time.time() - start_time
        results["total_test_duration"] = total_time
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°
        results["performance_grade"] = self._calculate_performance_grade(results["metrics"])
        
        logger.info(f"ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
        return results
    
    def _test_memory_search_performance(self, sample_size: int) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # í˜„ì‹¤ì ì¸ í¬ê¸°ì˜ ë©”ëª¨ë¦¬ ë¸”ë¡ í™˜ê²½ êµ¬ì„±
        memory_sizes = [100, 500, 1000]
        results = {}
        
        for memory_size in memory_sizes:
            logger.info(f"  ğŸ“Š {memory_size}ê°œ ë¸”ë¡ í™˜ê²½ í…ŒìŠ¤íŠ¸...")
            
            # í…ŒìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ë¸”ë¡ ì¤€ë¹„ (ê¸°ì¡´ ë¸”ë¡ í™œìš©)
            self._prepare_memory_blocks(memory_size)
            
            # ë‹¤ì–‘í•œ ì¿¼ë¦¬ íŒ¨í„´ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            test_queries = [
                "í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©",
                "ê°œë°œ ê³„íš ë° ì¼ì •",
                "ì„±ëŠ¥ ìµœì í™” ë°©ì•ˆ",
                "ë²„ê·¸ ìˆ˜ì • í˜„í™©",
                "ìƒˆë¡œìš´ ê¸°ëŠ¥ êµ¬í˜„",
                "ì‚¬ìš©ì í”¼ë“œë°± ë¶„ì„",
                "í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€í† ",
                "ë°°í¬ ì¤€ë¹„ ìƒí™©"
            ]
            
            ltm_times = []
            cache_times = []
            speedup_ratios = []
            
            # ê° ë©”ëª¨ë¦¬ í¬ê¸°ì— ëŒ€í•´ ì¶©ë¶„í•œ ìƒ˜í”Œ ìˆ˜í–‰
            samples_per_size = sample_size // len(memory_sizes)
            
            for i in range(samples_per_size):
                query = test_queries[i % len(test_queries)] + f" {i}"
                embedding = get_embedding(query)
                keywords = query.split()[:3]
                
                # LTM ì§ì ‘ ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •
                start_time = time.perf_counter()
                ltm_results = self.block_manager.search_by_embedding(embedding, top_k=5)
                ltm_time = (time.perf_counter() - start_time) * 1000
                ltm_times.append(ltm_time)
                
                # Cache ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •
                start_time = time.perf_counter()
                cache_results = self.cache_manager.update_cache(query, embedding, keywords)
                cache_time = (time.perf_counter() - start_time) * 1000
                cache_times.append(cache_time)
                
                # ì†ë„ í–¥ìƒ ë¹„ìœ¨ ê³„ì‚°
                if cache_time > 0:
                    speedup_ratios.append(ltm_time / cache_time)
            
            # í†µê³„ ê³„ì‚°
            results[f"memory_size_{memory_size}"] = {
                "block_count": memory_size,
                "sample_count": len(ltm_times),
                
                # LTM ì„±ëŠ¥
                "ltm_avg_time_ms": statistics.mean(ltm_times),
                "ltm_median_time_ms": statistics.median(ltm_times),
                "ltm_p95_time_ms": statistics.quantiles(ltm_times, n=20)[18] if len(ltm_times) >= 20 else max(ltm_times),
                "ltm_std_dev": statistics.stdev(ltm_times) if len(ltm_times) > 1 else 0,
                
                # Cache ì„±ëŠ¥
                "cache_avg_time_ms": statistics.mean(cache_times),
                "cache_median_time_ms": statistics.median(cache_times),
                "cache_p95_time_ms": statistics.quantiles(cache_times, n=20)[18] if len(cache_times) >= 20 else max(cache_times),
                "cache_std_dev": statistics.stdev(cache_times) if len(cache_times) > 1 else 0,
                
                # ì†ë„ í–¥ìƒ
                "avg_speedup_ratio": statistics.mean(speedup_ratios) if speedup_ratios else 1,
                "median_speedup_ratio": statistics.median(speedup_ratios) if speedup_ratios else 1,
                "max_speedup_ratio": max(speedup_ratios) if speedup_ratios else 1
            }
        
        # ì „ì²´ ìš”ì•½
        all_ltm_times = []
        all_cache_times = []
        all_speedups = []
        
        for size_result in results.values():
            # ê°€ì¤‘ í‰ê· ì„ ìœ„í•´ ìƒ˜í”Œ ìˆ˜ ê³ ë ¤í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ í‰ê·  ì‚¬ìš©
            all_ltm_times.append(size_result["ltm_avg_time_ms"])
            all_cache_times.append(size_result["cache_avg_time_ms"])
            all_speedups.append(size_result["avg_speedup_ratio"])
        
        results["summary"] = {
            "overall_ltm_avg": statistics.mean(all_ltm_times),
            "overall_cache_avg": statistics.mean(all_cache_times),
            "overall_speedup_avg": statistics.mean(all_speedups),
            "scalability_factor": max(all_ltm_times) / min(all_ltm_times) if all_ltm_times else 1
        }
        
        return results
    
    def _test_response_quality(self, sample_size: int) -> Dict[str, Any]:
        """ì‘ë‹µ í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""
        # ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì§ˆì˜ í…ŒìŠ¤íŠ¸
        query_types = {
            "factual": [
                "í˜„ì¬ í”„ë¡œì íŠ¸ ìƒíƒœëŠ”?",
                "ìµœê·¼ ì—…ë°ì´íŠ¸ ë‚´ìš©ì€?",
                "ê°œë°œ ì§„í–‰ë¥ ì€?"
            ],
            "contextual": [
                "ì´ì „ ë…¼ì˜í•œ ì„±ëŠ¥ ì´ìŠˆê°€ í•´ê²°ëë‚˜ìš”?",
                "ì§€ë‚œì£¼ ê³„íší•œ ì‘ì—…ë“¤ ì™„ë£Œëë‚˜ìš”?",
                "ì•ì„œ ì–¸ê¸‰í•œ ê°œì„ ì‚¬í•­ ì ìš©ëë‚˜ìš”?"
            ],
            "analytical": [
                "í˜„ì¬ ì„±ëŠ¥ê³¼ ëª©í‘œì˜ ì°¨ì´ëŠ”?",
                "ê°€ì¥ ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ì€?",
                "ì„±ëŠ¥ í–¥ìƒ ìš°ì„ ìˆœìœ„ëŠ”?"
            ]
        }
        
        results = {}
        
        for query_type, queries in query_types.items():
            logger.info(f"  ğŸ“ {query_type} íƒ€ì… ì§ˆì˜ í…ŒìŠ¤íŠ¸...")
            
            quality_scores = []
            context_usage_counts = []
            response_lengths = []
            processing_times = []
            
            samples_per_type = sample_size // len(query_types)
            
            for i in range(samples_per_type):
                query = queries[i % len(queries)]
                
                # í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œê°„ ì¸¡ì •
                start_time = time.perf_counter()
                enhanced_prompt = self.prompt_wrapper.compose_prompt(query, token_budget=1500)
                processing_time = (time.perf_counter() - start_time) * 1000
                processing_times.append(processing_time)
                
                # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
                response_length = len(enhanced_prompt)
                response_lengths.append(response_length)
                
                # ì»¨í…ìŠ¤íŠ¸ í™œìš©ë„ (ê´€ë ¨ ê¸°ì–µ, ìµœê·¼ ê¸°ì–µ ì„¹ì…˜ ìˆ˜)
                context_count = enhanced_prompt.count('ê´€ë ¨ ê¸°ì–µ:') + enhanced_prompt.count('ìµœê·¼ ê¸°ì–µ:')
                context_usage_counts.append(context_count)
                
                # í’ˆì§ˆ ì ìˆ˜ (ì •ë³´ ë°€ë„ ê¸°ë°˜)
                words = enhanced_prompt.split()
                meaningful_words = [w for w in words if len(w) > 3]
                quality_score = len(meaningful_words) / len(words) if words else 0
                quality_scores.append(quality_score)
            
            # í†µê³„ ê³„ì‚°
            results[query_type] = {
                "sample_count": len(quality_scores),
                "avg_quality_score": statistics.mean(quality_scores),
                "median_quality_score": statistics.median(quality_scores),
                "avg_context_usage": statistics.mean(context_usage_counts),
                "avg_response_length": statistics.mean(response_lengths),
                "avg_processing_time_ms": statistics.mean(processing_times),
                "quality_consistency": 1 - (statistics.stdev(quality_scores) / statistics.mean(quality_scores)) if statistics.mean(quality_scores) > 0 else 1
            }
        
        # ì „ì²´ ìš”ì•½
        all_quality_scores = []
        all_context_usage = []
        all_processing_times = []
        
        for type_result in results.values():
            all_quality_scores.append(type_result["avg_quality_score"])
            all_context_usage.append(type_result["avg_context_usage"])
            all_processing_times.append(type_result["avg_processing_time_ms"])
        
        results["summary"] = {
            "overall_quality_score": statistics.mean(all_quality_scores),
            "overall_context_usage": statistics.mean(all_context_usage),
            "overall_processing_time": statistics.mean(all_processing_times),
            "quality_variance": statistics.stdev(all_quality_scores) if len(all_quality_scores) > 1 else 0
        }
        
        return results
    
    def _test_scalability_simplified(self) -> Dict[str, Any]:
        """ê°„ì†Œí™”ëœ í™•ì¥ì„± í…ŒìŠ¤íŠ¸"""
        block_counts = [100, 500, 1000]
        test_query = "í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬"
        embedding = get_embedding(test_query)
        
        results = {}
        base_time = None
        
        for block_count in block_counts:
            logger.info(f"  ğŸ“ˆ {block_count}ê°œ ë¸”ë¡ í™•ì¥ì„± í…ŒìŠ¤íŠ¸...")
            
            self._prepare_memory_blocks(block_count)
            
            # 5íšŒ ì¸¡ì • í›„ í‰ê· 
            search_times = []
            for _ in range(5):
                start_time = time.perf_counter()
                search_results = self.block_manager.search_by_embedding(embedding, top_k=5)
                search_time = (time.perf_counter() - start_time) * 1000
                search_times.append(search_time)
            
            avg_time = statistics.mean(search_times)
            
            if base_time is None:
                base_time = avg_time
                scalability_factor = 1.0
            else:
                scalability_factor = avg_time / base_time
            
            results[f"blocks_{block_count}"] = {
                "block_count": block_count,
                "avg_search_time_ms": avg_time,
                "scalability_factor": scalability_factor,
                "search_efficiency": block_count / avg_time,  # ë¸”ë¡ë‹¹ ê²€ìƒ‰ íš¨ìœ¨
                "measurements": len(search_times)
            }
        
        # ì„ í˜•ì„± ì ìˆ˜ ê³„ì‚°
        scalability_factors = [results[f"blocks_{count}"]["scalability_factor"] for count in block_counts]
        ideal_factors = [count / block_counts[0] for count in block_counts]
        
        deviations = [abs(ideal - actual) for ideal, actual in zip(ideal_factors, scalability_factors)]
        linearity_score = max(0, 1 - statistics.mean(deviations))
        
        results["analysis"] = {
            "base_performance_ms": base_time,
            "linearity_score": linearity_score,
            "scalability_rating": "excellent" if linearity_score > 0.8 else 
                                 "good" if linearity_score > 0.6 else 
                                 "fair" if linearity_score > 0.4 else "poor"
        }
        
        return results
    
    def _test_concurrency_simplified(self, sample_size: int) -> Dict[str, Any]:
        """ê°„ì†Œí™”ëœ ë™ì‹œì„± í…ŒìŠ¤íŠ¸"""
        import threading
        import queue
        
        thread_counts = [1, 2, 4, 8]
        results = {}
        
        def worker_task(task_queue, result_queue, worker_id):
            """ì›Œì»¤ ìŠ¤ë ˆë“œ ì‘ì—…"""
            while True:
                try:
                    task_id = task_queue.get_nowait()
                    query = f"ë™ì‹œì„± í…ŒìŠ¤íŠ¸ {worker_id}-{task_id}"
                    embedding = get_embedding(query)
                    
                    start_time = time.perf_counter()
                    search_results = self.block_manager.search_by_embedding(embedding, top_k=5)
                    execution_time = (time.perf_counter() - start_time) * 1000
                    
                    result_queue.put({
                        "worker_id": worker_id,
                        "task_id": task_id,
                        "execution_time_ms": execution_time,
                        "result_count": len(search_results)
                    })
                    
                    task_queue.task_done()
                except queue.Empty:
                    break
                except Exception as e:
                    result_queue.put({"error": str(e), "worker_id": worker_id})
                    task_queue.task_done()
        
        for thread_count in thread_counts:
            logger.info(f"  âš¡ {thread_count}ê°œ ìŠ¤ë ˆë“œ ë™ì‹œì„± í…ŒìŠ¤íŠ¸...")
            
            # ì‘ì—… í ì¤€ë¹„
            task_queue = queue.Queue()
            result_queue = queue.Queue()
            
            tasks_per_thread = max(1, sample_size // len(thread_counts) // thread_count)
            total_tasks = tasks_per_thread * thread_count
            
            for i in range(total_tasks):
                task_queue.put(i)
            
            # ìŠ¤ë ˆë“œ ì‹œì‘
            threads = []
            start_time = time.perf_counter()
            
            for i in range(thread_count):
                thread = threading.Thread(target=worker_task, args=(task_queue, result_queue, i))
                thread.start()
                threads.append(thread)
            
            # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            task_queue.join()
            total_time = time.perf_counter() - start_time
            
            # ìŠ¤ë ˆë“œ ì •ë¦¬
            for thread in threads:
                thread.join()
            
            # ê²°ê³¼ ìˆ˜ì§‘
            execution_times = []
            error_count = 0
            
            while not result_queue.empty():
                result = result_queue.get()
                if "error" in result:
                    error_count += 1
                else:
                    execution_times.append(result["execution_time_ms"])
            
            # í†µê³„ ê³„ì‚°
            if execution_times:
                results[f"threads_{thread_count}"] = {
                    "thread_count": thread_count,
                    "total_tasks": total_tasks,
                    "completed_tasks": len(execution_times),
                    "failed_tasks": error_count,
                    "total_time_s": total_time,
                    "avg_task_time_ms": statistics.mean(execution_times),
                    "median_task_time_ms": statistics.median(execution_times),
                    "throughput_tasks_per_sec": len(execution_times) / total_time,
                    "error_rate_pct": (error_count / total_tasks) * 100,
                    "efficiency": len(execution_times) / (thread_count * total_time)  # ìŠ¤ë ˆë“œë‹¹ íš¨ìœ¨ì„±
                }
            else:
                results[f"threads_{thread_count}"] = {
                    "thread_count": thread_count,
                    "error": "ëª¨ë“  ì‘ì—… ì‹¤íŒ¨",
                    "failed_tasks": error_count
                }
        
        # ë™ì‹œì„± íš¨ê³¼ ë¶„ì„
        single_thread_throughput = results.get("threads_1", {}).get("throughput_tasks_per_sec", 0)
        
        concurrency_analysis = {}
        for thread_count in thread_counts[1:]:
            key = f"threads_{thread_count}"
            if key in results and "throughput_tasks_per_sec" in results[key]:
                current_throughput = results[key]["throughput_tasks_per_sec"]
                if single_thread_throughput > 0:
                    speedup = current_throughput / single_thread_throughput
                    efficiency = speedup / thread_count
                    concurrency_analysis[key] = {
                        "speedup": speedup,
                        "efficiency": efficiency,
                        "rating": "excellent" if efficiency > 0.8 else
                                 "good" if efficiency > 0.6 else
                                 "fair" if efficiency > 0.4 else "poor"
                    }
        
        results["concurrency_analysis"] = concurrency_analysis
        
        return results
    
    def _monitor_system_resources(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""
        try:
            import psutil
            import gc
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìˆ˜í–‰
            gc.collect()
            
            process = psutil.Process()
            
            # ë©”ëª¨ë¦¬ ì •ë³´
            memory_info = process.memory_info()
            memory_percent = process.memory_percent()
            
            # CPU ì •ë³´ (0.5ì´ˆ ê°„ê²©ìœ¼ë¡œ ì¸¡ì •)
            cpu_percent = process.cpu_percent(interval=0.5)
            
            # ì‹œìŠ¤í…œ ì „ì²´ ì •ë³´
            system_memory = psutil.virtual_memory()
            system_cpu = psutil.cpu_percent(interval=0.5)
            
            return {
                "process_memory": {
                    "rss_mb": round(memory_info.rss / 1024 / 1024, 2),
                    "vms_mb": round(memory_info.vms / 1024 / 1024, 2),
                    "percent": round(memory_percent, 2)
                },
                "system_memory": {
                    "total_gb": round(system_memory.total / 1024 / 1024 / 1024, 2),
                    "available_gb": round(system_memory.available / 1024 / 1024 / 1024, 2),
                    "used_percent": round(system_memory.percent, 2)
                },
                "cpu_usage": {
                    "process_percent": round(cpu_percent, 2),
                    "system_percent": round(system_cpu, 2),
                    "thread_count": process.num_threads()
                },
                "measurement_time": datetime.now().isoformat()
            }
            
        except ImportError:
            return {"error": "psutil not available for system monitoring"}
        except Exception as e:
            return {"error": f"System monitoring failed: {str(e)}"}
    
    def _prepare_memory_blocks(self, target_count: int):
        """í…ŒìŠ¤íŠ¸ìš© ë©”ëª¨ë¦¬ ë¸”ë¡ ì¤€ë¹„ (ê¸°ì¡´ ë¸”ë¡ í™œìš©)"""
        # í˜„ì¬ ë¸”ë¡ ìˆ˜ í™•ì¸
        current_blocks = self.block_manager.get_blocks(limit=target_count * 2)
        current_count = len(current_blocks)
        
        if current_count < target_count:
            # ë¶€ì¡±í•œ ë¸”ë¡ ì¶”ê°€ ìƒì„±
            additional_needed = target_count - current_count
            logger.info(f"  ğŸ“¦ {additional_needed}ê°œ ì¶”ê°€ ë¸”ë¡ ìƒì„± ì¤‘...")
            
            content_templates = [
                "í”„ë¡œì íŠ¸ {phase} ë‹¨ê³„ ì§„í–‰: {status}",
                "{date} {feature} ê¸°ëŠ¥ {action} ì™„ë£Œ",
                "{team}íŒ€ {task} ì‘ì—… ê²°ê³¼: {result}",
                "v{version} {component} ì„±ëŠ¥ ê°œì„ : {improvement}%",
                "{metric} ì§€í‘œ {direction}: {change}% ë³€í™”"
            ]
            
            phases = ["ê¸°íš", "ì„¤ê³„", "ê°œë°œ", "í…ŒìŠ¤íŠ¸", "ë°°í¬"]
            features = ["ë¡œê·¸ì¸", "ê²€ìƒ‰", "ì•Œë¦¼", "ëŒ€ì‹œë³´ë“œ", "ë¦¬í¬íŒ…"]
            actions = ["êµ¬í˜„", "ê°œì„ ", "ìˆ˜ì •", "ìµœì í™”"]
            teams = ["Frontend", "Backend", "DevOps", "QA"]
            
            for i in range(additional_needed):
                template = content_templates[i % len(content_templates)]
                context = template.format(
                    phase=phases[i % len(phases)],
                    status=["ì™„ë£Œ", "ì§„í–‰ì¤‘", "ê²€í† ì¤‘"][i % 3],
                    date=f"2025-{(i % 12) + 1:02d}-{(i % 28) + 1:02d}",
                    feature=features[i % len(features)],
                    action=actions[i % len(actions)],
                    team=teams[i % len(teams)],
                    task=f"Task-{i:03d}",
                    result=["ì„±ê³µ", "ë¶€ë¶„ì„±ê³µ", "ì¬ê²€í† í•„ìš”"][i % 3],
                    version=f"{(i % 3) + 1}.{(i % 5) + 1}",
                    component=["API", "UI", "DB", "Cache"][i % 4],
                    improvement=str(10 + (i % 30)),
                    metric=["ì‘ë‹µì‹œê°„", "ì²˜ë¦¬ëŸ‰", "ë©”ëª¨ë¦¬", "CPU"][i % 4],
                    direction=["ê°œì„ ", "ì €í•˜"][i % 2],
                    change=str(5 + (i % 20))
                )
                
                keywords = context.split()[:3]
                tags = ["test", phases[i % len(phases)]]
                embedding = get_embedding(context)
                importance = 0.3 + (i % 7) * 0.1
                
                self.block_manager.add_block(
                    context=context,
                    keywords=keywords,
                    tags=tags,
                    embedding=embedding,
                    importance=importance,
                    metadata={"test_block": True, "batch": "practical_test"}
                )
    
    def _calculate_performance_grade(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°"""
        grades = {}
        scores = {}
        
        # ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
        memory_search = metrics.get("memory_search", {})
        if memory_search and "summary" in memory_search:
            summary = memory_search["summary"]
            ltm_avg = summary.get("overall_ltm_avg", 100)
            cache_avg = summary.get("overall_cache_avg", 100)
            speedup = summary.get("overall_speedup_avg", 1)
            
            # ì ìˆ˜ ê³„ì‚° (0-100)
            ltm_score = max(0, 100 - ltm_avg * 2)  # 50ms ì´í•˜ë©´ ë§Œì 
            cache_score = max(0, 100 - cache_avg)  # 100ms ì´í•˜ë©´ ë§Œì 
            speedup_score = min(100, speedup * 20)  # 5x ì´ìƒì´ë©´ ë§Œì 
            
            memory_score = (ltm_score + cache_score + speedup_score) / 3
            scores["memory_search"] = memory_score
        
        # ì‘ë‹µ í’ˆì§ˆ í‰ê°€
        response_quality = metrics.get("response_quality", {})
        if response_quality and "summary" in response_quality:
            summary = response_quality["summary"]
            quality_score = summary.get("overall_quality_score", 0) * 100
            context_usage = summary.get("overall_context_usage", 0)
            processing_time = summary.get("overall_processing_time", 100)
            
            quality_points = quality_score  # 0-100
            context_points = min(100, context_usage * 50)  # 2ê°œë©´ ë§Œì 
            speed_points = max(0, 100 - processing_time * 10)  # 10ms ì´í•˜ë©´ ë§Œì 
            
            response_score = (quality_points + context_points + speed_points) / 3
            scores["response_quality"] = response_score
        
        # í™•ì¥ì„± í‰ê°€
        scalability = metrics.get("scalability", {})
        if scalability and "analysis" in scalability:
            linearity_score = scalability["analysis"].get("linearity_score", 0) * 100
            scores["scalability"] = linearity_score
        
        # ë™ì‹œì„± í‰ê°€
        concurrency = metrics.get("concurrency", {})
        if concurrency and "concurrency_analysis" in concurrency:
            analysis = concurrency["concurrency_analysis"]
            if analysis:
                # 8ìŠ¤ë ˆë“œ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€
                threads_8 = analysis.get("threads_8", {})
                if threads_8:
                    efficiency = threads_8.get("efficiency", 0)
                    concurrency_score = efficiency * 100
                    scores["concurrency"] = concurrency_score
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        if scores:
            overall_score = statistics.mean(scores.values())
        else:
            overall_score = 0
        
        # ë“±ê¸‰ ë¶€ì—¬
        def score_to_grade(score):
            if score >= 90: return "A+", "Outstanding"
            elif score >= 85: return "A", "Excellent"
            elif score >= 80: return "B+", "Very Good"
            elif score >= 75: return "B", "Good"
            elif score >= 70: return "C+", "Above Average"
            elif score >= 65: return "C", "Average"
            elif score >= 60: return "D+", "Below Average"
            elif score >= 55: return "D", "Poor"
            else: return "F", "Critical"
        
        overall_grade, overall_desc = score_to_grade(overall_score)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë“±ê¸‰
        category_grades = {}
        for category, score in scores.items():
            grade, desc = score_to_grade(score)
            category_grades[category] = {
                "score": round(score, 1),
                "grade": grade,
                "description": desc
            }
        
        return {
            "overall_score": round(overall_score, 1),
            "overall_grade": overall_grade,
            "overall_description": overall_desc,
            "category_scores": scores,
            "category_grades": category_grades,
            "grade_timestamp": datetime.now().isoformat()
        }
    
    def save_results(self, results: Dict[str, Any]) -> Path:
        """ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ê²°ê³¼ ì €ì¥
        results_file = self.data_dir / f"practical_performance_test_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {results_file}")
        return results_file
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        grade = results.get("performance_grade", {})
        
        report = f"""# Greeum v2.0.5 ì‹¤ìš©ì  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸

## ğŸ“Š í…ŒìŠ¤íŠ¸ ê°œìš”
- **í…ŒìŠ¤íŠ¸ ì‹œê°„**: {results['test_timestamp']}
- **ì´ ì†Œìš” ì‹œê°„**: {results['total_test_duration']:.2f}ì´ˆ
- **Greeum ë²„ì „**: {results['greeum_version']}

## ğŸ¯ ì „ì²´ ì„±ëŠ¥ ë“±ê¸‰
**{grade.get('overall_grade', 'N/A')}ë“±ê¸‰** ({grade.get('overall_score', 0):.1f}/100) - {grade.get('overall_description', '')}

## ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„

"""
        
        # ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥
        memory_search = results.get("metrics", {}).get("memory_search", {})
        if memory_search and "summary" in memory_search:
            summary = memory_search["summary"]
            category_grade = grade.get("category_grades", {}).get("memory_search", {})
            
            report += f"""### ğŸ§  ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥
- **ë“±ê¸‰**: {category_grade.get('grade', 'N/A')} ({category_grade.get('score', 0):.1f}/100)
- **í‰ê·  LTM ê²€ìƒ‰**: {summary.get('overall_ltm_avg', 0):.2f}ms
- **í‰ê·  ìºì‹œ ê²€ìƒ‰**: {summary.get('overall_cache_avg', 0):.2f}ms
- **í‰ê·  ì†ë„ í–¥ìƒ**: {summary.get('overall_speedup_avg', 1):.2f}x
- **í™•ì¥ì„± ê³„ìˆ˜**: {summary.get('scalability_factor', 1):.2f}

#### ë©”ëª¨ë¦¬ í¬ê¸°ë³„ ì„±ëŠ¥
"""
            
            for size_key in ["memory_size_100", "memory_size_500", "memory_size_1000"]:
                if size_key in memory_search:
                    data = memory_search[size_key]
                    size = data["block_count"]
                    report += f"- **{size:,}ê°œ ë¸”ë¡**: LTM {data['ltm_avg_time_ms']:.2f}ms, ìºì‹œ {data['cache_avg_time_ms']:.2f}ms, ì†ë„í–¥ìƒ {data['avg_speedup_ratio']:.2f}x\n"
        
        # ì‘ë‹µ í’ˆì§ˆ
        response_quality = results.get("metrics", {}).get("response_quality", {})
        if response_quality and "summary" in response_quality:
            summary = response_quality["summary"]
            category_grade = grade.get("category_grades", {}).get("response_quality", {})
            
            report += f"""
### ğŸ“ ì‘ë‹µ í’ˆì§ˆ
- **ë“±ê¸‰**: {category_grade.get('grade', 'N/A')} ({category_grade.get('score', 0):.1f}/100)
- **ì „ì²´ í’ˆì§ˆ ì ìˆ˜**: {summary.get('overall_quality_score', 0):.3f}
- **í‰ê·  ì»¨í…ìŠ¤íŠ¸ í™œìš©**: {summary.get('overall_context_usage', 0):.1f}ê°œ
- **í‰ê·  ì²˜ë¦¬ ì‹œê°„**: {summary.get('overall_processing_time', 0):.2f}ms

#### ì§ˆì˜ ìœ í˜•ë³„ ì„±ëŠ¥
"""
            
            for query_type in ["factual", "contextual", "analytical"]:
                if query_type in response_quality:
                    data = response_quality[query_type]
                    report += f"- **{query_type.title()}**: í’ˆì§ˆ {data['avg_quality_score']:.3f}, ì»¨í…ìŠ¤íŠ¸ {data['avg_context_usage']:.1f}ê°œ, ì²˜ë¦¬ì‹œê°„ {data['avg_processing_time_ms']:.2f}ms\n"
        
        # í™•ì¥ì„±
        scalability = results.get("metrics", {}).get("scalability", {})
        if scalability and "analysis" in scalability:
            analysis = scalability["analysis"]
            category_grade = grade.get("category_grades", {}).get("scalability", {})
            
            report += f"""
### ğŸ“ˆ í™•ì¥ì„±
- **ë“±ê¸‰**: {category_grade.get('grade', 'N/A')} ({category_grade.get('score', 0):.1f}/100)
- **ì„ í˜•ì„± ì ìˆ˜**: {analysis.get('linearity_score', 0):.3f}/1.000
- **í™•ì¥ì„± í‰ê°€**: {analysis.get('scalability_rating', 'unknown').upper()}
- **ê¸°ì¤€ ì„±ëŠ¥**: {analysis.get('base_performance_ms', 0):.2f}ms (100ë¸”ë¡)

#### ë¸”ë¡ ìˆ˜ë³„ ì„±ëŠ¥
"""
            
            for block_key in ["blocks_100", "blocks_500", "blocks_1000"]:
                if block_key in scalability:
                    data = scalability[block_key]
                    report += f"- **{data['block_count']:,}ê°œ ë¸”ë¡**: {data['avg_search_time_ms']:.2f}ms (í™•ì¥ê³„ìˆ˜ {data['scalability_factor']:.2f}x)\n"
        
        # ë™ì‹œì„±
        concurrency = results.get("metrics", {}).get("concurrency", {})
        if concurrency:
            category_grade = grade.get("category_grades", {}).get("concurrency", {})
            
            report += f"""
### âš¡ ë™ì‹œì„±
- **ë“±ê¸‰**: {category_grade.get('grade', 'N/A')} ({category_grade.get('score', 0):.1f}/100)

#### ìŠ¤ë ˆë“œë³„ ì„±ëŠ¥
"""
            
            for thread_key in ["threads_1", "threads_2", "threads_4", "threads_8"]:
                if thread_key in concurrency and "error" not in concurrency[thread_key]:
                    data = concurrency[thread_key]
                    report += f"- **{data['thread_count']}ìŠ¤ë ˆë“œ**: {data['throughput_tasks_per_sec']:.1f} ì‘ì—…/ì´ˆ, ì˜¤ë¥˜ìœ¨ {data['error_rate_pct']:.1f}%\n"
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
        system_resources = results.get("metrics", {}).get("system_resources", {})
        if system_resources and "error" not in system_resources:
            report += f"""
### ğŸ’» ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
- **í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬**: {system_resources['process_memory']['rss_mb']}MB ({system_resources['process_memory']['percent']}%)
- **ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬**: {system_resources['system_memory']['used_percent']}% ì‚¬ìš© ì¤‘
- **CPU ì‚¬ìš©ë¥ **: í”„ë¡œì„¸ìŠ¤ {system_resources['cpu_usage']['process_percent']}%, ì‹œìŠ¤í…œ {system_resources['cpu_usage']['system_percent']}%
- **ìŠ¤ë ˆë“œ ìˆ˜**: {system_resources['cpu_usage']['thread_count']}ê°œ
"""
        
        report += f"""
## ğŸ¯ ê¶Œì¥ì‚¬í•­

### ê°œì„  ìš°ì„ ìˆœìœ„
"""
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        category_grades = grade.get("category_grades", {})
        
        high_priority = []
        medium_priority = []
        low_priority = []
        
        for category, grade_info in category_grades.items():
            score = grade_info.get("score", 0)
            category_name = category.replace("_", " ").title()
            
            if score < 60:
                high_priority.append(f"{category_name} ì„±ëŠ¥ ê°œì„  ({grade_info.get('grade', 'F')}ë“±ê¸‰)")
            elif score < 75:
                medium_priority.append(f"{category_name} ìµœì í™” ê²€í†  ({grade_info.get('grade', 'C')}ë“±ê¸‰)")
            elif score < 85:
                low_priority.append(f"{category_name} ë¯¸ì„¸ ì¡°ì • ê¶Œì¥ ({grade_info.get('grade', 'B')}ë“±ê¸‰)")
        
        if high_priority:
            report += "\n#### ğŸ”´ ë†’ì€ ìš°ì„ ìˆœìœ„\n"
            for item in high_priority:
                report += f"- {item}\n"
        
        if medium_priority:
            report += "\n#### ğŸŸ¡ ì¤‘ê°„ ìš°ì„ ìˆœìœ„\n"
            for item in medium_priority:
                report += f"- {item}\n"
        
        if low_priority:
            report += "\n#### ğŸŸ¢ ë‚®ì€ ìš°ì„ ìˆœìœ„\n"
            for item in low_priority:
                report += f"- {item}\n"
        
        if not (high_priority or medium_priority or low_priority):
            report += "\n#### âœ… í˜„ì¬ ì„±ëŠ¥ ìˆ˜ì¤€ ì–‘í˜¸\n- ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ ë° ìœ ì§€ë³´ìˆ˜ ê¶Œì¥\n"
        
        report += f"""
---
**ë¦¬í¬íŠ¸ ìƒì„± ì‹œê°„**: {datetime.now().isoformat()}  
**ì¸¡ì • ì‹ ë¢°ë„**: ì‹¤ìš©ì  ìƒ˜í”Œë§ ê¸°ë°˜ ì‹ ë¢° ê°€ëŠ¥í•œ ê²°ê³¼
"""
        
        return report


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f"tests/performance_suite/results/practical_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
            logging.StreamHandler()
        ]
    )
    
    logger.info("ğŸš€ Greeum v2.0.5 ì‹¤ìš©ì  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tester = PracticalPerformanceTest()
    
    # ê· í˜•ì¡íŒ ìƒ˜í”Œ í¬ê¸°ë¡œ ì •í™•í•˜ë©´ì„œë„ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
    results = tester.run_comprehensive_test(
        memory_sample_size=120,  # ë©”ëª¨ë¦¬ ê²€ìƒ‰: ì¶©ë¶„í•œ í†µê³„ì  ì‹ ë¢°ë„
        quality_sample_size=45,  # ì‘ë‹µ í’ˆì§ˆ: ì§ˆì˜ ìœ í˜•ë³„ë¡œ ì¶©ë¶„í•œ ìƒ˜í”Œ
        concurrency_sample_size=32  # ë™ì‹œì„±: ìŠ¤ë ˆë“œë³„ë¡œ ì ì ˆí•œ ë¶€í•˜
    )
    
    # ê²°ê³¼ ì €ì¥
    results_file = tester.save_results(results)
    
    # ë¦¬í¬íŠ¸ ìƒì„± ë° ì €ì¥
    report = tester.generate_report(results)
    
    report_file = results_file.parent / f"practical_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # ê²°ê³¼ ì¶œë ¥
    grade = results.get("performance_grade", {})
    
    print(f"\n{'='*60}")
    print(f"ğŸ¯ Greeum v2.0.5 ì‹¤ìš©ì  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print(f"{'='*60}")
    print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {results['total_test_duration']:.1f}ì´ˆ")
    print(f"ğŸ† ì „ì²´ ì„±ëŠ¥ ë“±ê¸‰: {grade.get('overall_grade', 'N/A')} ({grade.get('overall_score', 0):.1f}/100)")
    print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë“±ê¸‰:")
    
    for category, grade_info in grade.get("category_grades", {}).items():
        category_name = category.replace("_", " ").title()
        print(f"   - {category_name}: {grade_info.get('grade', 'N/A')} ({grade_info.get('score', 0):.1f}/100)")
    
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {results_file.name}")
    print(f"ğŸ“„ ë¦¬í¬íŠ¸ íŒŒì¼: {report_file.name}")
    print(f"{'='*60}")
    
    logger.info(f"ì‹¤ìš©ì  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ê²°ê³¼: {results_file}, ë¦¬í¬íŠ¸: {report_file}")

if __name__ == "__main__":
    main()
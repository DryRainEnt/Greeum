#!/usr/bin/env python3
"""
Phase 1+2 í†µí•© ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸

Phase 1: ìºì‹œ ìµœì í™”
Phase 2: í•˜ì´ë¸Œë¦¬ë“œ STM
í†µí•© íš¨ê³¼ë¥¼ ì¸¡ì •í•˜ê³  ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê²€ì¦
"""

import time
import json
import threading
import gc
import os
import sys
import logging
import resource
import tracemalloc
from typing import Dict, List, Tuple, Any
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from greeum.core.block_manager import BlockManager
from greeum.core.stm_manager import STMManager  
from greeum.core.cache_manager import CacheManager
from greeum.core.prompt_wrapper import PromptWrapper
from greeum.core.database_manager import DatabaseManager
from greeum.text_utils import process_user_input
from greeum.embedding_models import get_embedding

class IntegratedPerformanceTest:
    """Phase 1+2 í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.results = {}
        self.memory_usage = []
        
        # ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘
        tracemalloc.start()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        self.test_memories = self._generate_test_memories()
        self.test_queries = self._generate_test_queries()
        
    def _setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger('integrated_performance')
        logger.setLevel(logging.INFO)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬
        log_file = f"tests/performance_suite/results/integrated_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        fh = logging.FileHandler(log_file)
        fh.setLevel(logging.INFO)
        
        # í¬ë§·í„°
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        fh.setFormatter(formatter)
        logger.addHandler(fh)
        
        return logger
    
    def _generate_test_memories(self) -> List[str]:
        """í…ŒìŠ¤íŠ¸ìš© ë©”ëª¨ë¦¬ ë°ì´í„° ìƒì„±"""
        memories = []
        
        # ì¼ë°˜ì ì¸ ëŒ€í™” íŒ¨í„´
        conversation_memories = [
            "ì˜¤ëŠ˜ ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ì •ë§ í¥ë¯¸ë¡œìš´ ë„ì „ì´ ë  ê²ƒ ê°™ì•„ìš”.",
            "Python í”„ë¡œê·¸ë˜ë°ì— ëŒ€í•´ ê³µë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì´ ì–´ë µë„¤ìš”.",
            "ìµœê·¼ì— ì½ì€ ì±…ì´ ë§¤ìš° ì¸ìƒê¹Šì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ 2ì¥ì´ ê¸°ì–µì— ë‚¨ì•„ìš”.",
            "ë‚´ì¼ ì¤‘ìš”í•œ ë¯¸íŒ…ì´ ìˆì–´ì„œ ì¤€ë¹„ë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤.",
            "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì—°êµ¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        ]
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒì„±
        for i in range(2000):
            if i < len(conversation_memories):
                memories.append(f"{conversation_memories[i]} (ë°˜ë³µ {i})")
            else:
                memories.append(f"í…ŒìŠ¤íŠ¸ ë©”ëª¨ë¦¬ í•­ëª© {i}: ì´ê²ƒì€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìƒ˜í”Œ ë°ì´í„°ì…ë‹ˆë‹¤. "
                              f"ë‹¤ì–‘í•œ í‚¤ì›Œë“œì™€ íŒ¨í„´ì„ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.")
        
        return memories
    
    def _generate_test_queries(self) -> List[str]:
        """í…ŒìŠ¤íŠ¸ìš© ì¿¼ë¦¬ ìƒì„±"""
        return [
            "í”„ë¡œì íŠ¸ ê´€ë ¨ ë‚´ìš©",
            "Python í”„ë¡œê·¸ë˜ë°",
            "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸",
            "ì±… ì¶”ì²œ",
            "ë¯¸íŒ… ì¤€ë¹„",
            "ì„±ëŠ¥ ìµœì í™”",
            "ë°ì´í„° ë¶„ì„",
            "ê°œë°œ ë„êµ¬",
            "í•™ìŠµ ê³„íš",
            "ê¸°ìˆ  íŠ¸ë Œë“œ"
        ]
    
    def _monitor_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        try:
            # resource ëª¨ë“ˆì„ ì‚¬ìš©í•œ ë©”ëª¨ë¦¬ ì •ë³´
            memory_info = resource.getrusage(resource.RUSAGE_SELF)
            
            # tracemallocì„ ì‚¬ìš©í•œ Python ë©”ëª¨ë¦¬ ì¶”ì 
            current_memory, peak_memory = tracemalloc.get_traced_memory()
            
            self.memory_usage.append({
                'timestamp': datetime.now().isoformat(),
                'max_rss_mb': memory_info.ru_maxrss / 1024 / 1024,  # macOSì—ì„œëŠ” ë°”ì´íŠ¸ ë‹¨ìœ„
                'current_memory_mb': current_memory / 1024 / 1024,
                'peak_memory_mb': peak_memory / 1024 / 1024,
                'gc_count': sum(gc.get_count()),
                'user_time': memory_info.ru_utime,
                'system_time': memory_info.ru_stime
            })
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
    
    def test_baseline_performance(self) -> Dict[str, Any]:
        """ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì • (Phase 1+2 ë¹„í™œì„±í™”)"""
        self.logger.info("ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì • ì‹œì‘")
        
        # ì„ì‹œ í™˜ê²½ ì„¤ì • (ìºì‹œ ë¹„í™œì„±í™”)
        os.environ['GREEUM_CACHE_ENABLED'] = 'false'
        os.environ['GREEUM_STM_HYBRID'] = 'false'
        
        try:
            # ì„ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©
            import tempfile
            temp_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)
            temp_db.close()
            
            db_manager = DatabaseManager(connection_string=temp_db.name)
            block_manager = BlockManager(db_manager)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€
            start_time = time.time()
            for i, memory in enumerate(self.test_memories[:100]):
                # í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ì„ë² ë”© ìƒì„±
                processed = process_user_input(memory)
                keywords = processed.get('keywords', [])
                tags = processed.get('tags', [])
                embedding = get_embedding(memory, model_name='simple')
                
                block_manager.add_block(
                    context=memory,
                    keywords=keywords,
                    tags=tags,
                    embedding=embedding,
                    importance=0.5
                )
                if i % 100 == 0:
                    self._monitor_memory_usage()
            
            add_time = time.time() - start_time
            
            # ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •
            search_times = []
            for query in self.test_queries:
                processed_query = process_user_input(query)
                keywords = processed_query.get('keywords', [])
                
                start_time = time.time()
                results = block_manager.search_by_keywords(keywords, limit=10)
                search_time = time.time() - start_time
                search_times.append(search_time)
                self._monitor_memory_usage()
            
            baseline_results = {
                'add_time': add_time,
                'avg_search_time': sum(search_times) / len(search_times),
                'total_searches': len(search_times),
                'memory_count': 100
            }
            
            self.logger.info(f"ê¸°ì¤€ ì„±ëŠ¥: {baseline_results}")
            return baseline_results
            
        finally:
            # í™˜ê²½ ë³€ìˆ˜ ë³µì›
            os.environ.pop('GREEUM_CACHE_ENABLED', None)
            os.environ.pop('GREEUM_STM_HYBRID', None)
            # ì„ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬
            try:
                os.unlink(temp_db.name)
            except:
                pass
    
    def test_integrated_performance(self) -> Dict[str, Any]:
        """í†µí•© ì„±ëŠ¥ ì¸¡ì • (Phase 1+2 í™œì„±í™”)"""
        self.logger.info("í†µí•© ì„±ëŠ¥ ì¸¡ì • ì‹œì‘")
        
        # Phase 1+2 í™œì„±í™”
        os.environ['GREEUM_CACHE_ENABLED'] = 'true'
        os.environ['GREEUM_STM_HYBRID'] = 'true'
        
        try:
            # ì„ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©
            import tempfile
            temp_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)
            temp_db.close()
            
            db_manager = DatabaseManager(connection_string=temp_db.name)
            block_manager = BlockManager(db_manager)
            stm_manager = STMManager(db_manager)
            cache_manager = CacheManager()
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€
            start_time = time.time()
            for i, memory in enumerate(self.test_memories[:100]):
                # í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ì„ë² ë”© ìƒì„±
                processed = process_user_input(memory)
                keywords = processed.get('keywords', [])
                tags = processed.get('tags', [])
                embedding = get_embedding(memory, model_name='simple')
                
                # LTMì— ì¶”ê°€
                block_manager.add_block(
                    context=memory,
                    keywords=keywords,
                    tags=tags,
                    embedding=embedding,
                    importance=0.5
                )
                
                # STMì—ë„ ì¶”ê°€ (ìµœê·¼ 100ê°œ)
                if i >= 900:
                    stm_manager.add_memory({
                    'content': memory,
                    'importance': 0.7,
                    'ttl': '1h'
                })
                
                if i % 100 == 0:
                    self._monitor_memory_usage()
            
            add_time = time.time() - start_time
            
            # ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •
            search_times = []
            cache_hits = 0
            stm_hits = 0
            
            for query in self.test_queries * 10:  # 10ë°° ë°˜ë³µìœ¼ë¡œ ìºì‹œ íš¨ê³¼ ì¸¡ì •
                processed_query = process_user_input(query)
                keywords = processed_query.get('keywords', [])
                
                start_time = time.time()
                
                # STM ê²€ìƒ‰ ë¨¼ì € (ìµœê·¼ ë©”ëª¨ë¦¬ ì¡°íšŒ)
                stm_results = stm_manager.get_recent_memories(count=5)
                if stm_results:
                    stm_hits += 1
                
                # ìºì‹œ ê²€ìƒ‰ (waypoints ì¡°íšŒ)
                cache_results = cache_manager.get_waypoints()
                if cache_results:
                    cache_hits += 1
                
                # LTM ê²€ìƒ‰
                ltm_results = block_manager.search_by_keywords(keywords, limit=10)
                
                search_time = time.time() - start_time
                search_times.append(search_time)
                
                if len(search_times) % 20 == 0:
                    self._monitor_memory_usage()
            
            integrated_results = {
                'add_time': add_time,
                'avg_search_time': sum(search_times) / len(search_times),
                'total_searches': len(search_times),
                'memory_count': 100,
                'cache_hits': cache_hits,
                'stm_hits': stm_hits,
                'cache_hit_rate': cache_hits / len(search_times),
                'stm_hit_rate': stm_hits / len(search_times)
            }
            
            self.logger.info(f"í†µí•© ì„±ëŠ¥: {integrated_results}")
            return integrated_results
            
        finally:
            os.environ.pop('GREEUM_CACHE_ENABLED', None)
            os.environ.pop('GREEUM_STM_HYBRID', None)
            # ì„ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬
            try:
                os.unlink(temp_db.name)
            except:
                pass
    
    def test_conversation_scenario(self) -> Dict[str, Any]:
        """ëŒ€í™”í˜• AI ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ëŒ€í™”í˜• AI ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # í†µí•© ì‹œìŠ¤í…œ ì„¤ì •
        os.environ['GREEUM_CACHE_ENABLED'] = 'true'
        os.environ['GREEUM_STM_HYBRID'] = 'true'
        
        try:
            db_manager = DatabaseManager()
            block_manager = BlockManager(db_manager)
            stm_manager = STMManager(db_manager)
            
            # 100íšŒ ì—°ì† ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜
            conversation_times = []
            prompt_enhancement_times = []
            
            for i in range(100):
                # ì‚¬ìš©ì ì…ë ¥ ì‹œë®¬ë ˆì´ì…˜
                user_input = f"ëŒ€í™” í„´ {i}: {self.test_queries[i % len(self.test_queries)]}ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                
                # ë©”ëª¨ë¦¬ì— ì¶”ê°€
                start_time = time.time()
                
                processed = process_user_input(user_input)
                keywords = processed.get('keywords', [])
                tags = processed.get('tags', [])
                embedding = get_embedding(user_input, model_name='simple')
                
                block_manager.add_block(
                    context=user_input,
                    keywords=keywords,
                    tags=tags,
                    embedding=embedding,
                    importance=0.6
                )
                stm_manager.add_memory({
                    'content': user_input,
                    'importance': 0.8,
                    'ttl': '30m'
                })
                add_time = time.time() - start_time
                
                # í”„ë¡¬í”„íŠ¸ ê°•í™” (ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜)
                start_time = time.time()
                # ìµœê·¼ ë©”ëª¨ë¦¬ ì¡°íšŒë¡œ í”„ë¡¬í”„íŠ¸ ê°•í™” ì‹œë®¬ë ˆì´ì…˜
                recent_memories = stm_manager.get_recent_memories(count=3)
                enhancement_time = time.time() - start_time
                
                conversation_times.append(add_time)
                prompt_enhancement_times.append(enhancement_time)
                
                if i % 10 == 0:
                    self._monitor_memory_usage()
                    gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            
            scenario_results = {
                'total_turns': 100,
                'avg_add_time': sum(conversation_times) / len(conversation_times),
                'avg_enhancement_time': sum(prompt_enhancement_times) / len(prompt_enhancement_times),
                'total_conversation_time': sum(conversation_times) + sum(prompt_enhancement_times),
                'memory_growth': len(self.memory_usage)
            }
            
            self.logger.info(f"ëŒ€í™”í˜• ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼: {scenario_results}")
            return scenario_results
            
        finally:
            os.environ.pop('GREEUM_CACHE_ENABLED', None)
            os.environ.pop('GREEUM_STM_HYBRID', None)
    
    def test_large_document_scenario(self) -> Dict[str, Any]:
        """ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤"""
        self.logger.info("ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # í° ë¬¸ì„œ ì‹œë®¬ë ˆì´ì…˜ (10,000ì)
        large_document = "ì´ê²ƒì€ ëŒ€ìš©ëŸ‰ ë¬¸ì„œì…ë‹ˆë‹¤. " * 500 + \
                        "ì¤‘ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. " * 200 + \
                        "ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë°ì´í„°ì…ë‹ˆë‹¤. " * 300
        
        os.environ['GREEUM_CACHE_ENABLED'] = 'true'
        os.environ['GREEUM_STM_HYBRID'] = 'true'
        
        try:
            db_manager = DatabaseManager()
            block_manager = BlockManager(db_manager)
            
            # ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬
            start_time = time.time()
            
            # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì €ì¥
            chunk_size = 500
            chunks = [large_document[i:i+chunk_size] 
                     for i in range(0, len(large_document), chunk_size)]
            
            chunk_times = []
            for i, chunk in enumerate(chunks):
                chunk_start = time.time()
                
                chunk_content = f"ë¬¸ì„œ ì²­í¬ {i}: {chunk}"
                processed = process_user_input(chunk_content)
                keywords = processed.get('keywords', [])
                tags = processed.get('tags', [])
                embedding = get_embedding(chunk_content, model_name='simple')
                
                block_manager.add_block(
                    context=chunk_content,
                    keywords=keywords,
                    tags=tags,
                    embedding=embedding,
                    importance=0.7
                )
                chunk_time = time.time() - chunk_start
                chunk_times.append(chunk_time)
                
                if i % 5 == 0:
                    self._monitor_memory_usage()
            
            total_processing_time = time.time() - start_time
            
            # ë¬¸ì„œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            search_queries = ["ì¤‘ìš”í•œ ì •ë³´", "ì„±ëŠ¥ í…ŒìŠ¤íŠ¸", "ëŒ€ìš©ëŸ‰ ë¬¸ì„œ", "ë°ì´í„°"]
            search_times = []
            
            for query in search_queries:
                processed_query = process_user_input(query)
                keywords = processed_query.get('keywords', [])
                
                start_time = time.time()
                results = block_manager.search_by_keywords(keywords, limit=10)
                search_time = time.time() - start_time
                search_times.append(search_time)
            
            document_results = {
                'document_size': len(large_document),
                'chunk_count': len(chunks),
                'total_processing_time': total_processing_time,
                'avg_chunk_time': sum(chunk_times) / len(chunk_times),
                'avg_search_time': sum(search_times) / len(search_times),
                'search_queries': len(search_queries)
            }
            
            self.logger.info(f"ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼: {document_results}")
            return document_results
            
        finally:
            os.environ.pop('GREEUM_CACHE_ENABLED', None)
            os.environ.pop('GREEUM_STM_HYBRID', None)
    
    def test_concurrent_access(self) -> Dict[str, Any]:
        """ë™ì‹œ ì ‘ê·¼ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ë™ì‹œ ì ‘ê·¼ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        results = {'threads': [], 'errors': []}
        
        def worker_thread(thread_id: int, operation_count: int):
            """ì›Œì»¤ ìŠ¤ë ˆë“œ"""
            try:
                db_manager = DatabaseManager()
                block_manager = BlockManager(db_manager)
                
                thread_results = {
                    'thread_id': thread_id,
                    'operations': 0,
                    'total_time': 0,
                    'errors': 0
                }
                
                start_time = time.time()
                
                for i in range(operation_count):
                    try:
                        # ë©”ëª¨ë¦¬ ì¶”ê°€
                        content = f"ìŠ¤ë ˆë“œ {thread_id} ì‘ì—… {i}: ë™ì‹œì„± í…ŒìŠ¤íŠ¸"
                        processed = process_user_input(content)
                        keywords = processed.get('keywords', [])
                        tags = processed.get('tags', [])
                        embedding = get_embedding(content, model_name='simple')
                        
                        block_manager.add_block(
                            context=content,
                            keywords=keywords,
                            tags=tags,
                            embedding=embedding,
                            importance=0.5
                        )
                        
                        # ê²€ìƒ‰
                        search_keywords = [f"ìŠ¤ë ˆë“œ", f"{thread_id}"]
                        block_manager.search_by_keywords(search_keywords, limit=5)
                        
                        thread_results['operations'] += 1
                        
                    except Exception as e:
                        thread_results['errors'] += 1
                        self.logger.error(f"ìŠ¤ë ˆë“œ {thread_id} ì˜¤ë¥˜: {e}")
                
                thread_results['total_time'] = time.time() - start_time
                results['threads'].append(thread_results)
                
            except Exception as e:
                results['errors'].append(f"ìŠ¤ë ˆë“œ {thread_id}: {str(e)}")
        
        # 5ê°œ ìŠ¤ë ˆë“œë¡œ ë™ì‹œ ì‹¤í–‰
        threads = []
        for i in range(5):
            thread = threading.Thread(target=worker_thread, args=(i, 20))
            threads.append(thread)
            thread.start()
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        for thread in threads:
            thread.join()
        
        # ê²°ê³¼ ì§‘ê³„
        total_operations = sum(t['operations'] for t in results['threads'])
        total_errors = sum(t['errors'] for t in results['threads'])
        avg_time = sum(t['total_time'] for t in results['threads']) / len(results['threads'])
        
        concurrent_results = {
            'thread_count': 5,
            'total_operations': total_operations,
            'total_errors': total_errors,
            'error_rate': total_errors / (total_operations + total_errors) if (total_operations + total_errors) > 0 else 0,
            'avg_thread_time': avg_time,
            'threads_detail': results['threads']
        }
        
        self.logger.info(f"ë™ì‹œ ì ‘ê·¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {concurrent_results}")
        return concurrent_results
    
    def test_edge_cases(self) -> Dict[str, Any]:
        """ì—ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ì—ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        edge_results = {}
        
        try:
            db_manager = DatabaseManager()
            block_manager = BlockManager(db_manager)
            
            # 1. ë¹ˆ ë©”ëª¨ë¦¬ ìƒíƒœì—ì„œ ê²€ìƒ‰
            start_time = time.time()
            empty_results = block_manager.search_by_keywords(["ì¡´ì¬í•˜ì§€ì•ŠëŠ”"], limit=10)
            empty_search_time = time.time() - start_time
            edge_results['empty_search'] = {
                'time': empty_search_time,
                'results_count': len(empty_results)
            }
            
            # 2. ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            very_long_text = "ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. " * 2000  # ì•½ 20,000ì
            start_time = time.time()
            
            processed = process_user_input(very_long_text)
            keywords = processed.get('keywords', [])
            tags = processed.get('tags', [])
            embedding = get_embedding(very_long_text, model_name='simple')
            
            block_manager.add_block(
                context=very_long_text,
                keywords=keywords,
                tags=tags,
                embedding=embedding,
                importance=0.5
            )
            long_text_time = time.time() - start_time
            edge_results['long_text'] = {
                'text_length': len(very_long_text),
                'processing_time': long_text_time
            }
            
            # 3. íŠ¹ìˆ˜ ë¬¸ì ë° ìœ ë‹ˆì½”ë“œ ì²˜ë¦¬
            special_texts = [
                "íŠ¹ìˆ˜ë¬¸ì í…ŒìŠ¤íŠ¸: !@#$%^&*()_+-=[]{}|;':\",./<>?",
                "í•œê¸€ í…ŒìŠ¤íŠ¸: ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ ì²˜ë¦¬ê°€ ì˜ ë˜ë‚˜ìš”?",
                "Japanese: ã“ã‚“ã«ã¡ã¯ã€‚æ—¥æœ¬èªã‚‚å‡¦ç†ã§ãã¾ã™ã‹ï¼Ÿ",
                "Emoji: ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ˜‚ğŸ¤£ğŸ˜ŠğŸ˜‡ğŸ™‚ğŸ™ƒğŸ˜‰ğŸ˜ŒğŸ˜ğŸ¥°ğŸ˜˜",
                "Mixed: Hello ì•ˆë…• ã“ã‚“ã«ã¡ã¯ ğŸ˜€ !@#$%"
            ]
            
            special_times = []
            for text in special_texts:
                start_time = time.time()
                
                processed = process_user_input(text)
                keywords = processed.get('keywords', [])
                tags = processed.get('tags', [])
                embedding = get_embedding(text, model_name='simple')
                
                block_manager.add_block(
                    context=text,
                    keywords=keywords,
                    tags=tags,
                    embedding=embedding,
                    importance=0.5
                )
                processing_time = time.time() - start_time
                special_times.append(processing_time)
            
            edge_results['special_characters'] = {
                'test_count': len(special_texts),
                'avg_processing_time': sum(special_times) / len(special_times),
                'max_processing_time': max(special_times)
            }
            
            # 4. ë§¤ìš° ë†’ì€ ì¤‘ìš”ë„ì™€ ë‚®ì€ ì¤‘ìš”ë„
            extreme_importance_times = []
            for importance in [0.0, 0.1, 0.9, 1.0]:
                start_time = time.time()
                
                content = f"ì¤‘ìš”ë„ {importance} í…ŒìŠ¤íŠ¸"
                processed = process_user_input(content)
                keywords = processed.get('keywords', [])
                tags = processed.get('tags', [])
                embedding = get_embedding(content, model_name='simple')
                
                block_manager.add_block(
                    context=content,
                    keywords=keywords,
                    tags=tags,
                    embedding=embedding,
                    importance=importance
                )
                processing_time = time.time() - start_time
                extreme_importance_times.append(processing_time)
            
            edge_results['extreme_importance'] = {
                'test_count': len(extreme_importance_times),
                'avg_processing_time': sum(extreme_importance_times) / len(extreme_importance_times)
            }
            
            self.logger.info(f"ì—ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {edge_results}")
            return edge_results
            
        except Exception as e:
            self.logger.error(f"ì—ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            edge_results['error'] = str(e)
            return edge_results
    
    def analyze_memory_leaks(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„"""
        self.logger.info("ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„ ì‹œì‘")
        
        if not self.memory_usage:
            return {'error': 'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°ì´í„° ì—†ìŒ'}
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì„¸ ë¶„ì„
        initial_memory = self.memory_usage[0]['current_memory_mb']
        final_memory = self.memory_usage[-1]['current_memory_mb']
        peak_memory = max(usage['peak_memory_mb'] for usage in self.memory_usage)
        max_rss = max(usage['max_rss_mb'] for usage in self.memory_usage)
        
        memory_growth = final_memory - initial_memory
        memory_growth_rate = memory_growth / len(self.memory_usage)
        
        # CPU ì‹œê°„ ë¶„ì„
        initial_user_time = self.memory_usage[0]['user_time']
        final_user_time = self.memory_usage[-1]['user_time']
        total_cpu_time = final_user_time - initial_user_time
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ë¶„ì„
        initial_gc = self.memory_usage[0]['gc_count']
        final_gc = self.memory_usage[-1]['gc_count']
        gc_activity = final_gc - initial_gc
        
        leak_analysis = {
            'initial_memory_mb': initial_memory,
            'final_memory_mb': final_memory,
            'peak_memory_mb': peak_memory,
            'max_rss_mb': max_rss,
            'memory_growth_mb': memory_growth,
            'memory_growth_rate_mb_per_sample': memory_growth_rate,
            'total_cpu_time_seconds': total_cpu_time,
            'gc_activity': gc_activity,
            'sample_count': len(self.memory_usage),
            'leak_detected': memory_growth > 50,  # 50MB ì´ìƒ ì¦ê°€ì‹œ ëˆ„ìˆ˜ ì˜ì‹¬
            'memory_efficiency': 'GOOD' if memory_growth < 10 else 'MODERATE' if memory_growth < 50 else 'POOR'
        }
        
        self.logger.info(f"ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„ ê²°ê³¼: {leak_analysis}")
        return leak_analysis
    
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("=== Phase 1+2 í†µí•© ì„±ëŠ¥ ê²€ì¦ ì‹œì‘ ===")
        
        comprehensive_results = {
            'test_start_time': datetime.now().isoformat(),
            'greeum_version': '2.0.5',
            'test_type': 'Phase 1+2 Integration Performance Test'
        }
        
        try:
            # 1. ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì •
            self.logger.info("1. ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì •")
            comprehensive_results['baseline'] = self.test_baseline_performance()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            time.sleep(1)
            
            # 2. í†µí•© ì„±ëŠ¥ ì¸¡ì •
            self.logger.info("2. í†µí•© ì„±ëŠ¥ ì¸¡ì •")
            comprehensive_results['integrated'] = self.test_integrated_performance()
            
            # 3. ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
            self.logger.info("3. ëŒ€í™”í˜• AI ì‹œë‚˜ë¦¬ì˜¤")
            comprehensive_results['conversation_scenario'] = self.test_conversation_scenario()
            
            self.logger.info("4. ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì‹œë‚˜ë¦¬ì˜¤")
            comprehensive_results['document_scenario'] = self.test_large_document_scenario()
            
            # 4. ë™ì‹œì„± í…ŒìŠ¤íŠ¸
            self.logger.info("5. ë™ì‹œ ì ‘ê·¼ í…ŒìŠ¤íŠ¸")
            comprehensive_results['concurrent_access'] = self.test_concurrent_access()
            
            # 5. ì—ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸
            self.logger.info("6. ì—ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸")
            comprehensive_results['edge_cases'] = self.test_edge_cases()
            
            # 6. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„
            self.logger.info("7. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„")
            comprehensive_results['memory_analysis'] = self.analyze_memory_leaks()
            
            # ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ê³„ì‚°
            baseline_search = comprehensive_results['baseline']['avg_search_time']
            integrated_search = comprehensive_results['integrated']['avg_search_time']
            performance_improvement = baseline_search / integrated_search if integrated_search > 0 else 0
            
            comprehensive_results['performance_summary'] = {
                'baseline_search_time': baseline_search,
                'integrated_search_time': integrated_search,
                'performance_improvement_ratio': performance_improvement,
                'cache_hit_rate': comprehensive_results['integrated'].get('cache_hit_rate', 0),
                'stm_hit_rate': comprehensive_results['integrated'].get('stm_hit_rate', 0)
            }
            
            comprehensive_results['test_end_time'] = datetime.now().isoformat()
            self.logger.info("=== í†µí•© ì„±ëŠ¥ ê²€ì¦ ì™„ë£Œ ===")
            
            return comprehensive_results
            
        except Exception as e:
            self.logger.error(f"ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            comprehensive_results['error'] = str(e)
            comprehensive_results['test_end_time'] = datetime.now().isoformat()
            return comprehensive_results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    test = IntegratedPerformanceTest()
    results = test.run_comprehensive_test()
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    result_file = f"tests/performance_suite/results/integrated_performance_test_{timestamp}.json"
    
    os.makedirs(os.path.dirname(result_file), exist_ok=True)
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ê°„ë‹¨í•œ ë¦¬í¬íŠ¸ ì¶œë ¥
    print("\n=== Phase 1+2 í†µí•© ì„±ëŠ¥ ê²€ì¦ ê²°ê³¼ ===")
    
    if 'performance_summary' in results:
        summary = results['performance_summary']
        print(f"ê¸°ì¤€ ê²€ìƒ‰ ì‹œê°„: {summary['baseline_search_time']:.4f}ì´ˆ")
        print(f"í†µí•© ê²€ìƒ‰ ì‹œê°„: {summary['integrated_search_time']:.4f}ì´ˆ")
        print(f"ì„±ëŠ¥ ê°œì„  ë¹„ìœ¨: {summary['performance_improvement_ratio']:.2f}ë°°")
        print(f"ìºì‹œ ì ì¤‘ë¥ : {summary['cache_hit_rate']:.1%}")
        print(f"STM ì ì¤‘ë¥ : {summary['stm_hit_rate']:.1%}")
    
    if 'memory_analysis' in results:
        memory = results['memory_analysis']
        print(f"ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {memory.get('memory_efficiency', 'UNKNOWN')}")
        print(f"ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€: {'ì˜ˆ' if memory.get('leak_detected', False) else 'ì•„ë‹ˆì˜¤'}")
    
    print(f"\nìƒì„¸ ê²°ê³¼ ì €ì¥: {result_file}")
    
    return results

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
Greeum v2.0.5 ì •ë°€ ì„±ëŠ¥ ê¸°ì¤€ì  ì¸¡ì • ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ í†µê³„ì ìœ¼ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ ì§€í‘œë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´
ëŒ€ìš©ëŸ‰ ìƒ˜í”Œê³¼ ì •ë°€í•œ ì¸¡ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
import statistics
from pathlib import Path
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.abspath('../../..'))

from greeum import DatabaseManager, BlockManager, STMManager, CacheManager
from greeum.core.prompt_wrapper import PromptWrapper
from greeum.core.quality_validator import QualityValidator
from greeum.core.duplicate_detector import DuplicateDetector
from greeum.core.usage_analytics import UsageAnalytics
from greeum.embedding_models import get_embedding
import numpy as np

logger = logging.getLogger(__name__)

class ComprehensiveBaselineTracker:
    """ì •ë°€í•œ ì„±ëŠ¥ ê¸°ì¤€ì  ì¸¡ì •ì„ ìœ„í•œ ì¢…í•© ì¶”ì ê¸°"""
    
    def __init__(self, data_dir: str = "tests/performance_suite/results/baselines"):
        """ì´ˆê¸°í™”"""
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Greeum ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.db_manager = DatabaseManager()
        self.block_manager = BlockManager(self.db_manager)
        self.stm_manager = STMManager(self.db_manager)
        self.cache_manager = CacheManager(block_manager=self.block_manager, stm_manager=self.stm_manager)
        self.prompt_wrapper = PromptWrapper(self.cache_manager, self.stm_manager)
        
        # v2.0.5 ì‹ ê·œ ì»´í¬ë„ŒíŠ¸ (ì—ëŸ¬ ì²˜ë¦¬)
        try:
            self.quality_validator = QualityValidator()
        except Exception as e:
            logger.warning(f"QualityValidator ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.quality_validator = None
            
        try:
            self.duplicate_detector = DuplicateDetector(self.db_manager)
        except Exception as e:
            logger.warning(f"DuplicateDetector ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.duplicate_detector = None
            
        try:
            self.usage_analytics = UsageAnalytics()
        except Exception as e:
            logger.warning(f"UsageAnalytics ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.usage_analytics = None
        
        # í†µê³„ì  ì‹ ë¢°ë„ë¥¼ ìœ„í•œ ì„¤ì •
        self.min_sample_size = 100
        self.confidence_level = 0.95
        self.warmup_iterations = 10  # ì‹œìŠ¤í…œ ì•ˆì •í™”ë¥¼ ìœ„í•œ ì›œì—…
        
        logger.info(f"ComprehensiveBaselineTracker ì´ˆê¸°í™” ì™„ë£Œ - ë°ì´í„° ë””ë ‰í† ë¦¬: {self.data_dir}")
    
    def measure_comprehensive_performance(self, sample_size: int = 500) -> Dict[str, Any]:
        """
        ì¢…í•©ì ì´ê³  ì •ë°€í•œ ì„±ëŠ¥ ì¸¡ì •
        
        Args:
            sample_size: ì¸¡ì • ìƒ˜í”Œ í¬ê¸° (í†µê³„ì  ì‹ ë¢°ë„ë¥¼ ìœ„í•´ 500+ ê¶Œì¥)
        """
        if sample_size < self.min_sample_size:
            logger.warning(f"ìƒ˜í”Œ í¬ê¸° {sample_size}ê°€ ìµœì†Œ ê¶Œì¥ì¹˜ {self.min_sample_size}ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.")
            
        logger.info(f"ì¢…í•© ì„±ëŠ¥ ì¸¡ì • ì‹œì‘ - ìƒ˜í”Œ í¬ê¸°: {sample_size}")
        start_time = time.time()
        
        performance_data = {
            "measurement_timestamp": datetime.now().isoformat(),
            "greeum_version": "2.0.5",
            "sample_size": sample_size,
            "confidence_level": self.confidence_level,
            "metrics": {}
        }
        
        # ì‹œìŠ¤í…œ ì›œì—…
        logger.info("ì‹œìŠ¤í…œ ì›œì—… ìˆ˜í–‰ ì¤‘...")
        self._perform_warmup()
        
        # 1. ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ ì •ë°€ ì¸¡ì •
        logger.info("ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ ì •ë°€ ì¸¡ì • ì¤‘...")
        performance_data["metrics"]["memory_search"] = self._measure_memory_search_comprehensive(sample_size)
        
        # 2. ì‘ë‹µ í’ˆì§ˆ ë‹¤ê°ë„ ì¸¡ì •
        logger.info("ì‘ë‹µ í’ˆì§ˆ ë‹¤ê°ë„ ì¸¡ì • ì¤‘...")
        performance_data["metrics"]["response_quality"] = self._measure_response_quality_comprehensive(sample_size // 10)
        
        # 3. ì»¨ì»¤ëŸ°ì‹œ ì„±ëŠ¥ ì¸¡ì •
        logger.info("ë™ì‹œì„± ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        performance_data["metrics"]["concurrency"] = self._measure_concurrency_performance(sample_size // 20)
        
        # 4. í™•ì¥ì„± ì¸¡ì • (ë©”ëª¨ë¦¬ ë¸”ë¡ ìˆ˜ ì¦ê°€ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”)
        logger.info("í™•ì¥ì„± ì¸¡ì • ì¤‘...")
        performance_data["metrics"]["scalability"] = self._measure_scalability_performance()
        
        # 5. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë°€ ëª¨ë‹ˆí„°ë§
        logger.info("ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë°€ ëª¨ë‹ˆí„°ë§ ì¤‘...")
        performance_data["metrics"]["system_resources"] = self._measure_system_resources_comprehensive()
        
        # 6. v2.0.5 ì‹ ê·œ ê¸°ëŠ¥ ì„±ëŠ¥ (ê°œì„ ëœ ì¸¡ì •)
        if self.quality_validator or self.duplicate_detector:
            logger.info("v2.0.5 ì‹ ê·œ ê¸°ëŠ¥ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
            performance_data["metrics"]["v205_features"] = self._measure_v205_features_comprehensive(sample_size)
        
        total_time = time.time() - start_time
        performance_data["measurement_duration"] = total_time
        
        # í†µê³„ì  ì‹ ë¢°ë„ ê³„ì‚°
        performance_data["statistical_confidence"] = self._calculate_statistical_confidence(performance_data["metrics"])
        
        logger.info(f"ì¢…í•© ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ - ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
        return performance_data
    
    def _perform_warmup(self):
        """ì‹œìŠ¤í…œ ì›œì—… ìˆ˜í–‰"""
        warmup_queries = ["ì›œì—… ì¿¼ë¦¬ " + str(i) for i in range(self.warmup_iterations)]
        
        for query in warmup_queries:
            embedding = get_embedding(query)
            self.block_manager.search_by_embedding(embedding, top_k=5)
            self.cache_manager.update_cache(query, embedding, query.split())
    
    def _measure_memory_search_comprehensive(self, sample_size: int) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ ì¢…í•© ì¸¡ì •"""
        # ë‹¤ì–‘í•œ í¬ê¸°ì˜ ë©”ëª¨ë¦¬ ë¸”ë¡ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ (ì‹¤ìš©ì  í¬ê¸°ë¡œ ì¡°ì •)
        memory_sizes = [100, 500, 1000]
        results = {}
        
        for memory_size in memory_sizes:
            logger.info(f"ë©”ëª¨ë¦¬ í¬ê¸° {memory_size}ì—ì„œ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
            
            # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì¤€ë¹„
            self._prepare_test_memory_blocks(memory_size, clear_existing=True)
            
            # ë‹¤ì–‘í•œ ê²€ìƒ‰ íŒ¨í„´ í…ŒìŠ¤íŠ¸
            search_patterns = [
                "ë‹¨ì¼ í‚¤ì›Œë“œ ê²€ìƒ‰",
                "ë³µí•© í‚¤ì›Œë“œ ê²€ìƒ‰ í”„ë¡œì íŠ¸ ê°œë°œ",
                "ê¸´ ë¬¸ì¥ ê²€ìƒ‰ ìµœì‹  í”„ë¡œì íŠ¸ì˜ ì§„í–‰ ìƒí™©ê³¼ ê°œë°œ ê³„íšì— ëŒ€í•´",
                "íŠ¹ìˆ˜ ë¬¸ì í¬í•¨ ê²€ìƒ‰ v2.0.5 ì—…ë°ì´íŠ¸",
                "ìˆ«ì í¬í•¨ ê²€ìƒ‰ 2025ë…„ ê³„íš"
            ]
            
            ltm_times = []
            cache_times = []
            
            for i in range(sample_size // len(memory_sizes)):
                pattern = search_patterns[i % len(search_patterns)]
                query = f"{pattern} - {i}"
                embedding = get_embedding(query)
                
                # LTM ì§ì ‘ ê²€ìƒ‰ (2íšŒ ì¸¡ì • í›„ ì¤‘ê°„ê°’ìœ¼ë¡œ ì •í™•ë„ ìœ ì§€)
                ltm_measurements = []
                for _ in range(2):
                    start_time = time.perf_counter()
                    ltm_results = self.block_manager.search_by_embedding(embedding, top_k=5)
                    ltm_time = time.perf_counter() - start_time
                    ltm_measurements.append(ltm_time * 1000)
                
                ltm_times.append(statistics.median(ltm_measurements))
                
                # Cache ê²€ìƒ‰ (2íšŒ ì¸¡ì • í›„ ì¤‘ê°„ê°’ìœ¼ë¡œ ì •í™•ë„ ìœ ì§€)
                cache_measurements = []
                for _ in range(2):
                    start_time = time.perf_counter()
                    cache_results = self.cache_manager.update_cache(query, embedding, query.split()[:3])
                    cache_time = time.perf_counter() - start_time
                    cache_measurements.append(cache_time * 1000)
                
                cache_times.append(statistics.median(cache_measurements))
            
            # í†µê³„ ê³„ì‚°
            results[f"memory_size_{memory_size}"] = {
                "avg_ltm_search_time": statistics.mean(ltm_times),
                "median_ltm_search_time": statistics.median(ltm_times),
                "p95_ltm_search_time": statistics.quantiles(ltm_times, n=20)[18] if len(ltm_times) > 20 else max(ltm_times),
                "p99_ltm_search_time": statistics.quantiles(ltm_times, n=100)[98] if len(ltm_times) > 100 else max(ltm_times),
                "stdev_ltm_search_time": statistics.stdev(ltm_times) if len(ltm_times) > 1 else 0,
                
                "avg_cache_search_time": statistics.mean(cache_times),
                "median_cache_search_time": statistics.median(cache_times),
                "p95_cache_search_time": statistics.quantiles(cache_times, n=20)[18] if len(cache_times) > 20 else max(cache_times),
                "p99_cache_search_time": statistics.quantiles(cache_times, n=100)[98] if len(cache_times) > 100 else max(cache_times),
                "stdev_cache_search_time": statistics.stdev(cache_times) if len(cache_times) > 1 else 0,
                
                "avg_speedup_ratio": statistics.mean(ltm_times) / statistics.mean(cache_times) if statistics.mean(cache_times) > 0 else 1,
                "sample_count": len(ltm_times),
                "memory_block_count": memory_size
            }
        
        return results
    
    def _measure_response_quality_comprehensive(self, sample_size: int) -> Dict[str, Any]:
        """ì‘ë‹µ í’ˆì§ˆ ì¢…í•© ì¸¡ì •"""
        # ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì§ˆì˜ì— ëŒ€í•œ í’ˆì§ˆ ì¸¡ì •
        query_types = {
            "factual": [
                "í”„ë¡œì íŠ¸ì˜ í˜„ì¬ ìƒíƒœëŠ”?",
                "ìµœê·¼ ì—…ë°ì´íŠ¸ ë‚´ìš©ì€?",
                "ê°œë°œ ì§„í–‰ë¥ ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
            ],
            "contextual": [
                "ì´ì „ì— ë…¼ì˜í–ˆë˜ ì„±ëŠ¥ ì´ìŠˆê°€ í•´ê²°ëë‚˜ìš”?",
                "ì§€ë‚œì£¼ ê³„íší–ˆë˜ ì‘ì—…ë“¤ì€ ì™„ë£Œëë‚˜ìš”?",
                "ì•ì„œ ì–¸ê¸‰í•œ ê°œì„ ì‚¬í•­ì´ ì ìš©ëë‚˜ìš”?"
            ],
            "analytical": [
                "í˜„ì¬ ì„±ëŠ¥ê³¼ ëª©í‘œ ì„±ëŠ¥ì˜ ì°¨ì´ëŠ”?",
                "ì–´ë–¤ ë¶€ë¶„ì´ ê°€ì¥ ê°œì„ ì´ í•„ìš”í•œê°€ìš”?",
                "ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ìš°ì„ ìˆœìœ„ëŠ”?"
            ]
        }
        
        results = {}
        
        for query_type, queries in query_types.items():
            logger.info(f"{query_type} íƒ€ì… ì§ˆì˜ í’ˆì§ˆ ì¸¡ì • ì¤‘...")
            
            quality_scores = []
            context_usage_scores = []
            response_completeness_scores = []
            
            for i in range(sample_size // len(query_types)):
                query = queries[i % len(queries)]
                
                # ë©”ëª¨ë¦¬ í™œìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
                enhanced_prompt = self.prompt_wrapper.compose_prompt(query, token_budget=2000)
                
                # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ë‹¤ì–‘í•œ ì§€í‘œ)
                if self.quality_validator:
                    try:
                        quality_result = self.quality_validator.validate_memory_quality(enhanced_prompt)
                        quality_scores.append(quality_result.get('overall_score', 0))
                    except:
                        quality_scores.append(0)
                else:
                    # ê¸°ë³¸ í’ˆì§ˆ ì¸¡ì • (í”„ë¡¬í”„íŠ¸ ê¸¸ì´, ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€ ë“±)
                    basic_quality = min(1.0, len(enhanced_prompt) / 1000)  # ê¸¸ì´ ê¸°ë°˜ í’ˆì§ˆ
                    quality_scores.append(basic_quality)
                
                # ì»¨í…ìŠ¤íŠ¸ í™œìš©ë„ ì¸¡ì •
                context_indicators = enhanced_prompt.count('ê´€ë ¨ ê¸°ì–µ:') + enhanced_prompt.count('ìµœê·¼ ê¸°ì–µ:')
                context_usage_scores.append(context_indicators)
                
                # ì‘ë‹µ ì™„ì„±ë„ ì¸¡ì • (ì •ë³´ ë°€ë„)
                info_density = len([word for word in enhanced_prompt.split() if len(word) > 3]) / len(enhanced_prompt.split())
                response_completeness_scores.append(info_density)
            
            # í†µê³„ ê³„ì‚°
            results[query_type] = {
                "avg_quality_score": statistics.mean(quality_scores) if quality_scores else 0,
                "median_quality_score": statistics.median(quality_scores) if quality_scores else 0,
                "stdev_quality_score": statistics.stdev(quality_scores) if len(quality_scores) > 1 else 0,
                
                "avg_context_usage": statistics.mean(context_usage_scores) if context_usage_scores else 0,
                "avg_completeness": statistics.mean(response_completeness_scores) if response_completeness_scores else 0,
                
                "sample_count": len(quality_scores)
            }
        
        return results
    
    def _measure_concurrency_performance(self, sample_size: int) -> Dict[str, Any]:
        """ë™ì‹œì„± ì„±ëŠ¥ ì¸¡ì •"""
        thread_counts = [1, 2, 4, 8, 16]
        results = {}
        
        for thread_count in thread_counts:
            logger.info(f"{thread_count}ê°œ ìŠ¤ë ˆë“œë¡œ ë™ì‹œì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            def concurrent_search_task(task_id):
                """ë™ì‹œ ê²€ìƒ‰ ì‘ì—…"""
                query = f"ë™ì‹œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ {task_id}"
                embedding = get_embedding(query)
                
                start_time = time.perf_counter()
                ltm_results = self.block_manager.search_by_embedding(embedding, top_k=5)
                cache_results = self.cache_manager.update_cache(query, embedding, query.split())
                end_time = time.perf_counter()
                
                return {
                    "task_id": task_id,
                    "execution_time": (end_time - start_time) * 1000,
                    "ltm_result_count": len(ltm_results),
                    "cache_result_count": len(cache_results)
                }
            
            # ë™ì‹œ ì‹¤í–‰
            execution_times = []
            error_count = 0
            
            start_time = time.perf_counter()
            
            with ThreadPoolExecutor(max_workers=thread_count) as executor:
                futures = [executor.submit(concurrent_search_task, i) for i in range(sample_size // len(thread_counts))]
                
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=30)
                        execution_times.append(result["execution_time"])
                    except Exception as e:
                        error_count += 1
                        logger.warning(f"ë™ì‹œì„± í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            
            total_time = time.perf_counter() - start_time
            
            # í†µê³„ ê³„ì‚°
            if execution_times:
                results[f"threads_{thread_count}"] = {
                    "avg_task_time": statistics.mean(execution_times),
                    "median_task_time": statistics.median(execution_times),
                    "p95_task_time": statistics.quantiles(execution_times, n=20)[18] if len(execution_times) > 20 else max(execution_times),
                    "total_execution_time": total_time * 1000,
                    "throughput_tasks_per_sec": len(execution_times) / total_time,
                    "error_rate": error_count / (len(execution_times) + error_count) * 100,
                    "thread_count": thread_count,
                    "completed_tasks": len(execution_times),
                    "failed_tasks": error_count
                }
            else:
                results[f"threads_{thread_count}"] = {
                    "error": "ëª¨ë“  ì‘ì—… ì‹¤íŒ¨",
                    "thread_count": thread_count,
                    "failed_tasks": error_count
                }
        
        return results
    
    def _measure_scalability_performance(self) -> Dict[str, Any]:
        """í™•ì¥ì„± ì„±ëŠ¥ ì¸¡ì • (ë©”ëª¨ë¦¬ ë¸”ë¡ ìˆ˜ ì¦ê°€ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”)"""
        block_counts = [100, 500, 1000, 2000, 5000]
        results = {}
        
        for block_count in block_counts:
            logger.info(f"{block_count}ê°œ ë¸”ë¡ í™˜ê²½ì—ì„œ í™•ì¥ì„± ì¸¡ì • ì¤‘...")
            
            # í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì„±
            self._prepare_test_memory_blocks(block_count, clear_existing=True)
            
            # í‘œì¤€ ê²€ìƒ‰ ì‘ì—… ìˆ˜í–‰
            test_queries = [
                "í”„ë¡œì íŠ¸ ìƒíƒœ",
                "ê°œë°œ ê³„íš",
                "ì„±ëŠ¥ ìµœì í™”",
                "ë²„ê·¸ ìˆ˜ì •",
                "ê¸°ëŠ¥ ì¶”ê°€"
            ]
            
            search_times = []
            
            for query in test_queries * 20:  # ê° ì¿¼ë¦¬ë¥¼ 20ë²ˆì”© ë°˜ë³µ
                embedding = get_embedding(query)
                
                start_time = time.perf_counter()
                results_ltm = self.block_manager.search_by_embedding(embedding, top_k=5)
                search_time = time.perf_counter() - start_time
                
                search_times.append(search_time * 1000)
            
            # í†µê³„ ê³„ì‚°
            results[f"blocks_{block_count}"] = {
                "avg_search_time": statistics.mean(search_times),
                "median_search_time": statistics.median(search_times),
                "p95_search_time": statistics.quantiles(search_times, n=20)[18] if len(search_times) > 20 else max(search_times),
                "block_count": block_count,
                "search_efficiency": block_count / statistics.mean(search_times),  # ë¸”ë¡ë‹¹ ê²€ìƒ‰ íš¨ìœ¨ì„±
                "sample_count": len(search_times)
            }
        
        # í™•ì¥ì„± ê³„ìˆ˜ ê³„ì‚° (ë¸”ë¡ ìˆ˜ ì¦ê°€ì— ë”°ë¥¸ ì„±ëŠ¥ ì €í•˜ ì •ë„)
        base_performance = results["blocks_100"]["avg_search_time"]
        scalability_factors = {}
        
        for block_count in block_counts[1:]:
            current_performance = results[f"blocks_{block_count}"]["avg_search_time"]
            scalability_factor = current_performance / base_performance
            scalability_factors[f"blocks_{block_count}"] = scalability_factor
        
        results["scalability_analysis"] = {
            "base_performance_100_blocks": base_performance,
            "scalability_factors": scalability_factors,
            "linear_scalability_score": self._calculate_scalability_score(scalability_factors, block_counts[1:])
        }
        
        return results
    
    def _calculate_scalability_score(self, scalability_factors: Dict, block_counts: List[int]) -> float:
        """í™•ì¥ì„± ì ìˆ˜ ê³„ì‚° (1.0 = ì™„ë²½í•œ ì„ í˜• í™•ì¥ì„±)"""
        if not scalability_factors:
            return 1.0
        
        # ì´ìƒì ì¸ ì„ í˜• í™•ì¥ì„±ê³¼ ì‹¤ì œ í™•ì¥ì„± ë¹„êµ
        ideal_factors = [count / 100 for count in block_counts]  # 100 ë¸”ë¡ ê¸°ì¤€ ì„ í˜• ì¦ê°€
        actual_factors = [scalability_factors[f"blocks_{count}"] for count in block_counts]
        
        # í‰ê·  í¸ì°¨ ê³„ì‚°
        deviations = [abs(ideal - actual) for ideal, actual in zip(ideal_factors, actual_factors)]
        avg_deviation = statistics.mean(deviations)
        
        # ì ìˆ˜ ê³„ì‚° (í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        scalability_score = max(0, 1 - avg_deviation)
        return round(scalability_score, 3)
    
    def _measure_system_resources_comprehensive(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë°€ ëª¨ë‹ˆí„°ë§"""
        try:
            import psutil
            import gc
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìˆ˜í–‰
            gc.collect()
            
            # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ì •ë³´
            process = psutil.Process()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìƒì„¸ ì¸¡ì •
            memory_info = process.memory_info()
            memory_percent = process.memory_percent()
            
            # CPU ì‚¬ìš©ëŸ‰ ì¸¡ì • (1ì´ˆê°„ ëª¨ë‹ˆí„°ë§)
            cpu_percent_1s = process.cpu_percent(interval=1)
            
            # ì‹œìŠ¤í…œ ì „ì²´ ì •ë³´
            system_memory = psutil.virtual_memory()
            system_cpu = psutil.cpu_percent(interval=1)
            
            # ë””ìŠ¤í¬ I/O ì •ë³´
            io_counters = process.io_counters() if hasattr(process, 'io_counters') else None
            
            return {
                "process_memory": {
                    "rss_mb": memory_info.rss / 1024 / 1024,
                    "vms_mb": memory_info.vms / 1024 / 1024,
                    "percent": memory_percent,
                    "available_mb": system_memory.available / 1024 / 1024,
                    "total_mb": system_memory.total / 1024 / 1024
                },
                "process_cpu": {
                    "percent_1s": cpu_percent_1s,
                    "num_threads": process.num_threads(),
                    "system_cpu_percent": system_cpu
                },
                "io_counters": {
                    "read_count": io_counters.read_count if io_counters else 0,
                    "write_count": io_counters.write_count if io_counters else 0,
                    "read_bytes": io_counters.read_bytes if io_counters else 0,
                    "write_bytes": io_counters.write_bytes if io_counters else 0
                } if io_counters else {"error": "IO counters not available"},
                "file_descriptors": process.num_fds() if hasattr(process, 'num_fds') else 0,
                "measurement_timestamp": datetime.now().isoformat()
            }
            
        except ImportError:
            logger.warning("psutil ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"error": "psutil not available"}
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¸¡ì • ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def _measure_v205_features_comprehensive(self, sample_size: int) -> Dict[str, Any]:
        """v2.0.5 ì‹ ê·œ ê¸°ëŠ¥ ì¢…í•© ì„±ëŠ¥ ì¸¡ì •"""
        results = {}
        
        # í’ˆì§ˆ ê²€ì¦ ì„±ëŠ¥
        if self.quality_validator:
            logger.info("í’ˆì§ˆ ê²€ì¦ ì„±ëŠ¥ ì •ë°€ ì¸¡ì • ì¤‘...")
            
            test_contents = [
                "ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ë‚´ìš©ì…ë‹ˆë‹¤.",
                "ë³µì¡í•˜ê³  ìƒì„¸í•œ í”„ë¡œì íŠ¸ ê°œë°œ ì§„í–‰ ìƒí™©ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë³´ê³ ì„œì…ë‹ˆë‹¤.",
                "ì§§ì€ ë©”ëª¨",
                "ë§¤ìš° ê¸´ ë¬¸ì„œ ë‚´ìš©ìœ¼ë¡œì„œ ë‹¤ì–‘í•œ ì •ë³´ì™€ ë°ì´í„°, ë¶„ì„ ê²°ê³¼, ê²°ë¡  ë“±ì„ í¬í•¨í•˜ê³  ìˆìœ¼ë©° ì „ì²´ì ì¸ ë§¥ë½ê³¼ íë¦„ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ì‹œê°„ê³¼ ì§‘ì¤‘ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                "ì¤‘ê°„ ê¸¸ì´ì˜ ì„¤ëª… ë¬¸ì„œë¡œì„œ ì ì ˆí•œ ìˆ˜ì¤€ì˜ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤."
            ]
            
            validation_times = []
            quality_scores = []
            
            for i in range(sample_size):
                content = test_contents[i % len(test_contents)] + f" (í…ŒìŠ¤íŠ¸ {i})"
                
                # ì—¬ëŸ¬ ë²ˆ ì¸¡ì •í•˜ì—¬ ì •í™•ë„ í–¥ìƒ
                measurements = []
                for _ in range(5):
                    start_time = time.perf_counter()
                    try:
                        result = self.quality_validator.validate_memory_quality(content)
                        validation_time = time.perf_counter() - start_time
                        measurements.append(validation_time * 1000)
                        quality_scores.append(result.get('overall_score', 0))
                    except Exception as e:
                        logger.warning(f"í’ˆì§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                        measurements.append(0)
                        quality_scores.append(0)
                
                validation_times.append(statistics.median(measurements))
            
            results["quality_validation"] = {
                "avg_validation_time": statistics.mean(validation_times),
                "median_validation_time": statistics.median(validation_times),
                "p95_validation_time": statistics.quantiles(validation_times, n=20)[18] if len(validation_times) > 20 else max(validation_times),
                "stdev_validation_time": statistics.stdev(validation_times) if len(validation_times) > 1 else 0,
                "avg_quality_score": statistics.mean(quality_scores) if quality_scores else 0,
                "validation_throughput": sample_size / (sum(validation_times) / 1000),
                "sample_count": sample_size
            }
        
        # ì¤‘ë³µ ê°ì§€ ì„±ëŠ¥
        if self.duplicate_detector:
            logger.info("ì¤‘ë³µ ê°ì§€ ì„±ëŠ¥ ì •ë°€ ì¸¡ì • ì¤‘...")
            
            # ì‹¤ì œì ì¸ ì¤‘ë³µ íŒ¨í„´ ìƒì„±
            base_contents = [
                "í”„ë¡œì íŠ¸ ê°œë°œì´ ìˆœì¡°ë¡­ê²Œ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤",
                "ìƒˆë¡œìš´ ê¸°ëŠ¥ êµ¬í˜„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤",
                "ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤",
                "ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ê°œì„ í–ˆìŠµë‹ˆë‹¤",
                "ë‹¤ìŒ ë²„ì „ ì¶œì‹œë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤"
            ]
            
            test_contents = []
            expected_duplicates = 0
            
            # 50% ì¤‘ë³µ, 50% ê³ ìœ  ì»¨í…ì¸  ìƒì„±
            for i in range(sample_size):
                if i % 3 == 0:  # 33% ì¤‘ë³µ
                    content = base_contents[i % len(base_contents)]
                    expected_duplicates += 1
                elif i % 3 == 1:  # 33% ìœ ì‚¬ (ì•½ê°„ ë³€í˜•)
                    content = base_contents[i % len(base_contents)] + f" - ì—…ë°ì´íŠ¸ {i}"
                    expected_duplicates += 1
                else:  # 33% ì™„ì „ ê³ ìœ 
                    content = f"ì™„ì „íˆ ìƒˆë¡œìš´ ê³ ìœ  ì»¨í…ì¸  {i} - {datetime.now().microsecond}"
                
                test_contents.append(content)
            
            detection_times = []
            duplicates_detected = 0
            
            for content in test_contents:
                # ì—¬ëŸ¬ ë²ˆ ì¸¡ì •í•˜ì—¬ ì •í™•ë„ í–¥ìƒ
                measurements = []
                for _ in range(3):
                    start_time = time.perf_counter()
                    try:
                        duplicate_result = self.duplicate_detector.check_duplicate(content)
                        detection_time = time.perf_counter() - start_time
                        measurements.append(detection_time * 1000)
                        
                        if duplicate_result.get('is_duplicate', False):
                            duplicates_detected += 1
                    except Exception as e:
                        logger.warning(f"ì¤‘ë³µ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
                        measurements.append(0)
                
                detection_times.append(statistics.median(measurements))
            
            # ì •í™•ë„ ê³„ì‚°
            detection_accuracy = (duplicates_detected / expected_duplicates * 100) if expected_duplicates > 0 else 0
            
            results["duplicate_detection"] = {
                "avg_detection_time": statistics.mean(detection_times),
                "median_detection_time": statistics.median(detection_times),
                "p95_detection_time": statistics.quantiles(detection_times, n=20)[18] if len(detection_times) > 20 else max(detection_times),
                "stdev_detection_time": statistics.stdev(detection_times) if len(detection_times) > 1 else 0,
                "detection_accuracy": detection_accuracy,
                "detection_throughput": sample_size / (sum(detection_times) / 1000),
                "expected_duplicates": expected_duplicates,
                "detected_duplicates": duplicates_detected,
                "sample_count": sample_size
            }
        
        return results
    
    def _prepare_test_memory_blocks(self, count: int, clear_existing: bool = False):
        """í…ŒìŠ¤íŠ¸ìš© ë©”ëª¨ë¦¬ ë¸”ë¡ ì¤€ë¹„ (ê°œì„ ëœ ë²„ì „)"""
        if clear_existing:
            # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ë¸”ë¡ ì •ë¦¬ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì£¼ì˜ í•„ìš”)
            logger.info("ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ë¸”ë¡ ì •ë¦¬ ì¤‘...")
        
        logger.info(f"í…ŒìŠ¤íŠ¸ìš© ë©”ëª¨ë¦¬ ë¸”ë¡ {count}ê°œ ì¤€ë¹„ ì¤‘...")
        
        # ë” í˜„ì‹¤ì ì¸ ì»¨í…ì¸  íŒ¨í„´
        content_templates = [
            "í”„ë¡œì íŠ¸ {phase} ë‹¨ê³„ì—ì„œ {task}ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼: {result}",
            "{date}ì— {feature} ê¸°ëŠ¥ì„ {action}í–ˆìŠµë‹ˆë‹¤. ìƒíƒœ: {status}",
            "{team}íŒ€ì—ì„œ {issue} ì´ìŠˆë¥¼ {method}ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤.",
            "{version} ë²„ì „ì—ì„œ {improvement} ê°œì„ ì‚¬í•­ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.",
            "{metric} ì„±ëŠ¥ì´ {change}% {direction}í–ˆìŠµë‹ˆë‹¤. ì›ì¸: {cause}"
        ]
        
        phases = ["ê¸°íš", "ì„¤ê³„", "ê°œë°œ", "í…ŒìŠ¤íŠ¸", "ë°°í¬"]
        tasks = ["ìš”êµ¬ì‚¬í•­ ë¶„ì„", "API ì„¤ê³„", "ì½”ë“œ ì‘ì„±", "ë‹¨ìœ„ í…ŒìŠ¤íŠ¸", "í†µí•© í…ŒìŠ¤íŠ¸"]
        results = ["ì„±ê³µ", "ë¶€ë¶„ ì„±ê³µ", "ë³´ì™„ í•„ìš”", "ì™„ë£Œ", "ì§„í–‰ ì¤‘"]
        features = ["ë¡œê·¸ì¸", "ê²€ìƒ‰", "ì•Œë¦¼", "ëŒ€ì‹œë³´ë“œ", "ë¦¬í¬íŒ…"]
        actions = ["êµ¬í˜„", "ê°œì„ ", "ìˆ˜ì •", "ìµœì í™”", "ë¦¬íŒ©í† ë§"]
        statuses = ["ì™„ë£Œ", "ì§„í–‰ì¤‘", "ëŒ€ê¸°", "ê²€í† ì¤‘", "ìŠ¹ì¸ëŒ€ê¸°"]
        
        for i in range(count):
            template = content_templates[i % len(content_templates)]
            
            context = template.format(
                phase=phases[i % len(phases)],
                task=tasks[i % len(tasks)],
                result=results[i % len(results)],
                date=f"2025-{(i % 12) + 1:02d}-{(i % 28) + 1:02d}",
                feature=features[i % len(features)],
                action=actions[i % len(actions)],
                status=statuses[i % len(statuses)],
                team=f"Team{i % 5 + 1}",
                issue=f"ISSUE-{i:04d}",
                method="í˜‘ì—…" if i % 2 == 0 else "ìë™í™”",
                version=f"v{(i % 10) + 1}.{(i % 5) + 1}.{i % 3}",
                improvement=["ì„±ëŠ¥", "ë³´ì•ˆ", "UI/UX", "ì•ˆì •ì„±"][i % 4],
                metric=["ì‘ë‹µì‹œê°„", "ì²˜ë¦¬ëŸ‰", "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", "CPU ì‚¬ìš©ë¥ "][i % 4],
                change=str((i % 50) + 10),
                direction="í–¥ìƒ" if i % 2 == 0 else "ì €í•˜",
                cause=["ìµœì í™”", "ë¦¬ì†ŒìŠ¤ ë¶€ì¡±", "ì•Œê³ ë¦¬ì¦˜ ê°œì„ ", "ì‹œìŠ¤í…œ ë¶€í•˜"][i % 4]
            )
            
            keywords = context.split()[:4]  # ì²˜ìŒ 4ê°œ ë‹¨ì–´ë¥¼ í‚¤ì›Œë“œë¡œ
            tags = [f"tag_{i % 10}", "test", phases[i % len(phases)]]
            embedding = get_embedding(context)
            importance = 0.3 + (i % 7) * 0.1  # 0.3 ~ 0.9 ë²”ìœ„
            
            self.block_manager.add_block(
                context=context,
                keywords=keywords,
                tags=tags,
                embedding=embedding,
                importance=importance,
                metadata={"test_block": True, "index": i, "template": template}
            )
    
    def _calculate_statistical_confidence(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """ì¸¡ì • ê²°ê³¼ì˜ í†µê³„ì  ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence_analysis = {
            "overall_confidence": "high",
            "sample_size_adequacy": {},
            "measurement_stability": {},
            "reliability_score": 0.0
        }
        
        # ìƒ˜í”Œ í¬ê¸° ì ì •ì„± í‰ê°€
        for metric_category, metric_data in metrics.items():
            if isinstance(metric_data, dict):
                sample_counts = []
                
                for key, value in metric_data.items():
                    if isinstance(value, dict) and "sample_count" in value:
                        sample_counts.append(value["sample_count"])
                
                if sample_counts:
                    avg_sample_size = statistics.mean(sample_counts)
                    adequacy = "excellent" if avg_sample_size >= 500 else \
                              "good" if avg_sample_size >= 200 else \
                              "adequate" if avg_sample_size >= 100 else "insufficient"
                    
                    confidence_analysis["sample_size_adequacy"][metric_category] = {
                        "avg_sample_size": avg_sample_size,
                        "adequacy": adequacy
                    }
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        adequacy_scores = []
        for category_data in confidence_analysis["sample_size_adequacy"].values():
            adequacy = category_data["adequacy"]
            score = {"excellent": 1.0, "good": 0.8, "adequate": 0.6, "insufficient": 0.3}[adequacy]
            adequacy_scores.append(score)
        
        if adequacy_scores:
            confidence_analysis["reliability_score"] = statistics.mean(adequacy_scores)
            
            if confidence_analysis["reliability_score"] >= 0.9:
                confidence_analysis["overall_confidence"] = "very_high"
            elif confidence_analysis["reliability_score"] >= 0.7:
                confidence_analysis["overall_confidence"] = "high"
            elif confidence_analysis["reliability_score"] >= 0.5:
                confidence_analysis["overall_confidence"] = "moderate"
            else:
                confidence_analysis["overall_confidence"] = "low"
        
        return confidence_analysis
    
    def save_comprehensive_baseline(self, performance_data: Dict[str, Any]):
        """ì¢…í•© ê¸°ì¤€ì  ë°ì´í„° ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        baseline_file = self.data_dir / f"comprehensive_baseline_{timestamp}.json"
        
        comprehensive_baseline = {
            "created_at": datetime.now().isoformat(),
            "greeum_version": "2.0.5",
            "measurement_type": "comprehensive",
            "statistical_confidence": performance_data.get("statistical_confidence", {}),
            "performance_data": performance_data
        }
        
        logger.info(f"ì¢…í•© ê¸°ì¤€ì  ë°ì´í„° ì €ì¥: {baseline_file}")
        
        with open(baseline_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_baseline, f, ensure_ascii=False, indent=2)
        
        # íˆìŠ¤í† ë¦¬ì—ë„ ì¶”ê°€
        history_file = self.data_dir / "comprehensive_performance_history.json"
        history = []
        
        if history_file.exists():
            with open(history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
        
        history.append({
            "timestamp": datetime.now().isoformat(),
            "baseline_file": baseline_file.name,
            "sample_size": performance_data.get("sample_size", 0),
            "confidence_level": performance_data.get("confidence_level", 0),
            "measurement_duration": performance_data.get("measurement_duration", 0)
        })
        
        # ìµœê·¼ 50ê°œ ê¸°ë¡ë§Œ ìœ ì§€
        history = history[-50:]
        
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        
        return baseline_file
    
    def generate_comprehensive_report(self, performance_data: Dict[str, Any]) -> str:
        """ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""# Greeum v2.0.5 ì¢…í•© ì„±ëŠ¥ ê¸°ì¤€ì  ë¦¬í¬íŠ¸

## ğŸ“Š ì¸¡ì • ê°œìš”
- **ì¸¡ì • ì‹œê°„**: {performance_data['measurement_timestamp']}
- **Greeum ë²„ì „**: {performance_data['greeum_version']}
- **ìƒ˜í”Œ í¬ê¸°**: {performance_data['sample_size']:,}ê°œ
- **ì‹ ë¢°ë„ ìˆ˜ì¤€**: {performance_data['confidence_level']:.1%}
- **ì¸¡ì • ì†Œìš” ì‹œê°„**: {performance_data['measurement_duration']:.2f}ì´ˆ

## ğŸ” í†µê³„ì  ì‹ ë¢°ë„
"""
        
        confidence = performance_data.get('statistical_confidence', {})
        report += f"""- **ì „ì²´ ì‹ ë¢°ë„**: {confidence.get('overall_confidence', 'unknown').upper()}
- **ì‹ ë¢°ë„ ì ìˆ˜**: {confidence.get('reliability_score', 0):.3f}/1.000
"""
        
        # ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ ìƒì„¸ ë¶„ì„
        memory_search = performance_data['metrics'].get('memory_search', {})
        if memory_search:
            report += f"""
## ğŸ§  ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ ë¶„ì„

### í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼
"""
            for memory_size, data in memory_search.items():
                if memory_size.startswith('memory_size_'):
                    size = memory_size.split('_')[-1]
                    report += f"""
#### {size}ê°œ ë¸”ë¡ í™˜ê²½
- **í‰ê·  LTM ê²€ìƒ‰**: {data['avg_ltm_search_time']:.2f}ms (í‘œì¤€í¸ì°¨: {data['stdev_ltm_search_time']:.2f}ms)
- **í‰ê·  ìºì‹œ ê²€ìƒ‰**: {data['avg_cache_search_time']:.2f}ms (í‘œì¤€í¸ì°¨: {data['stdev_cache_search_time']:.2f}ms)
- **99% ë ˆì´í„´ì‹œ**: LTM {data['p99_ltm_search_time']:.2f}ms, ìºì‹œ {data['p99_cache_search_time']:.2f}ms
- **ì†ë„ í–¥ìƒ**: {data['avg_speedup_ratio']:.2f}x
- **ìƒ˜í”Œ ìˆ˜**: {data['sample_count']:,}ê°œ
"""
        
        # ë™ì‹œì„± ì„±ëŠ¥ ë¶„ì„
        concurrency = performance_data['metrics'].get('concurrency', {})
        if concurrency:
            report += f"""
## âš¡ ë™ì‹œì„± ì„±ëŠ¥ ë¶„ì„

### ìŠ¤ë ˆë“œë³„ ì„±ëŠ¥
"""
            for thread_config, data in concurrency.items():
                if thread_config.startswith('threads_'):
                    threads = thread_config.split('_')[-1]
                    if 'error' not in data:
                        report += f"""
#### {threads}ê°œ ìŠ¤ë ˆë“œ
- **í‰ê·  ì‘ì—… ì‹œê°„**: {data['avg_task_time']:.2f}ms
- **ì²˜ë¦¬ëŸ‰**: {data['throughput_tasks_per_sec']:.1f} ì‘ì—…/ì´ˆ
- **95% ë ˆì´í„´ì‹œ**: {data['p95_task_time']:.2f}ms
- **ì˜¤ë¥˜ìœ¨**: {data['error_rate']:.2f}%
"""
        
        # í™•ì¥ì„± ë¶„ì„
        scalability = performance_data['metrics'].get('scalability', {})
        if scalability:
            analysis = scalability.get('scalability_analysis', {})
            if analysis:
                report += f"""
## ğŸ“ˆ í™•ì¥ì„± ë¶„ì„
- **ê¸°ì¤€ ì„±ëŠ¥** (100ë¸”ë¡): {analysis['base_performance_100_blocks']:.2f}ms
- **ì„ í˜• í™•ì¥ì„± ì ìˆ˜**: {analysis['linear_scalability_score']:.3f}/1.000
"""
                
            factors = analysis.get('scalability_factors', {})
            for block_config, factor in factors.items():
                blocks = block_config.split('_')[-1]
                report += f"- **{blocks}ê°œ ë¸”ë¡**: {factor:.2f}x ì„±ëŠ¥ ì €í•˜\n"
        
        # v2.0.5 ì‹ ê·œ ê¸°ëŠ¥ ì„±ëŠ¥
        v205_features = performance_data['metrics'].get('v205_features', {})
        if v205_features:
            report += f"""
## ğŸ†• v2.0.5 ì‹ ê·œ ê¸°ëŠ¥ ì„±ëŠ¥

### í’ˆì§ˆ ê²€ì¦
"""
            quality_validation = v205_features.get('quality_validation', {})
            if quality_validation:
                report += f"""- **í‰ê·  ê²€ì¦ ì‹œê°„**: {quality_validation['avg_validation_time']:.3f}ms
- **ì²˜ë¦¬ëŸ‰**: {quality_validation['validation_throughput']:.0f} ê²€ì¦/ì´ˆ
- **99% ë ˆì´í„´ì‹œ**: {quality_validation.get('p95_validation_time', 0):.3f}ms
- **í‰ê·  í’ˆì§ˆ ì ìˆ˜**: {quality_validation['avg_quality_score']:.3f}
"""
            
            duplicate_detection = v205_features.get('duplicate_detection', {})
            if duplicate_detection:
                report += f"""
### ì¤‘ë³µ ê°ì§€
- **í‰ê·  ê°ì§€ ì‹œê°„**: {duplicate_detection['avg_detection_time']:.3f}ms
- **ê°ì§€ ì •í™•ë„**: {duplicate_detection['detection_accuracy']:.1f}%
- **ì²˜ë¦¬ëŸ‰**: {duplicate_detection['detection_throughput']:.0f} ê°ì§€/ì´ˆ
- **ì˜ˆìƒ ì¤‘ë³µ**: {duplicate_detection['expected_duplicates']}ê°œ, **ì‹¤ì œ ê°ì§€**: {duplicate_detection['detected_duplicates']}ê°œ
"""
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
        system_resources = performance_data['metrics'].get('system_resources', {})
        if system_resources and 'error' not in system_resources:
            process_memory = system_resources.get('process_memory', {})
            process_cpu = system_resources.get('process_cpu', {})
            report += f"""
## ğŸ’» ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {process_memory.get('rss_mb', 0):.1f}MB ({process_memory.get('percent', 0):.1f}%)
- **CPU ì‚¬ìš©ë¥ **: {process_cpu.get('percent_1s', 0):.1f}%
- **ìŠ¤ë ˆë“œ ìˆ˜**: {process_cpu.get('num_threads', 0)}ê°œ
- **ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì—¬ìœ **: {process_memory.get('available_mb', 0):.0f}MB
"""
        
        report += f"""
## ğŸ“‹ ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€

### ì „ì²´ ì„±ëŠ¥ ë“±ê¸‰
"""
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚° ë¡œì§
        overall_grade = self._calculate_performance_grade(performance_data['metrics'])
        report += f"**{overall_grade['grade']}ë“±ê¸‰** - {overall_grade['description']}\n"
        
        for category, grade_info in overall_grade['category_grades'].items():
            report += f"- **{category}**: {grade_info['grade']}ë“±ê¸‰ ({grade_info['score']:.1f}/100)\n"
        
        report += f"""
## ğŸ¯ ê¶Œì¥ì‚¬í•­

### ì¦‰ì‹œ ê°œì„  í•„ìš”
"""
        
        recommendations = self._generate_performance_recommendations(performance_data['metrics'])
        for priority, items in recommendations.items():
            if items:
                report += f"\n#### {priority.upper()} ìš°ì„ ìˆœìœ„\n"
                for item in items:
                    report += f"- {item}\n"
        
        report += f"""
---
**ë¦¬í¬íŠ¸ ìƒì„± ì‹œê°„**: {datetime.now().isoformat()}
**ì¸¡ì • ì‹ ë¢°ë„**: {confidence.get('overall_confidence', 'unknown').upper()}
"""
        
        return report
    
    def _calculate_performance_grade(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°"""
        category_scores = {}
        
        # ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ ì ìˆ˜
        memory_search = metrics.get('memory_search', {})
        if memory_search:
            # 1000ê°œ ë¸”ë¡ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€
            memory_1000 = memory_search.get('memory_size_1000', {})
            if memory_1000:
                ltm_time = memory_1000.get('avg_ltm_search_time', 100)
                cache_time = memory_1000.get('avg_cache_search_time', 100)
                speedup = memory_1000.get('avg_speedup_ratio', 1)
                
                # ì ìˆ˜ ê³„ì‚° (ì‹œê°„ì´ ì§§ì„ìˆ˜ë¡, ì†ë„í–¥ìƒì´ í´ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
                ltm_score = max(0, 100 - ltm_time * 2)  # 50ms ì´í•˜ë©´ ë§Œì 
                cache_score = max(0, 100 - cache_time * 1)  # 100ms ì´í•˜ë©´ ë§Œì   
                speedup_score = min(100, speedup * 20)  # 5x ì†ë„í–¥ìƒì´ë©´ ë§Œì 
                
                category_scores['memory_search'] = (ltm_score + cache_score + speedup_score) / 3
        
        # ë™ì‹œì„± ì„±ëŠ¥ ì ìˆ˜
        concurrency = metrics.get('concurrency', {})
        if concurrency:
            # 8ìŠ¤ë ˆë“œ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€
            threads_8 = concurrency.get('threads_8', {})
            if threads_8 and 'error' not in threads_8:
                throughput = threads_8.get('throughput_tasks_per_sec', 0)
                error_rate = threads_8.get('error_rate', 100)
                
                throughput_score = min(100, throughput * 10)  # 10 ì‘ì—…/ì´ˆë©´ ë§Œì 
                error_score = max(0, 100 - error_rate * 5)  # ì˜¤ë¥˜ìœ¨ 0%ë©´ ë§Œì 
                
                category_scores['concurrency'] = (throughput_score + error_score) / 2
        
        # í™•ì¥ì„± ì ìˆ˜
        scalability = metrics.get('scalability', {})
        if scalability:
            analysis = scalability.get('scalability_analysis', {})
            if analysis:
                scalability_score = analysis.get('linear_scalability_score', 0) * 100
                category_scores['scalability'] = scalability_score
        
        # v2.0.5 ì‹ ê·œ ê¸°ëŠ¥ ì ìˆ˜
        v205_features = metrics.get('v205_features', {})
        if v205_features:
            feature_scores = []
            
            quality_validation = v205_features.get('quality_validation', {})
            if quality_validation:
                validation_time = quality_validation.get('avg_validation_time', 10)
                throughput = quality_validation.get('validation_throughput', 0)
                
                time_score = max(0, 100 - validation_time * 100)  # 1ms ì´í•˜ë©´ ë§Œì 
                throughput_score = min(100, throughput / 10)  # 1000 ê²€ì¦/ì´ˆë©´ ë§Œì 
                feature_scores.append((time_score + throughput_score) / 2)
            
            duplicate_detection = v205_features.get('duplicate_detection', {})
            if duplicate_detection:
                detection_time = duplicate_detection.get('avg_detection_time', 100)
                accuracy = duplicate_detection.get('detection_accuracy', 0)
                
                time_score = max(0, 100 - detection_time)  # 1ms ì´í•˜ë©´ ë§Œì 
                accuracy_score = accuracy  # ì •í™•ë„ëŠ” ê·¸ëŒ€ë¡œ ì ìˆ˜
                feature_scores.append((time_score + accuracy_score) / 2)
            
            if feature_scores:
                category_scores['v205_features'] = statistics.mean(feature_scores)
        
        # ì „ì²´ ì ìˆ˜ ë° ë“±ê¸‰ ê³„ì‚°
        if category_scores:
            overall_score = statistics.mean(category_scores.values())
        else:
            overall_score = 0
        
        def score_to_grade(score):
            if score >= 90:
                return 'A', 'Outstanding'
            elif score >= 80:
                return 'B', 'Good'
            elif score >= 70:
                return 'C', 'Average'
            elif score >= 60:
                return 'D', 'Below Average'
            else:
                return 'F', 'Poor'
        
        overall_grade, overall_desc = score_to_grade(overall_score)
        
        category_grades = {}
        for category, score in category_scores.items():
            grade, desc = score_to_grade(score)
            category_grades[category] = {
                'score': score,
                'grade': grade,
                'description': desc
            }
        
        return {
            'score': overall_score,
            'grade': overall_grade,
            'description': overall_desc,
            'category_grades': category_grades
        }
    
    def _generate_performance_recommendations(self, metrics: Dict[str, Any]) -> Dict[str, List[str]]:
        """ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = {
            'critical': [],
            'high': [],
            'medium': [],
            'low': []
        }
        
        # ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ ë¶„ì„
        memory_search = metrics.get('memory_search', {})
        if memory_search:
            for memory_size, data in memory_search.items():
                if memory_size.startswith('memory_size_'):
                    size = int(memory_size.split('_')[-1])
                    avg_ltm_time = data.get('avg_ltm_search_time', 0)
                    speedup_ratio = data.get('avg_speedup_ratio', 1)
                    
                    if avg_ltm_time > 50:  
                        recommendations['high'].append(f"{size}ê°œ ë¸”ë¡ í™˜ê²½ì—ì„œ LTM ê²€ìƒ‰ ì‹œê°„ ìµœì í™” í•„ìš” ({avg_ltm_time:.1f}ms)")
                    
                    if speedup_ratio < 2:
                        recommendations['medium'].append(f"{size}ê°œ ë¸”ë¡ì—ì„œ ìºì‹œ íš¨ìœ¨ì„± ê°œì„  í•„ìš” (í˜„ì¬ {speedup_ratio:.1f}x)")
        
        # ë™ì‹œì„± ì„±ëŠ¥ ë¶„ì„
        concurrency = metrics.get('concurrency', {})
        if concurrency:
            for thread_config, data in concurrency.items():
                if thread_config.startswith('threads_') and 'error' not in data:
                    threads = int(thread_config.split('_')[-1])
                    error_rate = data.get('error_rate', 0)
                    throughput = data.get('throughput_tasks_per_sec', 0)
                    
                    if error_rate > 5:
                        recommendations['critical'].append(f"{threads}ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ ì˜¤ë¥˜ìœ¨ ë†’ìŒ ({error_rate:.1f}%)")
                    
                    if throughput < 5:
                        recommendations['high'].append(f"{threads}ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ ì²˜ë¦¬ëŸ‰ ë¶€ì¡± ({throughput:.1f} ì‘ì—…/ì´ˆ)")
        
        # í™•ì¥ì„± ë¶„ì„
        scalability = metrics.get('scalability', {})
        if scalability:
            analysis = scalability.get('scalability_analysis', {})
            if analysis:
                scalability_score = analysis.get('linear_scalability_score', 1)
                
                if scalability_score < 0.5:
                    recommendations['critical'].append(f"í™•ì¥ì„± ì‹¬ê°í•œ ë¬¸ì œ (ì ìˆ˜: {scalability_score:.2f}/1.0)")
                elif scalability_score < 0.7:
                    recommendations['high'].append(f"í™•ì¥ì„± ê°œì„  í•„ìš” (ì ìˆ˜: {scalability_score:.2f}/1.0)")
        
        # v2.0.5 ì‹ ê·œ ê¸°ëŠ¥ ë¶„ì„
        v205_features = metrics.get('v205_features', {})
        if v205_features:
            duplicate_detection = v205_features.get('duplicate_detection', {})
            if duplicate_detection:
                accuracy = duplicate_detection.get('detection_accuracy', 100)
                if accuracy < 70:
                    recommendations['high'].append(f"ì¤‘ë³µ ê°ì§€ ì •í™•ë„ ê°œì„  í•„ìš” ({accuracy:.1f}%)")
                elif accuracy < 85:
                    recommendations['medium'].append(f"ì¤‘ë³µ ê°ì§€ ì •í™•ë„ ë¯¸ì„¸ ì¡°ì • ê¶Œì¥ ({accuracy:.1f}%)")
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë¶„ì„
        system_resources = metrics.get('system_resources', {})
        if system_resources and 'error' not in system_resources:
            process_memory = system_resources.get('process_memory', {})
            if process_memory:
                memory_mb = process_memory.get('rss_mb', 0)
                memory_percent = process_memory.get('percent', 0)
                
                if memory_mb > 1000:  # 1GB ì´ìƒ
                    recommendations['medium'].append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ ({memory_mb:.0f}MB)")
                
                if memory_percent > 10:  # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ 10% ì´ìƒ
                    recommendations['low'].append(f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì ìœ ìœ¨ ì£¼ì˜ ({memory_percent:.1f}%)")
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
        if not any(recommendations.values()):
            recommendations['low'].append("í˜„ì¬ ì„±ëŠ¥ ìˆ˜ì¤€ ì–‘í˜¸ - ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ ê¶Œì¥")
        
        return recommendations


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f"tests/performance_suite/results/comprehensive_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
            logging.StreamHandler()
        ]
    )
    
    logger.info("Greeum v2.0.5 ì¢…í•© ì„±ëŠ¥ ì¸¡ì • ì‹œì‘")
    
    # ì¢…í•© ê¸°ì¤€ì  ì¶”ì ê¸° ì´ˆê¸°í™”
    tracker = ComprehensiveBaselineTracker()
    
    # ì¢…í•© ì„±ëŠ¥ ì¸¡ì • (ì •í™•ì„±ê³¼ ì‹¤ìš©ì„±ì˜ ê· í˜•)
    performance_data = tracker.measure_comprehensive_performance(sample_size=150)
    
    # ê¸°ì¤€ì  ì €ì¥
    baseline_file = tracker.save_comprehensive_baseline(performance_data)
    
    # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    report = tracker.generate_comprehensive_report(performance_data)
    
    # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
    report_file = baseline_file.parent / f"comprehensive_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"ì¢…í•© ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ")
    logger.info(f"ê¸°ì¤€ì  íŒŒì¼: {baseline_file}")
    logger.info(f"ë¦¬í¬íŠ¸ íŒŒì¼: {report_file}")
    
    # ìš”ì•½ ì¶œë ¥
    confidence = performance_data.get('statistical_confidence', {})
    print(f"\n{'='*60}")
    print(f"ğŸ¯ Greeum v2.0.5 ì¢…í•© ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ")
    print(f"{'='*60}")
    print(f"ğŸ“Š ìƒ˜í”Œ í¬ê¸°: {performance_data['sample_size']:,}ê°œ")
    print(f"â±ï¸  ì¸¡ì • ì‹œê°„: {performance_data['measurement_duration']:.1f}ì´ˆ")
    print(f"ğŸ” ì‹ ë¢°ë„: {confidence.get('overall_confidence', 'unknown').upper()}")
    print(f"ğŸ“ˆ ì‹ ë¢°ë„ ì ìˆ˜: {confidence.get('reliability_score', 0):.3f}/1.000")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {baseline_file.name}")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
"""
Performance Analysis and Optimization
ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™” ë°©ì•ˆ
"""

import json
import time
import statistics
from typing import Dict, List, Any

def analyze_benchmark_results(results_file: str) -> Dict[str, Any]:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì‹¬ì¸µ ë¶„ì„"""
    
    with open(results_file, 'r', encoding='utf-8') as f:
        all_results = json.load(f)
    
    analysis = {
        'performance_issues': [],
        'optimization_opportunities': [],
        'scaling_analysis': {},
        'recommendations': []
    }
    
    # ì„±ëŠ¥ ì´ìŠˆ ì‹ë³„
    for result in all_results:
        workload_size = result['workload_size']
        improvements = result['improvements']
        
        # ê²€ìƒ‰ ì‹œê°„ì´ ì•…í™”ëœ ê²½ìš°
        if improvements['search_time_improvement'] < 0:
            analysis['performance_issues'].append({
                'type': 'search_time_degradation',
                'workload_size': workload_size,
                'degradation_pct': abs(improvements['search_time_improvement']),
                'cause': 'initialization_overhead'
            })
        
        # Local hit rateê°€ 0%ì¸ ê²½ìš°
        branch_results = result['branch_results']
        if branch_results.get('local_hit_rate', 0) == 0:
            analysis['performance_issues'].append({
                'type': 'zero_local_hit_rate',
                'workload_size': workload_size,
                'fallback_rate': branch_results.get('fallback_rate', 0),
                'cause': 'insufficient_branch_locality'
            })
    
    # ìµœì í™” ê¸°íšŒ
    hop_reductions = [r['improvements']['hops_reduction'] for r in all_results]
    avg_hop_reduction = statistics.mean(hop_reductions)
    
    if avg_hop_reduction > 90:
        analysis['optimization_opportunities'].append({
            'type': 'excellent_hop_reduction',
            'avg_reduction': avg_hop_reduction,
            'impact': 'navigation_efficiency_high'
        })
    
    # ìŠ¤ì¼€ì¼ë§ ë¶„ì„
    workload_sizes = [r['workload_size'] for r in all_results]
    search_times_legacy = [r['legacy_results']['avg_search_time'] for r in all_results]
    search_times_branch = [r['branch_results']['avg_search_time'] for r in all_results]
    
    analysis['scaling_analysis'] = {
        'legacy_scaling': {
            'sizes': workload_sizes,
            'search_times': search_times_legacy,
            'scaling_factor': search_times_legacy[-1] / search_times_legacy[0] if search_times_legacy else 1
        },
        'branch_scaling': {
            'sizes': workload_sizes,
            'search_times': search_times_branch,
            'scaling_factor': search_times_branch[-1] / search_times_branch[0] if search_times_branch else 1
        }
    }
    
    # ê¶Œì¥ì‚¬í•­
    analysis['recommendations'] = [
        {
            'priority': 'HIGH',
            'issue': 'Local Hit Rate Optimization',
            'solution': 'Improve DFS local search effectiveness by adjusting depth limits and similarity thresholds',
            'expected_impact': '20-40% improvement in search performance'
        },
        {
            'priority': 'MEDIUM',
            'issue': 'Initialization Overhead',
            'solution': 'Optimize BranchManager initialization and reduce per-search overhead',
            'expected_impact': '15-30% reduction in search time'
        },
        {
            'priority': 'LOW',
            'issue': 'Global Index Efficiency',
            'solution': 'Implement FAISS-based vector search for better fallback performance',
            'expected_impact': '10-20% improvement in fallback scenarios'
        }
    ]
    
    return analysis


def generate_optimized_branch_config() -> Dict[str, Any]:
    """ìµœì í™”ëœ ë¸Œëœì¹˜ ì„¤ì • ìƒì„±"""
    
    return {
        'dfs_config': {
            'depth_default': 4,  # 3ì—ì„œ 4ë¡œ ì¦ê°€
            'k_default': 12,     # 8ì—ì„œ 12ë¡œ ì¦ê°€
            'similarity_threshold': 0.05,  # ë” ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì€ í›„ë³´
        },
        'scoring_weights': {
            'content_similarity': 0.4,
            'recency_boost': 0.3,      # ìµœê·¼ì„± ê°€ì¤‘ì¹˜ ì¦ê°€
            'branch_locality': 0.2,    # ë¸Œëœì¹˜ ë‚´ ìš°ì„ ìˆœìœ„
            'keyword_bonus': 0.1
        },
        'auto_merge': {
            'ema_alpha': 0.8,          # ë” ë¹ ë¥¸ í•™ìŠµ
            'theta_high': 0.65,        # ë” ë‚®ì€ ë¨¸ì§€ ì„ê³„ê°’
            'evaluation_window': 3,    # ë” ì§§ì€ í‰ê°€ ìœˆë„ìš°
            'min_confidence': 2        # ë” ë¹ ë¥¸ ë¨¸ì§€
        },
        'global_index': {
            'vector_dimensions': 128,
            'hybrid_search_weight': 0.6,  # í‚¤ì›Œë“œ ê²€ìƒ‰ ë¹„ì¤‘ ì¦ê°€
            'entry_point_diversity': True
        }
    }


def simulate_optimized_performance(baseline_results: List[Dict], 
                                 optimized_config: Dict) -> Dict[str, Any]:
    """ìµœì í™”ëœ ì„¤ì •ì˜ ì˜ˆìƒ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜"""
    
    simulated_improvements = {}
    
    for result in baseline_results:
        workload_size = result['workload_size']
        branch_result = result['branch_results']
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ê°œì„ 
        optimized_search_time = branch_result['avg_search_time'] * 0.7  # 30% ê°œì„  ê°€ì •
        optimized_local_hit_rate = 0.4  # 40% ë¡œì»¬ íˆíŠ¸ìœ¨ ê°€ì •
        optimized_fallback_rate = 0.6   # 60% í´ë°±ìœ¨ ê°€ì •
        optimized_hops = branch_result['avg_hops'] * 0.8  # 20% ì¶”ê°€ í™‰ ê°ì†Œ
        
        simulated_improvements[workload_size] = {
            'current': {
                'search_time': branch_result['avg_search_time'],
                'local_hit_rate': branch_result.get('local_hit_rate', 0),
                'avg_hops': branch_result['avg_hops']
            },
            'optimized': {
                'search_time': optimized_search_time,
                'local_hit_rate': optimized_local_hit_rate,
                'avg_hops': optimized_hops
            },
            'improvement': {
                'search_time_pct': ((branch_result['avg_search_time'] - optimized_search_time) 
                                   / branch_result['avg_search_time']) * 100,
                'local_hit_improvement': optimized_local_hit_rate - branch_result.get('local_hit_rate', 0),
                'hops_improvement_pct': ((branch_result['avg_hops'] - optimized_hops) 
                                       / branch_result['avg_hops']) * 100
            }
        }
    
    return simulated_improvements


def create_performance_report(results_file: str) -> str:
    """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
    
    # ë¶„ì„ ì‹¤í–‰
    analysis = analyze_benchmark_results(results_file)
    
    with open(results_file, 'r', encoding='utf-8') as f:
        all_results = json.load(f)
    
    optimized_config = generate_optimized_branch_config()
    simulated_improvements = simulate_optimized_performance(all_results, optimized_config)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report = f"""
# Greeum Branch-based Memory System Performance Analysis Report

## Executive Summary

The comparative performance test between legacy graph-based and new branch-based memory systems reveals significant **navigation efficiency improvements** with **95.7% reduction in average hops**, while highlighting optimization opportunities for search performance.

## Key Findings

### âœ… Major Improvements
- **Navigation Efficiency**: 95.7% reduction in hops across all workload sizes
- **Structural Benefits**: Clear branch-based organization improves logical navigation
- **Scalability**: Consistent hop reduction regardless of dataset size

### âš ï¸ Performance Issues Identified
"""
    
    for issue in analysis['performance_issues']:
        if issue['type'] == 'search_time_degradation':
            report += f"- **Search Time Degradation**: {issue['degradation_pct']:.1f}% slower at {issue['workload_size']} blocks\n"
        elif issue['type'] == 'zero_local_hit_rate':
            report += f"- **Zero Local Hit Rate**: {issue['fallback_rate']:.1%} fallback rate at {issue['workload_size']} blocks\n"
    
    report += f"""
## Performance Metrics Breakdown

| Workload Size | Legacy Hops | Branch Hops | Hop Reduction | Search Time Impact |
|---------------|-------------|-------------|---------------|-------------------|
"""
    
    for result in all_results:
        legacy = result['legacy_results']
        branch = result['branch_results']
        improvements = result['improvements']
        
        report += f"| {result['workload_size']:>4} blocks | {legacy['avg_hops']:>8.1f} | {branch['avg_hops']:>8.1f} | {improvements['hops_reduction']:>10.1f}% | {improvements['search_time_improvement']:>+10.1f}% |\n"
    
    report += f"""
## Root Cause Analysis

### 1. Search Time Degradation
- **Cause**: Initialization overhead and complex DFS traversal
- **Impact**: 254-1358% slower search times
- **Severity**: HIGH - needs immediate optimization

### 2. Zero Local Hit Rate  
- **Cause**: DFS parameters too restrictive for realistic workloads
- **Impact**: 100% fallback to global search
- **Severity**: HIGH - defeats branch locality purpose

### 3. Excellent Navigation Efficiency
- **Achievement**: 95.7% hop reduction consistently
- **Benefit**: Dramatically improved memory traversal
- **Status**: WORKING AS INTENDED

## Optimization Recommendations

### ğŸ”´ Priority 1: Local Search Optimization
```python
# Current Config (Too Restrictive)
DEPTH_DEFAULT = 3
K_DEFAULT = 8
similarity_threshold = 0.1

# Optimized Config
DEPTH_DEFAULT = 4          # +33% deeper search
K_DEFAULT = 12             # +50% more candidates  
similarity_threshold = 0.05 # -50% more inclusive
```
**Expected Impact**: 30-50% improvement in local hit rate

### ğŸŸ¡ Priority 2: Performance Tuning
```python
# Reduce per-search overhead
- Cache similarity calculations
- Optimize embedding operations  
- Lazy-load global index
```
**Expected Impact**: 20-30% faster search times

### ğŸŸ¢ Priority 3: Advanced Features
```python
# Enhanced global index
- FAISS vector search
- Smarter entry point selection
- Adaptive depth adjustment
```
**Expected Impact**: 10-15% overall improvement

## Simulated Optimized Performance

"""
    
    for workload_size, sim in simulated_improvements.items():
        current = sim['current']
        optimized = sim['optimized']
        improvement = sim['improvement']
        
        report += f"""
### {workload_size} Blocks Workload
- **Search Time**: {current['search_time']:.2f}ms â†’ {optimized['search_time']:.2f}ms ({improvement['search_time_pct']:+.1f}%)
- **Local Hit Rate**: {current['local_hit_rate']:.1%} â†’ {optimized['local_hit_rate']:.1%} (+{improvement['local_hit_improvement']:.1%})
- **Avg Hops**: {current['avg_hops']:.1f} â†’ {optimized['avg_hops']:.1f} ({improvement['hops_improvement_pct']:+.1f}%)
"""

    report += f"""
## Implementation Roadmap

### Phase 1: Critical Fixes (Week 1-2)
1. Adjust DFS depth and candidate limits
2. Lower similarity thresholds  
3. Implement search result caching

### Phase 2: Performance Optimization (Week 3-4)
1. Optimize BranchManager initialization
2. Reduce per-search overhead
3. Implement lazy loading

### Phase 3: Advanced Features (Week 5-6)
1. FAISS-based vector search
2. Adaptive parameter tuning
3. Enhanced merge algorithms

## Conclusion

The branch-based memory system demonstrates **exceptional promise** with its 95.7% hop reduction, proving the core architectural concept. However, immediate optimization is needed to address search performance regression.

**Bottom Line**: With targeted optimizations, the branch-based system can achieve both superior navigation efficiency AND competitive search performance, delivering the promised "ì´ì–´ ì“°ê¸° â†’ ì´ì–´ ì°¾ê¸°" user experience.

---
*Report generated: {time.strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    return report


if __name__ == "__main__":
    # ìµœì‹  ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
    import glob
    import os
    
    benchmark_files = glob.glob("benchmark_results_*.json")
    if benchmark_files:
        latest_file = max(benchmark_files, key=os.path.getctime)
        print(f"ğŸ“Š Analyzing: {latest_file}")
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = create_performance_report(latest_file)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_file = f"performance_analysis_report_{int(time.time())}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“‹ Analysis report saved: {report_file}")
        print("\n" + "="*60)
        print("ğŸ“ˆ KEY INSIGHTS")
        print("="*60)
        print("ğŸš€ Navigation Efficiency: 95.7% hop reduction")  
        print("âš ï¸  Search Performance: Needs optimization")
        print("ğŸ¯ Local Hit Rate: 0% â†’ Target 40%+")
        print("ğŸ”§ Optimization Potential: 30-50% improvement")
        print("="*60)
    else:
        print("âŒ No benchmark results found. Run benchmark first.")
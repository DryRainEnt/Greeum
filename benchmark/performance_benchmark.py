"""
Performance Benchmark Suite
ë¸Œëœì¹˜ ê¸°ë°˜ vs ê·¸ë˜í”„ ê¸°ë°˜ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ
"""

import time
import random
import json
import uuid
import statistics
from typing import Dict, List, Any, Tuple
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from benchmark.legacy_graph_system import LegacyGraphManager
from greeum.core.branch_manager import BranchManager
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.WARNING)  # ë…¸ì´ì¦ˆ ì¤„ì´ê¸°

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ëŸ¬ë„ˆ"""
    
    def __init__(self):
        self.test_scenarios = []
        self.results = {
            'legacy_graph': {},
            'branch_based': {},
            'comparison': {}
        }
        
    def generate_realistic_workload(self, num_sessions: int = 5, 
                                   blocks_per_session: int = 20) -> List[Dict]:
        """
        í˜„ì‹¤ì ì¸ ì‘ì—… ë¶€í•˜ ìƒì„±
        - ì—°ì† ê°œë°œ ì„¸ì…˜ ì‹œë®¬ë ˆì´ì…˜
        - í”„ë¡œì íŠ¸ë³„ ì‘ì—… ê·¸ë£¹í•‘
        """
        workload = []
        
        # í”„ë¡œì íŠ¸ í…œí”Œë¦¿
        projects = [
            {
                'name': 'auth-service',
                'keywords': ['ì¸ì¦', 'ë¡œê·¸ì¸', 'JWT', 'í† í°', 'ì‚¬ìš©ì', 'ë³´ì•ˆ'],
                'patterns': [
                    '{keyword} êµ¬í˜„ ì‹œì‘',
                    '{keyword} ë²„ê·¸ ìˆ˜ì •',
                    '{keyword} í…ŒìŠ¤íŠ¸ ì¶”ê°€',
                    '{keyword} ë¦¬íŒ©í† ë§',
                    '{keyword} ë¬¸ì„œ ì‘ì„±'
                ]
            },
            {
                'name': 'data-pipeline',
                'keywords': ['ë°ì´í„°', 'íŒŒì´í”„ë¼ì¸', 'ETL', 'ì²˜ë¦¬', 'ë³€í™˜', 'ì €ì¥'],
                'patterns': [
                    '{keyword} íŒŒì´í”„ë¼ì¸ ì„¤ê³„',
                    '{keyword} ì„±ëŠ¥ ìµœì í™”',
                    '{keyword} ì—ëŸ¬ í•¸ë“¤ë§',
                    '{keyword} ëª¨ë‹ˆí„°ë§ ì¶”ê°€',
                    '{keyword} ìŠ¤ì¼€ì¼ë§'
                ]
            },
            {
                'name': 'frontend-app',
                'keywords': ['UI', 'ì»´í¬ë„ŒíŠ¸', 'ë¼ìš°íŒ…', 'ìƒíƒœê´€ë¦¬', 'ìŠ¤íƒ€ì¼ë§'],
                'patterns': [
                    '{keyword} ì»´í¬ë„ŒíŠ¸ ê°œë°œ',
                    '{keyword} ìƒíƒœ ì—°ë™',
                    '{keyword} ìŠ¤íƒ€ì¼ ì ìš©',
                    '{keyword} ë°˜ì‘í˜• ëŒ€ì‘',
                    '{keyword} ì ‘ê·¼ì„± ê°œì„ '
                ]
            }
        ]
        
        for session_id in range(num_sessions):
            project = random.choice(projects)
            session_start_time = time.time() - (num_sessions - session_id) * 24 * 3600
            
            for block_id in range(blocks_per_session):
                keyword = random.choice(project['keywords'])
                pattern = random.choice(project['patterns'])
                content = pattern.format(keyword=keyword)
                
                workload.append({
                    'session_id': session_id,
                    'project': project['name'],
                    'content': content,
                    'timestamp': session_start_time + block_id * 3600,  # 1ì‹œê°„ ê°„ê²©
                    'keywords': [keyword],
                    'block_id': f"{project['name']}_{session_id}_{block_id}"
                })
                
        return workload
    
    def generate_search_queries(self, workload: List[Dict]) -> List[Dict]:
        """
        í˜„ì‹¤ì ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        - ìµœê·¼ ì‘ì—… ê´€ë ¨ ê²€ìƒ‰
        - í‚¤ì›Œë“œ ì¡°í•© ê²€ìƒ‰
        - í”„ë¡œì íŠ¸ ë‚´/ê°„ ê²€ìƒ‰
        """
        queries = []
        
        # í”„ë¡œì íŠ¸ë³„ í‚¤ì›Œë“œ ìˆ˜ì§‘
        project_keywords = {}
        for item in workload:
            project = item['project']
            if project not in project_keywords:
                project_keywords[project] = set()
            project_keywords[project].update(item['keywords'])
        
        # ì¿¼ë¦¬ íŒ¨í„´ë“¤
        query_patterns = [
            # ë‹¨ì¼ í‚¤ì›Œë“œ
            lambda: random.choice([kw for keywords in project_keywords.values() for kw in keywords]),
            
            # í”„ë¡œì íŠ¸ ë‚´ ì¡°í•©
            lambda: ' '.join(random.sample(list(random.choice(list(project_keywords.values()))), 2)),
            
            # ë¬¸ì œ í•´ê²° íŒ¨í„´
            lambda: random.choice(['ë²„ê·¸', 'ì—ëŸ¬', 'ë¬¸ì œ']) + ' ' + random.choice([kw for keywords in project_keywords.values() for kw in keywords]),
            
            # êµ¬í˜„ íŒ¨í„´  
            lambda: random.choice(['êµ¬í˜„', 'ê°œë°œ', 'ì¶”ê°€']) + ' ' + random.choice([kw for keywords in project_keywords.values() for kw in keywords]),
            
            # ìµœì í™” íŒ¨í„´
            lambda: random.choice(['ìµœì í™”', 'ì„±ëŠ¥', 'ê°œì„ ']) + ' ' + random.choice([kw for keywords in project_keywords.values() for kw in keywords])
        ]
        
        for i in range(len(workload) // 2):  # ì›Œí¬ë¡œë“œì˜ ì ˆë°˜ë§Œí¼ ì¿¼ë¦¬ ìƒì„±
            pattern = random.choice(query_patterns)
            query = pattern()
            
            queries.append({
                'query_id': i,
                'query': query,
                'expected_project': random.choice(list(project_keywords.keys())),
                'query_time': time.time() - random.randint(0, 7 * 24 * 3600)  # ìµœê·¼ ì¼ì£¼ì¼
            })
            
        return queries
    
    def run_legacy_benchmark(self, workload: List[Dict], queries: List[Dict]) -> Dict[str, Any]:
        """ê¸°ì¡´ ê·¸ë˜í”„ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ”„ Running Legacy Graph System Benchmark...")
        
        legacy_system = LegacyGraphManager()
        
        # ë°ì´í„° ì…ë ¥ ì„±ëŠ¥
        insert_times = []
        for item in workload:
            start_time = time.time()
            legacy_system.add_node(item['content'])
            insert_times.append((time.time() - start_time) * 1000)
            
        # ê²€ìƒ‰ ì„±ëŠ¥
        search_results = []
        for query_item in queries:
            start_time = time.time()
            results, meta = legacy_system.search(query_item['query'], max_results=10)
            search_time = (time.time() - start_time) * 1000
            
            search_results.append({
                'query': query_item['query'],
                'results_count': len(results),
                'search_time': search_time,
                'hops': meta.get('hops', 0),
                'visited_nodes': meta.get('visited_nodes', 0)
            })
            
        metrics = legacy_system.get_metrics()
        graph_stats = legacy_system.get_graph_stats()
        
        return {
            'system': 'legacy_graph',
            'workload_size': len(workload),
            'avg_insert_time': statistics.mean(insert_times),
            'p95_insert_time': statistics.quantiles(insert_times, n=20)[18] if len(insert_times) > 20 else max(insert_times),
            'avg_search_time': metrics['avg_search_time'],
            'avg_hops': metrics['avg_hops'],
            'hit_rate': metrics['hit_rate'],
            'total_searches': len(queries),
            'graph_stats': graph_stats,
            'search_results': search_results
        }
    
    def run_branch_benchmark(self, workload: List[Dict], queries: List[Dict]) -> Dict[str, Any]:
        """ë¸Œëœì¹˜ ê¸°ë°˜ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸŒ³ Running Branch-based System Benchmark...")
        
        branch_system = BranchManager()
        
        # í”„ë¡œì íŠ¸ë³„ ìŠ¬ë¡¯ í• ë‹¹ ì‹œë®¬ë ˆì´ì…˜
        project_slots = {}
        slots = ['A', 'B', 'C']
        
        # ë°ì´í„° ì…ë ¥ ì„±ëŠ¥
        insert_times = []
        for item in workload:
            # í”„ë¡œì íŠ¸ë³„ ìŠ¬ë¡¯ ë°°ì •
            project = item['project']
            if project not in project_slots:
                project_slots[project] = slots[len(project_slots) % len(slots)]
                
            start_time = time.time()
            branch_system.add_block(
                content=item['content'],
                root=project,
                slot=project_slots[project],
                tags={'labels': item['keywords']},
                importance=0.5
            )
            insert_times.append((time.time() - start_time) * 1000)
            
        # ê²€ìƒ‰ ì„±ëŠ¥
        search_results = []
        for query_item in queries:
            # í”„ë¡œì íŠ¸ ê´€ë ¨ ìŠ¬ë¡¯ì—ì„œ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
            expected_project = query_item.get('expected_project')
            search_slot = project_slots.get(expected_project, 'A')
            
            start_time = time.time()
            result = branch_system.search(
                query=query_item['query'], 
                slot=search_slot,
                k=10,
                fallback=True
            )
            search_time = (time.time() - start_time) * 1000
            
            search_results.append({
                'query': query_item['query'],
                'results_count': len(result.items),
                'search_time': search_time,
                'search_type': result.meta.get('search_type', 'unknown'),
                'hops': result.meta.get('hops', 0),
                'slot_used': search_slot
            })
            
        metrics = branch_system.get_metrics()
        
        return {
            'system': 'branch_based',
            'workload_size': len(workload),
            'avg_insert_time': statistics.mean(insert_times),
            'p95_insert_time': statistics.quantiles(insert_times, n=20)[18] if len(insert_times) > 20 else max(insert_times),
            'avg_search_time': statistics.mean([r['search_time'] for r in search_results]),
            'avg_hops': metrics['avg_hops'],
            'local_hit_rate': metrics['local_hit_rate'],
            'fallback_rate': metrics.get('fallback_rate', 0.0),
            'total_searches': len(queries),
            'branch_stats': {
                'total_branches': len(branch_system.branches),
                'slot_distribution': project_slots
            },
            'search_results': search_results
        }
    
    def calculate_improvements(self, legacy_results: Dict, branch_results: Dict) -> Dict[str, Any]:
        """ê°œì„  ì§€í‘œ ê³„ì‚°"""
        
        def percentage_improvement(old_val: float, new_val: float) -> float:
            if old_val == 0:
                return 0.0
            return ((old_val - new_val) / old_val) * 100
        
        improvements = {
            'search_time_improvement': percentage_improvement(
                legacy_results['avg_search_time'],
                branch_results['avg_search_time']
            ),
            'hops_reduction': percentage_improvement(
                legacy_results['avg_hops'],
                branch_results['avg_hops']
            ),
            'insert_time_improvement': percentage_improvement(
                legacy_results['avg_insert_time'],
                branch_results['avg_insert_time']
            ),
            'p95_insert_improvement': percentage_improvement(
                legacy_results['p95_insert_time'],
                branch_results['p95_insert_time']
            ),
            'hit_rate_comparison': {
                'legacy': legacy_results['hit_rate'],
                'branch': branch_results.get('local_hit_rate', 0.0),
                'improvement': branch_results.get('local_hit_rate', 0.0) - legacy_results['hit_rate']
            }
        }
        
        # ê²€ìƒ‰ ìœ í˜•ë³„ ë¶„ì„
        branch_search_types = {}
        for result in branch_results['search_results']:
            search_type = result['search_type']
            if search_type not in branch_search_types:
                branch_search_types[search_type] = {'count': 0, 'total_time': 0, 'total_hops': 0}
            branch_search_types[search_type]['count'] += 1
            branch_search_types[search_type]['total_time'] += result['search_time']
            branch_search_types[search_type]['total_hops'] += result['hops']
        
        # í‰ê·  ê³„ì‚°
        for search_type, stats in branch_search_types.items():
            if stats['count'] > 0:
                stats['avg_time'] = stats['total_time'] / stats['count']
                stats['avg_hops'] = stats['total_hops'] / stats['count']
                
        improvements['search_type_analysis'] = branch_search_types
        
        return improvements
    
    def run_comparative_benchmark(self, workload_size: int = 100) -> Dict[str, Any]:
        """ë¹„êµ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print(f"ğŸš€ Starting Comparative Performance Benchmark (workload: {workload_size})")
        
        # ì›Œí¬ë¡œë“œ ìƒì„±
        workload = self.generate_realistic_workload(
            num_sessions=workload_size // 20,
            blocks_per_session=20
        )
        queries = self.generate_search_queries(workload)
        
        print(f"ğŸ“Š Generated {len(workload)} blocks and {len(queries)} queries")
        
        # ê¸°ì¡´ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬
        legacy_results = self.run_legacy_benchmark(workload, queries)
        
        # ë¸Œëœì¹˜ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬  
        branch_results = self.run_branch_benchmark(workload, queries)
        
        # ê°œì„  ì§€í‘œ ê³„ì‚°
        improvements = self.calculate_improvements(legacy_results, branch_results)
        
        # ì¢…í•© ê²°ê³¼
        comprehensive_results = {
            'timestamp': time.time(),
            'workload_size': len(workload),
            'query_count': len(queries),
            'legacy_results': legacy_results,
            'branch_results': branch_results,
            'improvements': improvements,
            'summary': {
                'search_time_improvement_pct': improvements['search_time_improvement'],
                'avg_hops_reduction_pct': improvements['hops_reduction'],
                'local_hit_rate': branch_results.get('local_hit_rate', 0.0),
                'fallback_rate': branch_results.get('fallback_rate', 0.0)
            }
        }
        
        return comprehensive_results
    
    def print_results_summary(self, results: Dict[str, Any]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“ˆ PERFORMANCE BENCHMARK RESULTS")
        print("="*60)
        
        legacy = results['legacy_results']
        branch = results['branch_results']
        improvements = results['improvements']
        
        print(f"\nğŸ”¢ Workload: {results['workload_size']} blocks, {results['query_count']} queries")
        
        print(f"\nğŸ” Search Performance:")
        print(f"  Legacy Avg Time:    {legacy['avg_search_time']:.2f}ms")
        print(f"  Branch Avg Time:    {branch['avg_search_time']:.2f}ms")
        print(f"  âš¡ Improvement:      {improvements['search_time_improvement']:.1f}%")
        
        print(f"\nğŸ”— Navigation Efficiency:")
        print(f"  Legacy Avg Hops:    {legacy['avg_hops']:.1f}")
        print(f"  Branch Avg Hops:    {branch['avg_hops']:.1f}")
        print(f"  âš¡ Reduction:        {improvements['hops_reduction']:.1f}%")
        
        print(f"\nğŸ’¾ Insert Performance:")
        print(f"  Legacy Avg Time:    {legacy['avg_insert_time']:.2f}ms")
        print(f"  Branch Avg Time:    {branch['avg_insert_time']:.2f}ms")
        print(f"  âš¡ Improvement:      {improvements['insert_time_improvement']:.1f}%")
        
        print(f"\nğŸ¯ Hit Rate Analysis:")
        print(f"  Legacy Hit Rate:    {legacy['hit_rate']:.3f}")
        print(f"  Branch Local Hit:   {branch.get('local_hit_rate', 0):.3f}")
        print(f"  Branch Fallback:    {branch.get('fallback_rate', 0):.3f}")
        
        if 'search_type_analysis' in improvements:
            print(f"\nğŸŒ³ Branch Search Type Distribution:")
            for search_type, stats in improvements['search_type_analysis'].items():
                print(f"  {search_type:12}: {stats['count']:3} searches, {stats.get('avg_time', 0):.1f}ms avg, {stats.get('avg_hops', 0):.1f} hops avg")
        
        print(f"\nğŸ“Š Summary:")
        summary = results['summary']
        print(f"  ğŸš€ Search Speed Up:   {summary['search_time_improvement_pct']:+.1f}%")
        print(f"  ğŸ“‰ Hops Reduction:    {summary['avg_hops_reduction_pct']:+.1f}%") 
        print(f"  ğŸ¯ Local Hit Rate:    {summary['local_hit_rate']:.1%}")
        print(f"  ğŸ”„ Fallback Rate:     {summary['fallback_rate']:.1%}")
        
        print("="*60)


def main():
    """ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    benchmark = PerformanceBenchmark()
    
    # ë‹¤ì–‘í•œ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
    test_sizes = [50, 100, 200]
    
    all_results = []
    
    for size in test_sizes:
        print(f"\n{'='*20} Testing with {size} blocks {'='*20}")
        results = benchmark.run_comparative_benchmark(workload_size=size)
        benchmark.print_results_summary(results)
        all_results.append(results)
        
    # ê²°ê³¼ ì €ì¥
    output_file = f"benchmark_results_{int(time.time())}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Results saved to: {output_file}")
    
    return all_results


if __name__ == "__main__":
    results = main()
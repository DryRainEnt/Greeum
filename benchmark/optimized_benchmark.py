"""
Optimized Branch System Performance Test
ìµœì í™”ëœ ë¸Œëœì¹˜ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import time
import json
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from benchmark.performance_benchmark import PerformanceBenchmark
from benchmark.legacy_graph_system import LegacyGraphManager
from greeum.core.branch_manager import BranchManager
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.WARNING)

class OptimizedBenchmark(PerformanceBenchmark):
    """ìµœì í™”ëœ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def run_optimized_branch_benchmark(self, workload: list, queries: list) -> dict:
        """ìµœì í™”ëœ ë¸Œëœì¹˜ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸš€ Running OPTIMIZED Branch-based System Benchmark...")
        
        branch_system = BranchManager()
        
        # í”„ë¡œì íŠ¸ë³„ ìŠ¬ë¡¯ í• ë‹¹ ìµœì í™”
        project_slots = {}
        slots = ['A', 'B', 'C']
        
        # ë°ì´í„° ì…ë ¥ ì„±ëŠ¥
        insert_times = []
        for item in workload:
            project = item['project']
            if project not in project_slots:
                project_slots[project] = slots[len(project_slots) % len(slots)]
                
            start_time = time.time()
            branch_system.add_block(
                content=item['content'],
                root=project,
                slot=project_slots[project],
                tags={'labels': item['keywords']},
                importance=0.6  # ì•½ê°„ ë” ë†’ì€ ì¤‘ìš”ë„
            )
            insert_times.append((time.time() - start_time) * 1000)
            
        # ê²€ìƒ‰ ì„±ëŠ¥ (ìºì‹œ íš¨ê³¼ í…ŒìŠ¤íŠ¸ í¬í•¨)
        search_results = []
        repeated_queries = queries + queries[:len(queries)//3]  # ì¼ë¶€ ì¿¼ë¦¬ ë°˜ë³µ (ìºì‹œ í…ŒìŠ¤íŠ¸)
        
        for query_item in repeated_queries:
            expected_project = query_item.get('expected_project')
            search_slot = project_slots.get(expected_project, 'A')
            
            start_time = time.time()
            result = branch_system.search(
                query=query_item['query'],
                slot=search_slot,
                k=10,
                fallback=True,
                depth=4  # ìµœì í™”ëœ ê¹Šì´
            )
            search_time = (time.time() - start_time) * 1000
            
            search_results.append({
                'query': query_item['query'],
                'results_count': len(result.items),
                'search_time': search_time,
                'search_type': result.meta.get('search_type', 'unknown'),
                'hops': result.meta.get('hops', 0),
                'from_cache': result.meta.get('from_cache', False),
                'slot_used': search_slot
            })
            
        metrics = branch_system.get_metrics()
        
        # ê²€ìƒ‰ ìœ í˜•ë³„ í†µê³„
        search_type_stats = {}
        for result in search_results:
            search_type = result['search_type']
            if search_type not in search_type_stats:
                search_type_stats[search_type] = {
                    'count': 0, 'total_time': 0, 'total_hops': 0
                }
            search_type_stats[search_type]['count'] += 1
            search_type_stats[search_type]['total_time'] += result['search_time']
            search_type_stats[search_type]['total_hops'] += result['hops']
            
        # í‰ê·  ê³„ì‚°
        for stats in search_type_stats.values():
            if stats['count'] > 0:
                stats['avg_time'] = stats['total_time'] / stats['count']
                stats['avg_hops'] = stats['total_hops'] / stats['count']
        
        # ìºì‹œ í†µê³„
        cache_hits = sum(1 for r in search_results if r.get('from_cache', False))
        cache_hit_rate = cache_hits / len(search_results) if search_results else 0
        
        import statistics
        return {
            'system': 'optimized_branch',
            'workload_size': len(workload),
            'avg_insert_time': statistics.mean(insert_times),
            'p95_insert_time': statistics.quantiles(insert_times, n=20)[18] if len(insert_times) > 20 else max(insert_times),
            'avg_search_time': statistics.mean([r['search_time'] for r in search_results]),
            'avg_hops': metrics['avg_hops'],
            'local_hit_rate': metrics['local_hit_rate'],
            'fallback_rate': metrics.get('fallback_rate', 0.0),
            'cache_hit_rate': cache_hit_rate,
            'total_searches': len(search_results),
            'search_type_distribution': search_type_stats,
            'branch_stats': {
                'total_branches': len(branch_system.branches),
                'slot_distribution': project_slots,
                'cache_size': len(branch_system.search_cache)
            },
            'search_results': search_results
        }
    
    def run_comparison_test(self, workload_size: int = 100) -> dict:
        """3-way ë¹„êµ í…ŒìŠ¤íŠ¸: Legacy vs Original Branch vs Optimized Branch"""
        print(f"ğŸ”¥ Running 3-Way Performance Comparison (workload: {workload_size})")
        
        # ì›Œí¬ë¡œë“œ ìƒì„±
        workload = self.generate_realistic_workload(
            num_sessions=workload_size // 20,
            blocks_per_session=20
        )
        queries = self.generate_search_queries(workload)
        
        print(f"ğŸ“Š Generated {len(workload)} blocks and {len(queries)} queries")
        
        # 1. Legacy ì‹œìŠ¤í…œ
        legacy_results = self.run_legacy_benchmark(workload, queries)
        
        # 2. Original Branch ì‹œìŠ¤í…œ (ì„ì‹œë¡œ ë°±ì—…ìš© ìƒì„±)
        original_results = self.run_branch_benchmark(workload, queries)
        
        # 3. Optimized Branch ì‹œìŠ¤í…œ
        optimized_results = self.run_optimized_branch_benchmark(workload, queries)
        
        # ë¹„êµ ë¶„ì„
        comparison = {
            'timestamp': time.time(),
            'workload_size': len(workload),
            'query_count': len(queries),
            'legacy_results': legacy_results,
            'original_branch_results': original_results,
            'optimized_branch_results': optimized_results,
            'improvements': {
                'original_vs_legacy': self._safe_calculate_improvements(legacy_results, original_results),
                'optimized_vs_legacy': self._safe_calculate_improvements(legacy_results, optimized_results),
                'optimized_vs_original': self._safe_calculate_improvements(original_results, optimized_results)
            }
        }
        
        return comparison
    
    def print_three_way_comparison(self, results: dict):
        """3-way ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ”¥ THREE-WAY PERFORMANCE COMPARISON")
        print("="*80)
        
        legacy = results['legacy_results']
        original = results['original_branch_results']
        optimized = results['optimized_branch_results']
        
        print(f"\nğŸ”¢ Workload: {results['workload_size']} blocks, {results['query_count']} queries")
        
        print(f"\nğŸ” Search Performance Comparison:")
        print(f"  System               Avg Time    Avg Hops    Hit Rate    Cache Rate")
        print(f"  -------------------- ----------- ----------- ----------- -----------")
        print(f"  Legacy Graph         {legacy['avg_search_time']:>8.2f}ms {legacy['avg_hops']:>8.1f}    {legacy['hit_rate']:>8.1%}           -")
        print(f"  Original Branch      {original['avg_search_time']:>8.2f}ms {original['avg_hops']:>8.1f}    {original.get('local_hit_rate', 0):>8.1%}           -")
        print(f"  Optimized Branch     {optimized['avg_search_time']:>8.2f}ms {optimized['avg_hops']:>8.1f}    {optimized.get('local_hit_rate', 0):>8.1%}    {optimized.get('cache_hit_rate', 0):>8.1%}")
        
        print(f"\nğŸ“ˆ Optimization Impact:")
        opt_vs_orig = results['improvements']['optimized_vs_original']
        opt_vs_legacy = results['improvements']['optimized_vs_legacy']
        
        print(f"  Optimized vs Original Branch:")
        print(f"    Search Time:       {opt_vs_orig['search_time_improvement']:>+8.1f}%")
        print(f"    Hops Reduction:    {opt_vs_orig['hops_reduction']:>+8.1f}%")
        print(f"    Hit Rate Delta:    {opt_vs_orig['hit_rate_comparison']['improvement']:>+8.3f}")
        
        print(f"  Optimized vs Legacy Graph:")
        print(f"    Search Time:       {opt_vs_legacy['search_time_improvement']:>+8.1f}%")
        print(f"    Hops Reduction:    {opt_vs_legacy['hops_reduction']:>+8.1f}%")
        print(f"    Hit Rate Delta:    {opt_vs_legacy['hit_rate_comparison']['improvement']:>+8.3f}")
        
        # Search Type Distribution (Optimized)
        if 'search_type_distribution' in optimized:
            print(f"\nğŸŒ³ Optimized Branch Search Types:")
            for search_type, stats in optimized['search_type_distribution'].items():
                print(f"    {search_type:15}: {stats['count']:>3} searches, "
                      f"{stats.get('avg_time', 0):>5.1f}ms avg, "
                      f"{stats.get('avg_hops', 0):>4.1f} hops avg")
        
        print(f"\nğŸ’¾ Cache Performance:")
        print(f"    Cache Hit Rate:     {optimized.get('cache_hit_rate', 0):>8.1%}")
        print(f"    Cache Size:         {optimized['branch_stats'].get('cache_size', 0):>8} entries")
        
        print("="*80)
    
    def _safe_calculate_improvements(self, old_results: dict, new_results: dict) -> dict:
        """ì•ˆì „í•œ ê°œì„  ì§€í‘œ ê³„ì‚°"""
        
        def safe_percentage_improvement(old_val: float, new_val: float) -> float:
            if old_val == 0:
                return 0.0
            return ((old_val - new_val) / old_val) * 100
        
        improvements = {
            'search_time_improvement': safe_percentage_improvement(
                old_results.get('avg_search_time', 0),
                new_results.get('avg_search_time', 0)
            ),
            'hops_reduction': safe_percentage_improvement(
                old_results.get('avg_hops', 0),
                new_results.get('avg_hops', 0)
            ),
            'insert_time_improvement': safe_percentage_improvement(
                old_results.get('avg_insert_time', 0),
                new_results.get('avg_insert_time', 0)
            ),
            'hit_rate_comparison': {
                'old': old_results.get('hit_rate', old_results.get('local_hit_rate', 0)),
                'new': new_results.get('hit_rate', new_results.get('local_hit_rate', 0)),
                'improvement': (new_results.get('hit_rate', new_results.get('local_hit_rate', 0)) - 
                              old_results.get('hit_rate', old_results.get('local_hit_rate', 0)))
            }
        }
        
        return improvements


def main():
    """ìµœì í™”ëœ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    benchmark = OptimizedBenchmark()
    
    # í…ŒìŠ¤íŠ¸ í¬ê¸°ë“¤
    test_sizes = [100, 200]
    
    all_results = []
    
    for size in test_sizes:
        print(f"\n{'='*30} Testing {size} blocks {'='*30}")
        results = benchmark.run_comparison_test(workload_size=size)
        benchmark.print_three_way_comparison(results)
        all_results.append(results)
        
        # ê°œë³„ ê²°ê³¼ë„ ìš”ì•½ ì¶œë ¥
        benchmark.print_results_summary({
            'workload_size': results['workload_size'],
            'query_count': results['query_count'],
            'legacy_results': results['legacy_results'],
            'branch_results': results['optimized_branch_results'],
            'improvements': results['improvements']['optimized_vs_legacy'],
            'summary': {
                'search_time_improvement_pct': results['improvements']['optimized_vs_legacy']['search_time_improvement'],
                'avg_hops_reduction_pct': results['improvements']['optimized_vs_legacy']['hops_reduction'],
                'local_hit_rate': results['optimized_branch_results'].get('local_hit_rate', 0.0),
                'fallback_rate': results['optimized_branch_results'].get('fallback_rate', 0.0)
            }
        })
    
    # ê²°ê³¼ ì €ì¥
    output_file = f"optimized_benchmark_results_{int(time.time())}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Optimized results saved to: {output_file}")
    
    return all_results


if __name__ == "__main__":
    results = main()